{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83d\ude80 Floxide: The Power of Workflows in Rust \u00b6 A type-safe, composable directed graph workflow system written in Rust. \ud83d\udcab Overview \u00b6 Floxide transforms complex workflow orchestration into a delightful experience. Built with Rust's powerful type system at its core, Floxide provides a flexible, performant, and type-safe way to create sophisticated workflow graphs with crystal-clear transitions between steps. \u2728 Key Features \u00b6 \ud83d\udd12 Type-Safe By Design : Leverage Rust's type system for compile-time workflow correctness \ud83e\udde9 Composable Architecture : Build complex workflows from simple, reusable components \u26a1 Async First : Native support for asynchronous execution with Tokio \ud83d\udd04 Advanced Patterns : Support for batch processing, event-driven workflows, and more \ud83d\udcbe State Management : Built-in serialization for workflow persistence \ud83d\udd0d Observability : Comprehensive tracing and monitoring capabilities \ud83e\uddea Testable : Design your workflows for easy testing and verification \ud83d\udcd0 Architectural Decisions \u00b6 This project follows documented architectural decisions recorded in ADRs (Architectural Decision Records). Each ADR captures the context, decision, and consequences of significant architectural choices. Key architectural decisions include: Core Framework Abstractions - Defining the fundamental abstractions like Node, Action, and Workflow with a trait-based approach for type safety and flexibility. Project Structure and Crate Organization - Organizing the framework as a Cargo workspace with multiple specialized crates for modularity and separation of concerns. Async Runtime Selection - Choosing Tokio as the primary async runtime for its comprehensive feature set and wide adoption. Node Lifecycle Methods - Implementing a three-phase lifecycle (prep/exec/post) for workflow nodes to provide clear separation of concerns. Batch Processing Implementation - Designing a batch processing system that efficiently handles parallel execution with configurable concurrency limits. \ud83d\ude80 Quick Start \u00b6 Add Floxide to your project: [dependencies] floxide-core = \"0.1.0\" Create your first workflow: use floxide_core ::{ lifecycle_node , LifecycleNode , Workflow , DefaultAction , FloxideError }; use async_trait :: async_trait ; use std :: sync :: Arc ; // Define your context type #[derive(Debug, Clone)] struct MessageContext { input : String , result : Option < String > , } // Create a node using the convenience function fn create_processor_node () -> impl LifecycleNode < MessageContext , DefaultAction > { lifecycle_node ( Some ( \"processor\" ), // Node ID | ctx : & mut MessageContext | async move { // Preparation phase println! ( \"Preparing to process: {}\" , ctx . input ); Ok ( ctx . input . clone ()) }, | input : String | async move { // Execution phase println! ( \"Processing message...\" ); Ok ( format! ( \"\u2705 Processed: {}\" , input )) }, | _prep , exec_result , ctx : & mut MessageContext | async move { // Post-processing phase ctx . result = Some ( exec_result ); Ok ( DefaultAction :: Next ) }, ) } #[tokio::main] async fn main () -> Result < (), Box < dyn std :: error :: Error >> { // Create a context let mut context = MessageContext { input : \"Hello, Floxide!\" . to_string (), result : None , }; // Create a node and workflow let node = Arc :: new ( create_processor_node ()); let mut workflow = Workflow :: new ( node ); // Execute the workflow workflow . execute ( & mut context ). await ? ; // Print the result println! ( \"Result: {:?}\" , context . result ); Ok (()) } \ud83e\udde9 Workflow Pattern Examples \u00b6 Floxide supports a wide variety of workflow patterns through its modular crate system. Each pattern is designed to solve specific workflow challenges: \ud83d\udd04 Simple Chain (Linear Workflow) \u00b6 A basic sequence of nodes executed one after another. This is the foundation of all workflows. graph LR A[\"Process Data\"] --> B[\"Format Output\"] --> C[\"Store Result\"] style A fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style B fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style C fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px \ud83c\udf32 Conditional Branching \u00b6 Workflows that make decisions based on context data or node results, directing flow through different paths. graph TD A[\"Validate Input\"] -->|Valid| B[\"Process Data\"] A -->|Invalid| C[\"Error Handler\"] B -->|Success| D[\"Format Output\"] B -->|Error| C style A fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style B fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style C fill:#ffcccc,stroke:#e53935,stroke-width:2px style D fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px \ud83d\udd00 Parallel Batch Processing \u00b6 Process multiple items concurrently with controlled parallelism, ideal for high-throughput data processing. graph TD A[\"Batch Input\"] --> B[\"Split Batch\"] B --> C1[\"Process Item 1\"] B --> C2[\"Process Item 2\"] B --> C3[\"Process Item 3\"] C1 --> D[\"Aggregate Results\"] C2 --> D C3 --> D style A fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style B fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style C1 fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style C2 fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style C3 fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style D fill:#e3f2fd,stroke:#1565c0,stroke-width:2px \ud83d\udcda Documentation \u00b6 Explore our extensive documentation: Getting Started - Installation and quick start guides Core Concepts - Learn about the fundamental concepts of Floxide Guides - In-depth guides for specific use cases Examples - Example workflows to help you get started Architecture - Architectural Decision Records (ADRs) API Reference - Detailed API documentation","title":"Home"},{"location":"#floxide-the-power-of-workflows-in-rust","text":"A type-safe, composable directed graph workflow system written in Rust.","title":"\ud83d\ude80 Floxide: The Power of Workflows in Rust"},{"location":"#overview","text":"Floxide transforms complex workflow orchestration into a delightful experience. Built with Rust's powerful type system at its core, Floxide provides a flexible, performant, and type-safe way to create sophisticated workflow graphs with crystal-clear transitions between steps.","title":"\ud83d\udcab Overview"},{"location":"#key-features","text":"\ud83d\udd12 Type-Safe By Design : Leverage Rust's type system for compile-time workflow correctness \ud83e\udde9 Composable Architecture : Build complex workflows from simple, reusable components \u26a1 Async First : Native support for asynchronous execution with Tokio \ud83d\udd04 Advanced Patterns : Support for batch processing, event-driven workflows, and more \ud83d\udcbe State Management : Built-in serialization for workflow persistence \ud83d\udd0d Observability : Comprehensive tracing and monitoring capabilities \ud83e\uddea Testable : Design your workflows for easy testing and verification","title":"\u2728 Key Features"},{"location":"#architectural-decisions","text":"This project follows documented architectural decisions recorded in ADRs (Architectural Decision Records). Each ADR captures the context, decision, and consequences of significant architectural choices. Key architectural decisions include: Core Framework Abstractions - Defining the fundamental abstractions like Node, Action, and Workflow with a trait-based approach for type safety and flexibility. Project Structure and Crate Organization - Organizing the framework as a Cargo workspace with multiple specialized crates for modularity and separation of concerns. Async Runtime Selection - Choosing Tokio as the primary async runtime for its comprehensive feature set and wide adoption. Node Lifecycle Methods - Implementing a three-phase lifecycle (prep/exec/post) for workflow nodes to provide clear separation of concerns. Batch Processing Implementation - Designing a batch processing system that efficiently handles parallel execution with configurable concurrency limits.","title":"\ud83d\udcd0 Architectural Decisions"},{"location":"#quick-start","text":"Add Floxide to your project: [dependencies] floxide-core = \"0.1.0\" Create your first workflow: use floxide_core ::{ lifecycle_node , LifecycleNode , Workflow , DefaultAction , FloxideError }; use async_trait :: async_trait ; use std :: sync :: Arc ; // Define your context type #[derive(Debug, Clone)] struct MessageContext { input : String , result : Option < String > , } // Create a node using the convenience function fn create_processor_node () -> impl LifecycleNode < MessageContext , DefaultAction > { lifecycle_node ( Some ( \"processor\" ), // Node ID | ctx : & mut MessageContext | async move { // Preparation phase println! ( \"Preparing to process: {}\" , ctx . input ); Ok ( ctx . input . clone ()) }, | input : String | async move { // Execution phase println! ( \"Processing message...\" ); Ok ( format! ( \"\u2705 Processed: {}\" , input )) }, | _prep , exec_result , ctx : & mut MessageContext | async move { // Post-processing phase ctx . result = Some ( exec_result ); Ok ( DefaultAction :: Next ) }, ) } #[tokio::main] async fn main () -> Result < (), Box < dyn std :: error :: Error >> { // Create a context let mut context = MessageContext { input : \"Hello, Floxide!\" . to_string (), result : None , }; // Create a node and workflow let node = Arc :: new ( create_processor_node ()); let mut workflow = Workflow :: new ( node ); // Execute the workflow workflow . execute ( & mut context ). await ? ; // Print the result println! ( \"Result: {:?}\" , context . result ); Ok (()) }","title":"\ud83d\ude80 Quick Start"},{"location":"#workflow-pattern-examples","text":"Floxide supports a wide variety of workflow patterns through its modular crate system. Each pattern is designed to solve specific workflow challenges:","title":"\ud83e\udde9 Workflow Pattern Examples"},{"location":"#simple-chain-linear-workflow","text":"A basic sequence of nodes executed one after another. This is the foundation of all workflows. graph LR A[\"Process Data\"] --> B[\"Format Output\"] --> C[\"Store Result\"] style A fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style B fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style C fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px","title":"\ud83d\udd04 Simple Chain (Linear Workflow)"},{"location":"#conditional-branching","text":"Workflows that make decisions based on context data or node results, directing flow through different paths. graph TD A[\"Validate Input\"] -->|Valid| B[\"Process Data\"] A -->|Invalid| C[\"Error Handler\"] B -->|Success| D[\"Format Output\"] B -->|Error| C style A fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style B fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style C fill:#ffcccc,stroke:#e53935,stroke-width:2px style D fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px","title":"\ud83c\udf32 Conditional Branching"},{"location":"#parallel-batch-processing","text":"Process multiple items concurrently with controlled parallelism, ideal for high-throughput data processing. graph TD A[\"Batch Input\"] --> B[\"Split Batch\"] B --> C1[\"Process Item 1\"] B --> C2[\"Process Item 2\"] B --> C3[\"Process Item 3\"] C1 --> D[\"Aggregate Results\"] C2 --> D C3 --> D style A fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style B fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style C1 fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style C2 fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style C3 fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style D fill:#e3f2fd,stroke:#1565c0,stroke-width:2px","title":"\ud83d\udd00 Parallel Batch Processing"},{"location":"#documentation","text":"Explore our extensive documentation: Getting Started - Installation and quick start guides Core Concepts - Learn about the fundamental concepts of Floxide Guides - In-depth guides for specific use cases Examples - Example workflows to help you get started Architecture - Architectural Decision Records (ADRs) API Reference - Detailed API documentation","title":"\ud83d\udcda Documentation"},{"location":"REBRANDING/","text":"Project Rebranding: flowrs \u2192 floxide \u00b6 Overview \u00b6 This document summarizes the rebranding process from \"flowrs\" to \"floxide\" that was completed on February 26, 2025. Rebranding Steps \u00b6 Name Selection Selected \"floxide\" as the new name, combining \"flow\" (workflow) and \"oxide\" (a reference to Rust) Verified availability on crates.io Codebase Updates Updated all package names in Cargo.toml files Renamed crate directories from flowrs-* to floxide-* Updated all code references, including imports and error types Updated documentation references Repository Changes Renamed repository directory from flow-rs to floxide Updated Git remote URL to point to the new repository name Documentation Created ADR-0032 to document the rebranding decision Updated README.md and other documentation files Updated API documentation file names Cleanup Removed old directories and files Updated references in the site directory Verification \u00b6 The rebranding was verified through: - Successful build with cargo build - Successful test run with cargo test - Grep searches to ensure no remaining references to the old name Next Steps \u00b6 Update GitHub repository name Update crates.io package names when publishing Notify users of the name change Update external documentation and links References \u00b6 ADR-0032: Project Rebranding to Floxide","title":"Project Rebranding: flowrs \u2192 floxide"},{"location":"REBRANDING/#project-rebranding-flowrs-floxide","text":"","title":"Project Rebranding: flowrs \u2192 floxide"},{"location":"REBRANDING/#overview","text":"This document summarizes the rebranding process from \"flowrs\" to \"floxide\" that was completed on February 26, 2025.","title":"Overview"},{"location":"REBRANDING/#rebranding-steps","text":"Name Selection Selected \"floxide\" as the new name, combining \"flow\" (workflow) and \"oxide\" (a reference to Rust) Verified availability on crates.io Codebase Updates Updated all package names in Cargo.toml files Renamed crate directories from flowrs-* to floxide-* Updated all code references, including imports and error types Updated documentation references Repository Changes Renamed repository directory from flow-rs to floxide Updated Git remote URL to point to the new repository name Documentation Created ADR-0032 to document the rebranding decision Updated README.md and other documentation files Updated API documentation file names Cleanup Removed old directories and files Updated references in the site directory","title":"Rebranding Steps"},{"location":"REBRANDING/#verification","text":"The rebranding was verified through: - Successful build with cargo build - Successful test run with cargo test - Grep searches to ensure no remaining references to the old name","title":"Verification"},{"location":"REBRANDING/#next-steps","text":"Update GitHub repository name Update crates.io package names when publishing Notify users of the name change Update external documentation and links","title":"Next Steps"},{"location":"REBRANDING/#references","text":"ADR-0032: Project Rebranding to Floxide","title":"References"},{"location":"contributing/","text":"Contributing to Floxide \u00b6 Thank you for your interest in contributing to the Floxide framework! This document provides guidelines and instructions for contributing to the project. Code of Conduct \u00b6 Please be respectful and considerate of others when contributing to the project. We aim to foster an inclusive and welcoming community. Getting Started \u00b6 Fork the Repository : Start by forking the Floxide repository on GitHub. Clone Your Fork : Clone your fork to your local machine: git clone https://github.com/your-username/floxide.git cd floxide Set Up Development Environment : Make sure you have Rust and Cargo installed. We recommend using rustup for managing your Rust installation. Create a Branch : Create a branch for your changes: git checkout -b feature/your-feature-name Development Workflow \u00b6 Architectural Decision Records (ADRs) \u00b6 Before implementing any significant architectural changes, you must create or update an Architectural Decision Record (ADR). See the ADR Process for details. Code Style \u00b6 Follow the Rust API Guidelines . Use cargo fmt to format your code. Use cargo clippy to check for common mistakes and improve your code. Testing \u00b6 Write tests for all new functionality. Ensure all tests pass before submitting a pull request. Run tests with cargo test . Documentation \u00b6 Document all public APIs with doc comments. Update relevant documentation when making changes. Build and check the documentation with cargo doc . Submitting Changes \u00b6 Commit Your Changes : Make small, focused commits with clear commit messages: git commit -m \"Add feature X\" Push to Your Fork : git push origin feature/your-feature-name Create a Pull Request : Go to the Floxide repository and create a pull request from your branch. Code Review : Wait for code review and address any feedback. Pull Request Guidelines \u00b6 Provide a clear description of the changes. Link to any relevant issues. Ensure all tests pass. Make sure your code follows the project's style guidelines. Include documentation updates if necessary. Reporting Issues \u00b6 If you find a bug or have a feature request, please create an issue on the GitHub issue tracker . When reporting a bug, please include: A clear description of the issue Steps to reproduce Expected behavior Actual behavior Any relevant logs or error messages Community \u00b6 Join our community to discuss the project, ask questions, and get help: GitHub Discussions Discord (coming soon) License \u00b6 By contributing to Floxide, you agree that your contributions will be licensed under the project's license.","title":"Contributing"},{"location":"contributing/#contributing-to-floxide","text":"Thank you for your interest in contributing to the Floxide framework! This document provides guidelines and instructions for contributing to the project.","title":"Contributing to Floxide"},{"location":"contributing/#code-of-conduct","text":"Please be respectful and considerate of others when contributing to the project. We aim to foster an inclusive and welcoming community.","title":"Code of Conduct"},{"location":"contributing/#getting-started","text":"Fork the Repository : Start by forking the Floxide repository on GitHub. Clone Your Fork : Clone your fork to your local machine: git clone https://github.com/your-username/floxide.git cd floxide Set Up Development Environment : Make sure you have Rust and Cargo installed. We recommend using rustup for managing your Rust installation. Create a Branch : Create a branch for your changes: git checkout -b feature/your-feature-name","title":"Getting Started"},{"location":"contributing/#development-workflow","text":"","title":"Development Workflow"},{"location":"contributing/#architectural-decision-records-adrs","text":"Before implementing any significant architectural changes, you must create or update an Architectural Decision Record (ADR). See the ADR Process for details.","title":"Architectural Decision Records (ADRs)"},{"location":"contributing/#code-style","text":"Follow the Rust API Guidelines . Use cargo fmt to format your code. Use cargo clippy to check for common mistakes and improve your code.","title":"Code Style"},{"location":"contributing/#testing","text":"Write tests for all new functionality. Ensure all tests pass before submitting a pull request. Run tests with cargo test .","title":"Testing"},{"location":"contributing/#documentation","text":"Document all public APIs with doc comments. Update relevant documentation when making changes. Build and check the documentation with cargo doc .","title":"Documentation"},{"location":"contributing/#submitting-changes","text":"Commit Your Changes : Make small, focused commits with clear commit messages: git commit -m \"Add feature X\" Push to Your Fork : git push origin feature/your-feature-name Create a Pull Request : Go to the Floxide repository and create a pull request from your branch. Code Review : Wait for code review and address any feedback.","title":"Submitting Changes"},{"location":"contributing/#pull-request-guidelines","text":"Provide a clear description of the changes. Link to any relevant issues. Ensure all tests pass. Make sure your code follows the project's style guidelines. Include documentation updates if necessary.","title":"Pull Request Guidelines"},{"location":"contributing/#reporting-issues","text":"If you find a bug or have a feature request, please create an issue on the GitHub issue tracker . When reporting a bug, please include: A clear description of the issue Steps to reproduce Expected behavior Actual behavior Any relevant logs or error messages","title":"Reporting Issues"},{"location":"contributing/#community","text":"Join our community to discuss the project, ask questions, and get help: GitHub Discussions Discord (coming soon)","title":"Community"},{"location":"contributing/#license","text":"By contributing to Floxide, you agree that your contributions will be licensed under the project's license.","title":"License"},{"location":"adrs/","text":"Architectural Decision Records (ADRs) \u00b6 This directory contains the Architectural Decision Records (ADRs) for the floxide framework. ADRs document significant architectural decisions, their context, and their consequences. Index of ADRs \u00b6 ADR Title Status Date Summary ADR-0032 Project Rebranding to Floxide Accepted 2025-02-26 Documents the rebranding from floxide to floxide and its implications ADR-0007 Batch Processing Implementation Accepted 2024-02-25 Defines the approach for parallel batch processing using Tokio tasks and semaphores ADR-0008 Node Lifecycle Methods Accepted 2024-02-25 Defines the three-phase lifecycle (prep/exec/post) for nodes and adapter patterns ADR-0009 Cloneable Types for Batch Processing Accepted 2024-02-25 Requires item types in batch processing to implement Clone ADR-0010 Workflow Cloning Strategy Accepted 2024-02-25 Defines an approach for cloning workflows using Arc ADR-0011 Closure Lifetime Management in Async Contexts Accepted 2024-02-25 Outlines strategies for managing lifetimes in async closures ADR-0012 Testing Patterns for Async Node Implementations Accepted 2024-02-26 Defines patterns for testing async code with concrete trait implementations ADR-0013 Workflow Patterns Accepted 2024-02-25 Documents the eight core workflow patterns supported by the framework ADR-0014 Crate Publishing and CI/CD Setup Accepted 2024-02-25 Defines the approach for publishing crates and setting up CI/CD pipelines ADR-0015 Batch Processing Examples and Best Practices Accepted 2024-02-25 Documents concrete patterns and best practices for implementing batch processing ADR-0026 Documentation Deployment Strategy Accepted 2024-02-25 Defines the approach for deploying documentation to GitHub Pages ADR-0027 GitHub Actions Workflow Permissions Accepted 2024-02-25 Defines the permission requirements for GitHub Actions workflows How to Create a New ADR \u00b6 Copy the template from adr-template.md to ####-descriptive-title.md where #### is the next available number Fill in the sections: Status, Context, Decision, Consequences, etc. Add the new ADR to the index table above Submit the ADR for review before implementing significant changes ADR Format \u00b6 Each ADR follows this structure: Title : Clear, descriptive title prefixed with the ADR number Status : Proposed, Accepted, Deprecated, or Superseded Date : When the ADR was created Context : Problem description and background information Decision : The solution that addresses the context Consequences : Positive and negative effects of the decision Alternatives Considered : Other approaches that were evaluated Implementation Notes : Practical guidance for implementers ADR Process \u00b6 Proposal : Create a new ADR describing the architectural change Discussion : Review the ADR with the team Acceptance : Mark as Accepted once consensus is reached Implementation : Build according to the accepted ADR Maintenance : Update the ADR if understanding evolves Overview \u00b6 These ADRs document the key architectural decisions made during the development of the floxide framework. Each ADR provides context, the decision made, consequences, and alternatives considered. Index \u00b6 ADR-0001: ADR Process and Format Establishes the format and process for creating and maintaining ADRs Serves as a template for all future ADRs ADR-0002: Project Structure and Crate Organization (Accepted) Defines the Cargo workspace structure Establishes crate boundaries and relationships ADR-0003: Core Framework Abstractions (Accepted) Documents the core traits and structs that form the framework's foundation Defines the Node trait with a single process method Establishes ActionType as a trait for user-defined action types Provides a practical example of using type-safe action types in a workflow ADR-0004: Async Runtime Selection (Accepted) Justifies the selection of Tokio with full features Documents the use of async_trait for trait methods Establishes patterns for async execution ADR-0005: State Serialization and Deserialization (Accepted) Defines how workflow state is serialized for persistence and restoration Establishes storage abstractions for workflow snapshots Provides mechanisms for workflow checkpointing and resumption ADR-0006: Workflow Observability (Accepted) Adopts OpenTelemetry as the primary observability solution Designs an event-based system integrated with OpenTelemetry traces and metrics Provides tools for debugging, monitoring, and visualizing workflow execution ADR-0007: Batch Processing Implementation (Accepted) Defines the approach for parallel batch processing using Tokio tasks and semaphores Establishes patterns for handling multiple contexts concurrently ADR-0008: Node Lifecycle Methods (Accepted) Defines the three-phase lifecycle (prep/exec/post) for nodes Establishes adapter patterns for implementing lifecycle methods ADR-0009: Cloneable Types for Batch Processing (Accepted) Requires item types in batch processing to implement Clone Defines strategies for handling non-cloneable types ADR-0010: Workflow Cloning Strategy (Accepted) Defines an approach for cloning workflows using Arc Establishes patterns for sharing workflow definitions ADR-0011: Closure Lifetime Management (Accepted) Outlines strategies for managing lifetimes in async closures Provides solutions for common lifetime issues in async contexts ADR-0012: Testing Patterns for Async Node Implementations (Accepted) Defines patterns for testing async code with concrete trait implementations Establishes best practices for unit and integration testing ADR-0013: Workflow Patterns (Accepted) Documents the eight core workflow patterns supported by the framework Defines implementation approaches for each pattern Provides examples and use cases for each pattern type ADR-0014: Crate Publishing and CI/CD Setup (Accepted) Defines the approach for publishing crates to crates.io Establishes GitHub Actions workflows for CI/CD Documents versioning strategy and release automation ADR-0015: Batch Processing Examples and Best Practices (Accepted) Documents concrete patterns for batch processing implementations Provides solutions for handling type parameters in generic batch processing Outlines alternative approaches for simpler use cases Builds on ADR-0007 with practical implementation guidance ADR-0026: Documentation Deployment Strategy (Accepted) Defines the approach for deploying documentation to GitHub Pages Documents the use of GitHub Actions for automated documentation deployment Establishes best practices for permissions and security in deployment workflows Provides guidance on setting up GitHub Pages with GitHub Actions as the source ADR-0027: GitHub Actions Workflow Permissions (Accepted) Documents the permission requirements for GitHub Actions workflows Addresses issues with the version bump workflow permissions Establishes best practices for setting permissions in GitHub Actions workflows Provides guidance on using the principle of least privilege for workflow permissions ADR Statuses \u00b6 Proposed : Initial draft state Accepted : Approved for implementation Rejected : Declined, with reasons documented Superseded : Replaced by a newer ADR Amended : Modified after implementation Discarded : Determined to be unnecessary or redundant Process \u00b6 Please refer to ADR-0001 for details on how ADRs are created, reviewed, and maintained in this project. Relationships \u00b6 graph TD A[0001: ADR Process and Format] --> B[0002: Project Structure] A --> C[0003: Core Framework Abstractions] A --> D[0004: Async Runtime Selection] A --> E[0005: State Serialization] A --> F[0006: OpenTelemetry Observability] A --> G[0013: Workflow Patterns] A --> H[0014: Crate Publishing and CI/CD] A --> I[0026: Documentation Deployment] A --> J[0027: GitHub Actions Permissions] B --> C B --> H B --> I C --> D C --> E C --> F C --> G D --> G E -.-> F G --> F H --> I H --> J I --> J Note: This diagram shows the dependencies between ADRs. Each subsequent ADR builds upon the decisions established in the previous ones. The dotted line indicates that the observability system can utilize the state serialization system for event persistence. The workflow patterns ADR (0013) depends on core framework abstractions (0003) and async runtime selection (0004), and influences observability (0006).","title":"Architectural Decision Records (ADRs)"},{"location":"adrs/#architectural-decision-records-adrs","text":"This directory contains the Architectural Decision Records (ADRs) for the floxide framework. ADRs document significant architectural decisions, their context, and their consequences.","title":"Architectural Decision Records (ADRs)"},{"location":"adrs/#index-of-adrs","text":"ADR Title Status Date Summary ADR-0032 Project Rebranding to Floxide Accepted 2025-02-26 Documents the rebranding from floxide to floxide and its implications ADR-0007 Batch Processing Implementation Accepted 2024-02-25 Defines the approach for parallel batch processing using Tokio tasks and semaphores ADR-0008 Node Lifecycle Methods Accepted 2024-02-25 Defines the three-phase lifecycle (prep/exec/post) for nodes and adapter patterns ADR-0009 Cloneable Types for Batch Processing Accepted 2024-02-25 Requires item types in batch processing to implement Clone ADR-0010 Workflow Cloning Strategy Accepted 2024-02-25 Defines an approach for cloning workflows using Arc ADR-0011 Closure Lifetime Management in Async Contexts Accepted 2024-02-25 Outlines strategies for managing lifetimes in async closures ADR-0012 Testing Patterns for Async Node Implementations Accepted 2024-02-26 Defines patterns for testing async code with concrete trait implementations ADR-0013 Workflow Patterns Accepted 2024-02-25 Documents the eight core workflow patterns supported by the framework ADR-0014 Crate Publishing and CI/CD Setup Accepted 2024-02-25 Defines the approach for publishing crates and setting up CI/CD pipelines ADR-0015 Batch Processing Examples and Best Practices Accepted 2024-02-25 Documents concrete patterns and best practices for implementing batch processing ADR-0026 Documentation Deployment Strategy Accepted 2024-02-25 Defines the approach for deploying documentation to GitHub Pages ADR-0027 GitHub Actions Workflow Permissions Accepted 2024-02-25 Defines the permission requirements for GitHub Actions workflows","title":"Index of ADRs"},{"location":"adrs/#how-to-create-a-new-adr","text":"Copy the template from adr-template.md to ####-descriptive-title.md where #### is the next available number Fill in the sections: Status, Context, Decision, Consequences, etc. Add the new ADR to the index table above Submit the ADR for review before implementing significant changes","title":"How to Create a New ADR"},{"location":"adrs/#adr-format","text":"Each ADR follows this structure: Title : Clear, descriptive title prefixed with the ADR number Status : Proposed, Accepted, Deprecated, or Superseded Date : When the ADR was created Context : Problem description and background information Decision : The solution that addresses the context Consequences : Positive and negative effects of the decision Alternatives Considered : Other approaches that were evaluated Implementation Notes : Practical guidance for implementers","title":"ADR Format"},{"location":"adrs/#adr-process","text":"Proposal : Create a new ADR describing the architectural change Discussion : Review the ADR with the team Acceptance : Mark as Accepted once consensus is reached Implementation : Build according to the accepted ADR Maintenance : Update the ADR if understanding evolves","title":"ADR Process"},{"location":"adrs/#overview","text":"These ADRs document the key architectural decisions made during the development of the floxide framework. Each ADR provides context, the decision made, consequences, and alternatives considered.","title":"Overview"},{"location":"adrs/#index","text":"ADR-0001: ADR Process and Format Establishes the format and process for creating and maintaining ADRs Serves as a template for all future ADRs ADR-0002: Project Structure and Crate Organization (Accepted) Defines the Cargo workspace structure Establishes crate boundaries and relationships ADR-0003: Core Framework Abstractions (Accepted) Documents the core traits and structs that form the framework's foundation Defines the Node trait with a single process method Establishes ActionType as a trait for user-defined action types Provides a practical example of using type-safe action types in a workflow ADR-0004: Async Runtime Selection (Accepted) Justifies the selection of Tokio with full features Documents the use of async_trait for trait methods Establishes patterns for async execution ADR-0005: State Serialization and Deserialization (Accepted) Defines how workflow state is serialized for persistence and restoration Establishes storage abstractions for workflow snapshots Provides mechanisms for workflow checkpointing and resumption ADR-0006: Workflow Observability (Accepted) Adopts OpenTelemetry as the primary observability solution Designs an event-based system integrated with OpenTelemetry traces and metrics Provides tools for debugging, monitoring, and visualizing workflow execution ADR-0007: Batch Processing Implementation (Accepted) Defines the approach for parallel batch processing using Tokio tasks and semaphores Establishes patterns for handling multiple contexts concurrently ADR-0008: Node Lifecycle Methods (Accepted) Defines the three-phase lifecycle (prep/exec/post) for nodes Establishes adapter patterns for implementing lifecycle methods ADR-0009: Cloneable Types for Batch Processing (Accepted) Requires item types in batch processing to implement Clone Defines strategies for handling non-cloneable types ADR-0010: Workflow Cloning Strategy (Accepted) Defines an approach for cloning workflows using Arc Establishes patterns for sharing workflow definitions ADR-0011: Closure Lifetime Management (Accepted) Outlines strategies for managing lifetimes in async closures Provides solutions for common lifetime issues in async contexts ADR-0012: Testing Patterns for Async Node Implementations (Accepted) Defines patterns for testing async code with concrete trait implementations Establishes best practices for unit and integration testing ADR-0013: Workflow Patterns (Accepted) Documents the eight core workflow patterns supported by the framework Defines implementation approaches for each pattern Provides examples and use cases for each pattern type ADR-0014: Crate Publishing and CI/CD Setup (Accepted) Defines the approach for publishing crates to crates.io Establishes GitHub Actions workflows for CI/CD Documents versioning strategy and release automation ADR-0015: Batch Processing Examples and Best Practices (Accepted) Documents concrete patterns for batch processing implementations Provides solutions for handling type parameters in generic batch processing Outlines alternative approaches for simpler use cases Builds on ADR-0007 with practical implementation guidance ADR-0026: Documentation Deployment Strategy (Accepted) Defines the approach for deploying documentation to GitHub Pages Documents the use of GitHub Actions for automated documentation deployment Establishes best practices for permissions and security in deployment workflows Provides guidance on setting up GitHub Pages with GitHub Actions as the source ADR-0027: GitHub Actions Workflow Permissions (Accepted) Documents the permission requirements for GitHub Actions workflows Addresses issues with the version bump workflow permissions Establishes best practices for setting permissions in GitHub Actions workflows Provides guidance on using the principle of least privilege for workflow permissions","title":"Index"},{"location":"adrs/#adr-statuses","text":"Proposed : Initial draft state Accepted : Approved for implementation Rejected : Declined, with reasons documented Superseded : Replaced by a newer ADR Amended : Modified after implementation Discarded : Determined to be unnecessary or redundant","title":"ADR Statuses"},{"location":"adrs/#process","text":"Please refer to ADR-0001 for details on how ADRs are created, reviewed, and maintained in this project.","title":"Process"},{"location":"adrs/#relationships","text":"graph TD A[0001: ADR Process and Format] --> B[0002: Project Structure] A --> C[0003: Core Framework Abstractions] A --> D[0004: Async Runtime Selection] A --> E[0005: State Serialization] A --> F[0006: OpenTelemetry Observability] A --> G[0013: Workflow Patterns] A --> H[0014: Crate Publishing and CI/CD] A --> I[0026: Documentation Deployment] A --> J[0027: GitHub Actions Permissions] B --> C B --> H B --> I C --> D C --> E C --> F C --> G D --> G E -.-> F G --> F H --> I H --> J I --> J Note: This diagram shows the dependencies between ADRs. Each subsequent ADR builds upon the decisions established in the previous ones. The dotted line indicates that the observability system can utilize the state serialization system for event persistence. The workflow patterns ADR (0013) depends on core framework abstractions (0003) and async runtime selection (0004), and influences observability (0006).","title":"Relationships"},{"location":"adrs/0001-adr-process-and-format/","text":"ADR-0001: Architectural Decision Records Process and Format \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 As we begin the development of the Rust Flow Framework, we need a consistent way to document architectural decisions. Architectural Decision Records (ADRs) provide a mechanism to document important architectural decisions, their context, and their consequences. This project is starting with no existing ADRs, so we need to establish a process and format for creating and maintaining them. This ADR documents how we will create, format, and manage ADRs throughout the project lifecycle. Decision \u00b6 We will use Architectural Decision Records (ADRs) to document all significant architectural decisions in the Rust Flow Framework. The following guidelines will be followed: ADR Creation and Process \u00b6 Numbering : ADRs will be numbered sequentially with a four-digit number prefix (e.g., 0001 , 0002 ). Format : ADRs will be stored as Markdown files in the /docs/adrs/ directory. Naming Convention : ADR files will be named with their number and a kebab-case title, e.g., 0001-adr-process-and-format.md . Creation Timing : ADRs must be created before implementing any architectural decision, not after. Incremental Creation : ADRs can be created incrementally as decisions are made throughout the project. ADR Content Structure \u00b6 Each ADR will include: Title : A clear, descriptive title following the format \"ADR-NNNN: Title\" Status : One of: Proposed : Initial state when the ADR is first drafted Accepted : Approved for implementation Rejected : Declined, with reasons documented Superseded : Replaced by a newer ADR (with reference to the new ADR) Amended : Modified after implementation Date : The date the ADR was last updated Context : The problem being addressed and relevant background information Decision : The architectural decision that was made and the reasoning Consequences : The results of the decision, both positive and negative Alternatives Considered : Other options that were evaluated and why they were not chosen ADR Lifecycle Management \u00b6 Review Process : ADRs will be reviewed before they are accepted. Amendments : Existing ADRs can be amended with additional information, but the core decisions should not be changed after implementation. Superseding : If a decision changes fundamentally, a new ADR should be created that supersedes the old one. The old ADR should be updated to reference the new one. Retrospective Updates : Consequences that were not anticipated can be added to ADRs after implementation to serve as a record for future reference. ADR Index \u00b6 An index of all ADRs will be maintained in /docs/adrs/README.md to provide a central reference point. Consequences \u00b6 Positive \u00b6 Improved documentation of architectural decisions Better understanding of why certain approaches were chosen Historical context for future contributors Clear process for proposing and evaluating architectural changes Living documentation that evolves with the project Negative \u00b6 Additional overhead for documenting decisions Maintenance burden for keeping ADRs up-to-date Potential for ADRs to become outdated if not properly maintained Alternatives Considered \u00b6 No Formalized Documentation \u00b6 Pros : Less upfront work, more flexibility Cons : Loss of context over time, difficulty onboarding new contributors, inconsistent decision-making Wiki-Based Documentation \u00b6 Pros : Easier collaborative editing, more flexible format Cons : Separation from source code, less versioning control, potential for unstructured content Comments in Code \u00b6 Pros : Close proximity to implementation Cons : Limited space, difficult to get a holistic view, not suitable for decisions that span multiple files We chose the ADR approach because it provides a structured, version-controlled way to document decisions that is closely tied to the codebase but separate enough to provide a comprehensive view of the architecture.","title":"ADR-0001: Architectural Decision Records Process and Format"},{"location":"adrs/0001-adr-process-and-format/#adr-0001-architectural-decision-records-process-and-format","text":"","title":"ADR-0001: Architectural Decision Records Process and Format"},{"location":"adrs/0001-adr-process-and-format/#status","text":"Accepted","title":"Status"},{"location":"adrs/0001-adr-process-and-format/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0001-adr-process-and-format/#context","text":"As we begin the development of the Rust Flow Framework, we need a consistent way to document architectural decisions. Architectural Decision Records (ADRs) provide a mechanism to document important architectural decisions, their context, and their consequences. This project is starting with no existing ADRs, so we need to establish a process and format for creating and maintaining them. This ADR documents how we will create, format, and manage ADRs throughout the project lifecycle.","title":"Context"},{"location":"adrs/0001-adr-process-and-format/#decision","text":"We will use Architectural Decision Records (ADRs) to document all significant architectural decisions in the Rust Flow Framework. The following guidelines will be followed:","title":"Decision"},{"location":"adrs/0001-adr-process-and-format/#adr-creation-and-process","text":"Numbering : ADRs will be numbered sequentially with a four-digit number prefix (e.g., 0001 , 0002 ). Format : ADRs will be stored as Markdown files in the /docs/adrs/ directory. Naming Convention : ADR files will be named with their number and a kebab-case title, e.g., 0001-adr-process-and-format.md . Creation Timing : ADRs must be created before implementing any architectural decision, not after. Incremental Creation : ADRs can be created incrementally as decisions are made throughout the project.","title":"ADR Creation and Process"},{"location":"adrs/0001-adr-process-and-format/#adr-content-structure","text":"Each ADR will include: Title : A clear, descriptive title following the format \"ADR-NNNN: Title\" Status : One of: Proposed : Initial state when the ADR is first drafted Accepted : Approved for implementation Rejected : Declined, with reasons documented Superseded : Replaced by a newer ADR (with reference to the new ADR) Amended : Modified after implementation Date : The date the ADR was last updated Context : The problem being addressed and relevant background information Decision : The architectural decision that was made and the reasoning Consequences : The results of the decision, both positive and negative Alternatives Considered : Other options that were evaluated and why they were not chosen","title":"ADR Content Structure"},{"location":"adrs/0001-adr-process-and-format/#adr-lifecycle-management","text":"Review Process : ADRs will be reviewed before they are accepted. Amendments : Existing ADRs can be amended with additional information, but the core decisions should not be changed after implementation. Superseding : If a decision changes fundamentally, a new ADR should be created that supersedes the old one. The old ADR should be updated to reference the new one. Retrospective Updates : Consequences that were not anticipated can be added to ADRs after implementation to serve as a record for future reference.","title":"ADR Lifecycle Management"},{"location":"adrs/0001-adr-process-and-format/#adr-index","text":"An index of all ADRs will be maintained in /docs/adrs/README.md to provide a central reference point.","title":"ADR Index"},{"location":"adrs/0001-adr-process-and-format/#consequences","text":"","title":"Consequences"},{"location":"adrs/0001-adr-process-and-format/#positive","text":"Improved documentation of architectural decisions Better understanding of why certain approaches were chosen Historical context for future contributors Clear process for proposing and evaluating architectural changes Living documentation that evolves with the project","title":"Positive"},{"location":"adrs/0001-adr-process-and-format/#negative","text":"Additional overhead for documenting decisions Maintenance burden for keeping ADRs up-to-date Potential for ADRs to become outdated if not properly maintained","title":"Negative"},{"location":"adrs/0001-adr-process-and-format/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0001-adr-process-and-format/#no-formalized-documentation","text":"Pros : Less upfront work, more flexibility Cons : Loss of context over time, difficulty onboarding new contributors, inconsistent decision-making","title":"No Formalized Documentation"},{"location":"adrs/0001-adr-process-and-format/#wiki-based-documentation","text":"Pros : Easier collaborative editing, more flexible format Cons : Separation from source code, less versioning control, potential for unstructured content","title":"Wiki-Based Documentation"},{"location":"adrs/0001-adr-process-and-format/#comments-in-code","text":"Pros : Close proximity to implementation Cons : Limited space, difficult to get a holistic view, not suitable for decisions that span multiple files We chose the ADR approach because it provides a structured, version-controlled way to document decisions that is closely tied to the codebase but separate enough to provide a comprehensive view of the architecture.","title":"Comments in Code"},{"location":"adrs/0002-project-structure-and-crate-organization/","text":"ADR-0002: Project Structure and Crate Organization \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 As we begin development of the floxide framework, we need to determine how to structure the codebase. The way we organize our project will impact testability, maintainability, and the ability to evolve the codebase over time. It will also influence how consumers of our library will interact with it. Rust provides specific patterns and organization concepts such as crates, workspaces, and modules that we need to consider for optimal organization. Decision \u00b6 We will organize the floxide framework as a Cargo workspace with multiple crates to provide modularity and separation of concerns. Workspace Structure \u00b6 The project will be structured as follows: floxide/ \u251c\u2500\u2500 Cargo.toml # Workspace manifest \u251c\u2500\u2500 crates/ \u2502 \u251c\u2500\u2500 floxide-core/ # Core traits and structures \u2502 \u2502 \u251c\u2500\u2500 Cargo.toml \u2502 \u2502 \u2514\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 floxide-transform/ # Transform node implementations \u2502 \u2502 \u251c\u2500\u2500 Cargo.toml \u2502 \u2502 \u2514\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 floxide-derive/ # Optional proc macros for code generation \u2502 \u2502 \u251c\u2500\u2500 Cargo.toml \u2502 \u2502 \u2514\u2500\u2500 src/ \u2502 \u2514\u2500\u2500 floxide-test/ # Test utilities and fixtures \u2502 \u251c\u2500\u2500 Cargo.toml \u2502 \u2514\u2500\u2500 src/ \u251c\u2500\u2500 examples/ # Example implementations \u2502 \u251c\u2500\u2500 Cargo.toml \u2502 \u2514\u2500\u2500 examples/ # Standard example files \u251c\u2500\u2500 benches/ # Performance benchmarks \u2502 \u251c\u2500\u2500 Cargo.toml \u2502 \u2514\u2500\u2500 src/ \u2514\u2500\u2500 docs/ # Documentation \u2514\u2500\u2500 adrs/ # Architectural Decision Records Core Crates \u00b6 floxide-core : Contains the fundamental traits and structures for the framework Includes BaseNode , Flow , and BatchFlow implementations Provides the directed graph structure and core execution model Has minimal dependencies floxide-transform : Implements transformation node patterns Provides the TransformNode trait for data transformation Depends on Tokio for the async runtime Provides utilities for creating transformation workflows floxide-derive (optional): Provides procedural macros for code generation Simplifies common patterns through macros Makes the API more ergonomic floxide-test : Contains testing utilities and fixtures Provides mock implementations of framework components Simplifies writing tests for consumers of the framework Module Organization \u00b6 Within each crate, we will follow these module organization principles: Public API : Exposed through lib.rs with clear documentation Use the re-export pattern to provide a clean public API Versioned according to semver Internal Implementation : Organized in modules with a clear responsibility Private modules prefixed with underscore if not part of the public API Clear separation between public interfaces and internal details Tests : Unit tests in the same file as the code they test using #[cfg(test)] Integration tests in a separate tests/ directory Consequences \u00b6 Positive \u00b6 Modularity : Separate crates allow for focused concerns and clear dependencies Versioning : Each crate can evolve at its own pace Dependency Management : Consumers only need to depend on the crates they use Testability : Easier to write focused tests for each component Compilation Times : Smaller compilation units can improve development experience Negative \u00b6 Complexity : Multi-crate projects are more complex to manage Potential API Fragmentation : Need to ensure consistent patterns across crates Version Synchronization : Need to manage versions across interdependent crates Documentation : More effort to provide cohesive documentation across crates Alternatives Considered \u00b6 Single Crate Approach \u00b6 Pros : Simpler to manage All code in one place Easier to maintain API consistency Simpler dependency management for consumers Cons : Less flexibility for evolution Longer compile times as the project grows All consumers must take all features, even if not needed Could lead to monolithic design We chose the multi-crate approach to provide greater flexibility and maintainability, especially as the project grows. This aligns with Rust ecosystem practices seen in mature libraries like Tokio, Serde, and others. Feature-Based Single Crate \u00b6 Pros : Maintains single crate simplicity Allows optional features through Cargo features Provides some flexibility without multi-crate complexity Cons : Still results in longer compile times for the main crate Feature combinations can lead to complexity Less clear separation of concerns While feature-based configuration will still be used within individual crates, the multi-crate approach provides clearer boundaries and better separation of concerns.","title":"ADR-0002: Project Structure and Crate Organization"},{"location":"adrs/0002-project-structure-and-crate-organization/#adr-0002-project-structure-and-crate-organization","text":"","title":"ADR-0002: Project Structure and Crate Organization"},{"location":"adrs/0002-project-structure-and-crate-organization/#status","text":"Accepted","title":"Status"},{"location":"adrs/0002-project-structure-and-crate-organization/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0002-project-structure-and-crate-organization/#context","text":"As we begin development of the floxide framework, we need to determine how to structure the codebase. The way we organize our project will impact testability, maintainability, and the ability to evolve the codebase over time. It will also influence how consumers of our library will interact with it. Rust provides specific patterns and organization concepts such as crates, workspaces, and modules that we need to consider for optimal organization.","title":"Context"},{"location":"adrs/0002-project-structure-and-crate-organization/#decision","text":"We will organize the floxide framework as a Cargo workspace with multiple crates to provide modularity and separation of concerns.","title":"Decision"},{"location":"adrs/0002-project-structure-and-crate-organization/#workspace-structure","text":"The project will be structured as follows: floxide/ \u251c\u2500\u2500 Cargo.toml # Workspace manifest \u251c\u2500\u2500 crates/ \u2502 \u251c\u2500\u2500 floxide-core/ # Core traits and structures \u2502 \u2502 \u251c\u2500\u2500 Cargo.toml \u2502 \u2502 \u2514\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 floxide-transform/ # Transform node implementations \u2502 \u2502 \u251c\u2500\u2500 Cargo.toml \u2502 \u2502 \u2514\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 floxide-derive/ # Optional proc macros for code generation \u2502 \u2502 \u251c\u2500\u2500 Cargo.toml \u2502 \u2502 \u2514\u2500\u2500 src/ \u2502 \u2514\u2500\u2500 floxide-test/ # Test utilities and fixtures \u2502 \u251c\u2500\u2500 Cargo.toml \u2502 \u2514\u2500\u2500 src/ \u251c\u2500\u2500 examples/ # Example implementations \u2502 \u251c\u2500\u2500 Cargo.toml \u2502 \u2514\u2500\u2500 examples/ # Standard example files \u251c\u2500\u2500 benches/ # Performance benchmarks \u2502 \u251c\u2500\u2500 Cargo.toml \u2502 \u2514\u2500\u2500 src/ \u2514\u2500\u2500 docs/ # Documentation \u2514\u2500\u2500 adrs/ # Architectural Decision Records","title":"Workspace Structure"},{"location":"adrs/0002-project-structure-and-crate-organization/#core-crates","text":"floxide-core : Contains the fundamental traits and structures for the framework Includes BaseNode , Flow , and BatchFlow implementations Provides the directed graph structure and core execution model Has minimal dependencies floxide-transform : Implements transformation node patterns Provides the TransformNode trait for data transformation Depends on Tokio for the async runtime Provides utilities for creating transformation workflows floxide-derive (optional): Provides procedural macros for code generation Simplifies common patterns through macros Makes the API more ergonomic floxide-test : Contains testing utilities and fixtures Provides mock implementations of framework components Simplifies writing tests for consumers of the framework","title":"Core Crates"},{"location":"adrs/0002-project-structure-and-crate-organization/#module-organization","text":"Within each crate, we will follow these module organization principles: Public API : Exposed through lib.rs with clear documentation Use the re-export pattern to provide a clean public API Versioned according to semver Internal Implementation : Organized in modules with a clear responsibility Private modules prefixed with underscore if not part of the public API Clear separation between public interfaces and internal details Tests : Unit tests in the same file as the code they test using #[cfg(test)] Integration tests in a separate tests/ directory","title":"Module Organization"},{"location":"adrs/0002-project-structure-and-crate-organization/#consequences","text":"","title":"Consequences"},{"location":"adrs/0002-project-structure-and-crate-organization/#positive","text":"Modularity : Separate crates allow for focused concerns and clear dependencies Versioning : Each crate can evolve at its own pace Dependency Management : Consumers only need to depend on the crates they use Testability : Easier to write focused tests for each component Compilation Times : Smaller compilation units can improve development experience","title":"Positive"},{"location":"adrs/0002-project-structure-and-crate-organization/#negative","text":"Complexity : Multi-crate projects are more complex to manage Potential API Fragmentation : Need to ensure consistent patterns across crates Version Synchronization : Need to manage versions across interdependent crates Documentation : More effort to provide cohesive documentation across crates","title":"Negative"},{"location":"adrs/0002-project-structure-and-crate-organization/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0002-project-structure-and-crate-organization/#single-crate-approach","text":"Pros : Simpler to manage All code in one place Easier to maintain API consistency Simpler dependency management for consumers Cons : Less flexibility for evolution Longer compile times as the project grows All consumers must take all features, even if not needed Could lead to monolithic design We chose the multi-crate approach to provide greater flexibility and maintainability, especially as the project grows. This aligns with Rust ecosystem practices seen in mature libraries like Tokio, Serde, and others.","title":"Single Crate Approach"},{"location":"adrs/0002-project-structure-and-crate-organization/#feature-based-single-crate","text":"Pros : Maintains single crate simplicity Allows optional features through Cargo features Provides some flexibility without multi-crate complexity Cons : Still results in longer compile times for the main crate Feature combinations can lead to complexity Less clear separation of concerns While feature-based configuration will still be used within individual crates, the multi-crate approach provides clearer boundaries and better separation of concerns.","title":"Feature-Based Single Crate"},{"location":"adrs/0003-core-framework-abstractions/","text":"ADR-0003: Core Framework Abstractions \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 The floxide framework is designed as a directed graph workflow system that needs several key abstractions: A core node interface for workflow steps A retry mechanism that handles failures A directed graph structure for the workflow A batch processing capability for parallel execution To create a robust and flexible framework, we need to determine how to implement these abstractions in Rust, leveraging traits, enums, and Rust's ownership model. Decision \u00b6 We will implement the core abstractions of the floxide framework using a more idiomatic Rust approach that emphasizes clear ownership, strong typing, and composition over inheritance. Core Abstractions \u00b6 1. Action Type \u00b6 Instead of relying on string-based custom actions, we'll use a trait-based approach that allows users to define their own fully type-safe action types: /// Trait for types that can be used as actions in workflow transitions /// /// By implementing this trait for your own enums, you can define domain-specific /// actions that are fully type-safe at compile time. pub trait ActionType : Debug + Clone + PartialEq + Eq + Hash + Send + Sync + ' static {} /// Standard action types provided by the framework #[derive(Debug, Clone, PartialEq, Eq, Hash)] pub enum DefaultAction { /// Default transition to the next node Next , /// Successfully complete the workflow Complete , /// Signal an error condition Error , } impl ActionType for DefaultAction {} // Example of how users can define their own type-safe action types: // // #[derive(Debug, Clone, PartialEq, Eq, Hash)] // pub enum PaymentAction { // PaymentReceived, // PaymentDeclined, // RefundRequested, // RefundProcessed, // } // // impl ActionType for PaymentAction {} With this approach, users can define their own domain-specific action types that are fully checked at compile time, avoiding the runtime errors that could occur with string-based custom actions. Example: Order Processing Workflow \u00b6 Here's a practical example of how domain-specific action types can be used to model a real-world order processing workflow: // Define domain-specific action types for order processing #[derive(Debug, Clone, PartialEq, Eq, Hash)] pub enum OrderAction { Approved , Rejected , Shipped , Delivered , Returned , } impl ActionType for OrderAction {} // Create a context type to hold order data struct OrderContext { order_id : String , customer_id : String , items : Vec < OrderItem > , status : OrderStatus , // other order details... } // Create nodes for the workflow (implementation details omitted) fn create_order_node () -> impl Node < OrderContext , OrderAction > { // Implementation... # node ( | _ctx | async { Ok ( NodeOutcome :: Transition ( OrderAction :: Approved , ())) }) } fn validate_order_node () -> impl Node < OrderContext , OrderAction > { // Implementation... # node ( | ctx | async { # // Validate order logic... # if ctx . items . is_empty () { # Ok ( NodeOutcome :: Transition ( OrderAction :: Rejected , ())) # } else { # Ok ( NodeOutcome :: Transition ( OrderAction :: Approved , ())) # } # }) } fn process_order_node () -> impl Node < OrderContext , OrderAction > { // Implementation... # node ( | _ctx | async { Ok ( NodeOutcome :: Transition ( OrderAction :: Shipped , ())) }) } fn ship_order_node () -> impl Node < OrderContext , OrderAction > { // Implementation... # node ( | _ctx | async { Ok ( NodeOutcome :: Transition ( OrderAction :: Delivered , ())) }) } fn deliver_order_node () -> impl Node < OrderContext , OrderAction > { // Implementation... # node ( | _ctx | async { Ok ( NodeOutcome :: Complete (())) }) } fn reject_order_node () -> impl Node < OrderContext , OrderAction > { // Implementation... # node ( | _ctx | async { Ok ( NodeOutcome :: Complete (())) }) } // Create a function that builds and returns the complete workflow fn create_order_workflow () -> Workflow < OrderContext , OrderAction > { let mut workflow = Workflow :: new ( create_order_node ()); let validation_id = workflow . add_node ( validate_order_node ()); let processing_id = workflow . add_node ( process_order_node ()); let shipping_id = workflow . add_node ( ship_order_node ()); let delivery_id = workflow . add_node ( deliver_order_node ()); let rejection_id = workflow . add_node ( reject_order_node ()); // Connect the nodes with type-safe transitions workflow . connect ( workflow . entry_point , OrderAction :: Approved , validation_id ); workflow . connect ( validation_id , OrderAction :: Approved , processing_id ); workflow . connect ( validation_id , OrderAction :: Rejected , rejection_id ); workflow . connect ( processing_id , OrderAction :: Shipped , shipping_id ); workflow . connect ( shipping_id , OrderAction :: Delivered , delivery_id ); workflow } // Using the workflow async fn process_new_order ( order : Order ) -> Result < (), FloxideError > { let mut context = OrderContext :: from ( order ); let workflow = create_order_workflow (); workflow . execute ( & mut context ). await } This example demonstrates: Creating a domain-specific OrderAction enum with meaningful action names Building a workflow that uses these type-safe actions for transitions Clear self-documenting code where the action names express business logic Compiler-enforced correctness (e.g., can't accidentally use PaymentAction in an order workflow) 2. Node Outcome \u00b6 Instead of multiple lifecycle methods (prepare, execute, finalize), we'll use a single method with an enum return type that represents the outcome: /// The result of processing a node pub enum NodeOutcome < T , A = DefaultAction > where A : ActionType , { /// Node has completed processing with an output value Complete ( T ), /// Node wants to transition to another node via the specified action Transition ( A , T ), } 3. Node Trait \u00b6 The core node functionality is defined as a Rust trait with a single, clear processing method: /// Core trait representing a node in the workflow graph pub trait Node < Context , A = DefaultAction > where A : ActionType , { /// The output type produced by this node type Output ; /// Process this node with the given context async fn process ( & self , ctx : & mut Context ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > ; } 4. Workflow Structure \u00b6 Instead of nodes knowing their successors, we'll use a dedicated workflow structure to manage the graph: /// A unique identifier for nodes within a workflow #[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)] pub struct NodeId ( uuid :: Uuid ); impl NodeId { pub fn new () -> Self { Self ( uuid :: Uuid :: new_v4 ()) } } /// A workflow graph that connects nodes together pub struct Workflow < Context , A = DefaultAction > where A : ActionType , { nodes : HashMap < NodeId , Box < dyn Node < Context , A >>> , edges : HashMap < ( NodeId , A ), NodeId > , entry_point : NodeId , } impl < Context , A > Workflow < Context , A > where A : ActionType , { /// Create a new workflow with the specified entry point node pub fn new ( entry_node : impl Node < Context , A > + ' static ) -> Self { let entry_id = NodeId :: new (); let mut nodes = HashMap :: new (); nodes . insert ( entry_id , Box :: new ( entry_node )); Self { entry_point : entry_id , nodes , edges : HashMap :: new (), } } /// Add a node to the workflow and return its ID pub fn add_node ( & mut self , node : impl Node < Context , A > + ' static ) -> NodeId { let id = NodeId :: new (); self . nodes . insert ( id , Box :: new ( node )); id } /// Connect two nodes with a directed edge and an action pub fn connect ( & mut self , from : NodeId , action : A , to : NodeId ) -> & mut Self { self . edges . insert (( from , action ), to ); self } /// Execute the workflow with the provided context pub async fn execute ( & self , ctx : & mut Context ) -> Result < (), FloxideError > { let mut current = self . entry_point ; loop { let node = self . nodes . get ( & current ) . ok_or_else ( || FloxideError :: NodeNotFound ( format! ( \"{:?}\" , current ))) ? ; match node . process ( ctx ). await ? { NodeOutcome :: Complete ( _ ) => return Ok (()), NodeOutcome :: Transition ( action , _ ) => { current = * self . edges . get ( & ( current , action . clone ())) . ok_or_else ( || FloxideError :: EdgeNotFound ( format! ( \"{:?}\" , current ), format! ( \"{:?}\" , action ) )) ? ; } } } } } 5. Retry Mechanism \u00b6 We'll implement retry as a wrapper node that adds retry capability to any other node: /// Strategy for timing retries pub enum BackoffStrategy { /// No delay between retries Immediate , /// Fixed delay between retries Fixed ( Duration ), /// Exponential backoff with optional jitter Exponential { base_delay : Duration , max_delay : Duration , factor : f64 , jitter : bool , }, } /// Node wrapper that adds retry capability pub struct RetryNode < N > { inner : N , max_retries : usize , backoff_strategy : BackoffStrategy , } impl < Context , A , N > Node < Context , A > for RetryNode < N > where N : Node < Context , A > , A : ActionType , { type Output = N :: Output ; async fn process ( & self , ctx : & mut Context ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > { let mut attempts = 0 ; loop { match self . inner . process ( ctx ). await { Ok ( outcome ) => return Ok ( outcome ), Err ( err ) => { attempts += 1 ; if attempts >= self . max_retries { return Err ( err ); } match & self . backoff_strategy { BackoffStrategy :: Immediate => {}, BackoffStrategy :: Fixed ( duration ) => { tokio :: time :: sleep ( * duration ). await ; }, BackoffStrategy :: Exponential { base_delay , max_delay , factor , jitter } => { let backoff = base_delay . mul_f64 ( factor . powi ( attempts as i32 )); let capped_backoff = std :: cmp :: min ( backoff , * max_delay ); let actual_delay = if * jitter { let jitter_factor = rand :: random :: < f64 > () * 0.5 + 0.5 ; // 0.5 to 1.0 capped_backoff . mul_f64 ( jitter_factor ) } else { capped_backoff }; tokio :: time :: sleep ( actual_delay ). await ; } } } } } } } // Helper methods for creating retry nodes impl < N > RetryNode < N > { pub fn new ( inner : N , max_retries : usize ) -> Self { Self { inner , max_retries , backoff_strategy : BackoffStrategy :: Immediate , } } pub fn with_fixed_backoff ( inner : N , max_retries : usize , delay : Duration ) -> Self { Self { inner , max_retries , backoff_strategy : BackoffStrategy :: Fixed ( delay ), } } pub fn with_exponential_backoff ( inner : N , max_retries : usize , base_delay : Duration , max_delay : Duration , factor : f64 , jitter : bool , ) -> Self { Self { inner , max_retries , backoff_strategy : BackoffStrategy :: Exponential { base_delay , max_delay , factor , jitter , }, } } } 6. Batch Processing \u00b6 We'll implement batch processing as a specialized node that processes items in parallel: /// A node that processes a collection of items in parallel pub struct BatchNode < ItemNode , ItemType , Context , A = DefaultAction > where ItemNode : Node < Context , A > , A : ActionType , { item_node : ItemNode , parallelism : usize , _phantom : PhantomData < ( ItemType , Context , A ) > , } impl < ItemNode , ItemType , Context , A > BatchNode < ItemNode , ItemType , Context , A > where ItemNode : Node < Context , A > + Clone , A : ActionType , { pub fn new ( item_node : ItemNode , parallelism : usize ) -> Self { Self { item_node , parallelism , _phantom : PhantomData , } } } impl < ItemNode , ItemType , Context , A > Node < Context , A > for BatchNode < ItemNode , ItemType , Context , A > where ItemNode : Node < Context , A > + Clone + Send + Sync + ' static , ItemType : Send + Sync + ' static , Context : BatchContext < ItemType > + Send , A : ActionType , { type Output = Vec < Result < ItemNode :: Output , FloxideError >> ; async fn process ( & self , ctx : & mut Context ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > { let items = ctx . get_batch_items () ? ; let results = process_batch ( items , self . parallelism , | item | { let node = self . item_node . clone (); let mut item_ctx = ctx . create_item_context ( item ) ? ; async move { match node . process ( & mut item_ctx ). await { Ok ( NodeOutcome :: Complete ( output )) => Ok ( output ), Ok ( NodeOutcome :: Transition ( _ , output )) => Ok ( output ), Err ( e ) => Err ( e ), } } } ). await ; Ok ( NodeOutcome :: Complete ( results )) } } /// Helper trait for contexts that support batch processing pub trait BatchContext < T > { fn get_batch_items ( & self ) -> Result < Vec < T > , FloxideError > ; fn create_item_context ( & self , item : T ) -> Result < Self , FloxideError > where Self : Sized ; } async fn process_batch < T , F , Fut , R > ( items : Vec < T > , parallelism : usize , process_fn : F , ) -> Vec < Result < R , FloxideError >> where T : Send + ' static , F : Fn ( T ) -> Fut + Send + Sync + ' static , Fut : Future < Output = Result < R , FloxideError >> + Send , R : Send + ' static , { use futures :: stream ::{ self , StreamExt }; stream :: iter ( items ) . map ( | item | { let process = & process_fn ; async move { process ( item ). await } }) . buffer_unordered ( parallelism ) . collect :: < Vec < _ >> () . await } 7. Convenience Node Builders \u00b6 We'll provide helper functions to create nodes from closures: /// Create a simple node from an async function pub fn node < Context , A , T , F , Fut > ( f : F ) -> impl Node < Context , A , Output = T > where F : Fn ( & mut Context ) -> Fut + Send + Sync + ' static , Fut : Future < Output = Result < NodeOutcome < T , A > , FloxideError >> + Send + ' static , A : ActionType , T : ' static , { struct SimpleNode < F , T , Context , A > { func : F , _phantom : PhantomData < ( T , Context , A ) > , } impl < F , T , Context , A , Fut > Node < Context , A > for SimpleNode < F , T , Context , A > where F : Fn ( & mut Context ) -> Fut + Send + Sync + ' static , Fut : Future < Output = Result < NodeOutcome < T , A > , FloxideError >> + Send + ' static , A : ActionType , { type Output = T ; async fn process ( & self , ctx : & mut Context ) -> Result < NodeOutcome < T , A > , FloxideError > { ( self . func )( ctx ). await } } SimpleNode { func : f , _phantom : PhantomData , } } Type Safety and Composition \u00b6 This design emphasizes: Clear separation between the node behavior (the Node trait) and the graph structure (the Workflow struct) A single processing method instead of three lifecycle methods Strong typing with enums for outcomes and actions Composition through node wrappers rather than inheritance Powerful retry strategies with various backoff options Explicit node creation and connection rather than implicit knowledge of successors Type-safe custom action types through user-defined enums that implement the ActionType trait Consequences \u00b6 Positive \u00b6 Idiomatic Rust : Uses Rust's strengths like enums, traits, and composition Clear Ownership Model : Explicit about who owns what Simpler API : One method to implement instead of three Strong Type Safety : Makes invalid states unrepresentable Separation of Concerns : Nodes focus on processing, workflow manages connections Composable : Easy to wrap nodes with additional functionality Expressive : Outcome enums clearly express intents Builder Pattern : Fluent API for constructing workflows Type-Safe Actions : No string-based actions that could cause runtime errors Negative \u00b6 Learning Curve : Different conceptual model than some may be familiar with Serialization Complexity : Graph structure might be harder to serialize/deserialize Verbose Generics : Some implementations have complex type parameters Runtime Type Information : Still uses dynamic dispatch for heterogeneous nodes Multiple Action Types : Workflows can only use one action type, which might require conversion between different action types Alternatives Considered \u00b6 1. Multi-method Lifecycle Model \u00b6 Pros : Separate phases of execution are explicit Familiar pattern for those coming from OOP Cons : More complex to implement correctly Forces a specific execution model Less idiomatic in Rust 2. Self-referential Nodes \u00b6 Pros : Nodes can directly reference their successors No need for external graph structure Cons : Creates complex ownership issues in Rust Harder to serialize/deserialize Poor separation of concerns 3. Static Dispatch Approach \u00b6 Pros : Better performance No runtime overhead Cons : Much more complex type parameters Harder to compose nodes dynamically Limited heterogeneous collections 4. Channels-based Communication \u00b6 Pros : More actor-like model Better isolation between nodes Cons : More complex to reason about Harder to debug More overhead 5. String-Based Custom Actions \u00b6 Pros : More dynamic and flexible at runtime Easy to serialize/deserialize Cons : No compile-time type checking Prone to runtime errors from typos Less performant due to string comparison Not idiomatic Rust We chose the outcome-based node design with a separate workflow structure and user-defined action types because it provides a good balance of idiomatic Rust, type safety, and usability while maintaining the flexibility needed for a workflow system. The approach emphasizes composition over inheritance and makes excellent use of Rust's strengths in algebraic data types and ownership.","title":"ADR-0003: Core Framework Abstractions"},{"location":"adrs/0003-core-framework-abstractions/#adr-0003-core-framework-abstractions","text":"","title":"ADR-0003: Core Framework Abstractions"},{"location":"adrs/0003-core-framework-abstractions/#status","text":"Accepted","title":"Status"},{"location":"adrs/0003-core-framework-abstractions/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0003-core-framework-abstractions/#context","text":"The floxide framework is designed as a directed graph workflow system that needs several key abstractions: A core node interface for workflow steps A retry mechanism that handles failures A directed graph structure for the workflow A batch processing capability for parallel execution To create a robust and flexible framework, we need to determine how to implement these abstractions in Rust, leveraging traits, enums, and Rust's ownership model.","title":"Context"},{"location":"adrs/0003-core-framework-abstractions/#decision","text":"We will implement the core abstractions of the floxide framework using a more idiomatic Rust approach that emphasizes clear ownership, strong typing, and composition over inheritance.","title":"Decision"},{"location":"adrs/0003-core-framework-abstractions/#core-abstractions","text":"","title":"Core Abstractions"},{"location":"adrs/0003-core-framework-abstractions/#1-action-type","text":"Instead of relying on string-based custom actions, we'll use a trait-based approach that allows users to define their own fully type-safe action types: /// Trait for types that can be used as actions in workflow transitions /// /// By implementing this trait for your own enums, you can define domain-specific /// actions that are fully type-safe at compile time. pub trait ActionType : Debug + Clone + PartialEq + Eq + Hash + Send + Sync + ' static {} /// Standard action types provided by the framework #[derive(Debug, Clone, PartialEq, Eq, Hash)] pub enum DefaultAction { /// Default transition to the next node Next , /// Successfully complete the workflow Complete , /// Signal an error condition Error , } impl ActionType for DefaultAction {} // Example of how users can define their own type-safe action types: // // #[derive(Debug, Clone, PartialEq, Eq, Hash)] // pub enum PaymentAction { // PaymentReceived, // PaymentDeclined, // RefundRequested, // RefundProcessed, // } // // impl ActionType for PaymentAction {} With this approach, users can define their own domain-specific action types that are fully checked at compile time, avoiding the runtime errors that could occur with string-based custom actions.","title":"1. Action Type"},{"location":"adrs/0003-core-framework-abstractions/#example-order-processing-workflow","text":"Here's a practical example of how domain-specific action types can be used to model a real-world order processing workflow: // Define domain-specific action types for order processing #[derive(Debug, Clone, PartialEq, Eq, Hash)] pub enum OrderAction { Approved , Rejected , Shipped , Delivered , Returned , } impl ActionType for OrderAction {} // Create a context type to hold order data struct OrderContext { order_id : String , customer_id : String , items : Vec < OrderItem > , status : OrderStatus , // other order details... } // Create nodes for the workflow (implementation details omitted) fn create_order_node () -> impl Node < OrderContext , OrderAction > { // Implementation... # node ( | _ctx | async { Ok ( NodeOutcome :: Transition ( OrderAction :: Approved , ())) }) } fn validate_order_node () -> impl Node < OrderContext , OrderAction > { // Implementation... # node ( | ctx | async { # // Validate order logic... # if ctx . items . is_empty () { # Ok ( NodeOutcome :: Transition ( OrderAction :: Rejected , ())) # } else { # Ok ( NodeOutcome :: Transition ( OrderAction :: Approved , ())) # } # }) } fn process_order_node () -> impl Node < OrderContext , OrderAction > { // Implementation... # node ( | _ctx | async { Ok ( NodeOutcome :: Transition ( OrderAction :: Shipped , ())) }) } fn ship_order_node () -> impl Node < OrderContext , OrderAction > { // Implementation... # node ( | _ctx | async { Ok ( NodeOutcome :: Transition ( OrderAction :: Delivered , ())) }) } fn deliver_order_node () -> impl Node < OrderContext , OrderAction > { // Implementation... # node ( | _ctx | async { Ok ( NodeOutcome :: Complete (())) }) } fn reject_order_node () -> impl Node < OrderContext , OrderAction > { // Implementation... # node ( | _ctx | async { Ok ( NodeOutcome :: Complete (())) }) } // Create a function that builds and returns the complete workflow fn create_order_workflow () -> Workflow < OrderContext , OrderAction > { let mut workflow = Workflow :: new ( create_order_node ()); let validation_id = workflow . add_node ( validate_order_node ()); let processing_id = workflow . add_node ( process_order_node ()); let shipping_id = workflow . add_node ( ship_order_node ()); let delivery_id = workflow . add_node ( deliver_order_node ()); let rejection_id = workflow . add_node ( reject_order_node ()); // Connect the nodes with type-safe transitions workflow . connect ( workflow . entry_point , OrderAction :: Approved , validation_id ); workflow . connect ( validation_id , OrderAction :: Approved , processing_id ); workflow . connect ( validation_id , OrderAction :: Rejected , rejection_id ); workflow . connect ( processing_id , OrderAction :: Shipped , shipping_id ); workflow . connect ( shipping_id , OrderAction :: Delivered , delivery_id ); workflow } // Using the workflow async fn process_new_order ( order : Order ) -> Result < (), FloxideError > { let mut context = OrderContext :: from ( order ); let workflow = create_order_workflow (); workflow . execute ( & mut context ). await } This example demonstrates: Creating a domain-specific OrderAction enum with meaningful action names Building a workflow that uses these type-safe actions for transitions Clear self-documenting code where the action names express business logic Compiler-enforced correctness (e.g., can't accidentally use PaymentAction in an order workflow)","title":"Example: Order Processing Workflow"},{"location":"adrs/0003-core-framework-abstractions/#2-node-outcome","text":"Instead of multiple lifecycle methods (prepare, execute, finalize), we'll use a single method with an enum return type that represents the outcome: /// The result of processing a node pub enum NodeOutcome < T , A = DefaultAction > where A : ActionType , { /// Node has completed processing with an output value Complete ( T ), /// Node wants to transition to another node via the specified action Transition ( A , T ), }","title":"2. Node Outcome"},{"location":"adrs/0003-core-framework-abstractions/#3-node-trait","text":"The core node functionality is defined as a Rust trait with a single, clear processing method: /// Core trait representing a node in the workflow graph pub trait Node < Context , A = DefaultAction > where A : ActionType , { /// The output type produced by this node type Output ; /// Process this node with the given context async fn process ( & self , ctx : & mut Context ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > ; }","title":"3. Node Trait"},{"location":"adrs/0003-core-framework-abstractions/#4-workflow-structure","text":"Instead of nodes knowing their successors, we'll use a dedicated workflow structure to manage the graph: /// A unique identifier for nodes within a workflow #[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)] pub struct NodeId ( uuid :: Uuid ); impl NodeId { pub fn new () -> Self { Self ( uuid :: Uuid :: new_v4 ()) } } /// A workflow graph that connects nodes together pub struct Workflow < Context , A = DefaultAction > where A : ActionType , { nodes : HashMap < NodeId , Box < dyn Node < Context , A >>> , edges : HashMap < ( NodeId , A ), NodeId > , entry_point : NodeId , } impl < Context , A > Workflow < Context , A > where A : ActionType , { /// Create a new workflow with the specified entry point node pub fn new ( entry_node : impl Node < Context , A > + ' static ) -> Self { let entry_id = NodeId :: new (); let mut nodes = HashMap :: new (); nodes . insert ( entry_id , Box :: new ( entry_node )); Self { entry_point : entry_id , nodes , edges : HashMap :: new (), } } /// Add a node to the workflow and return its ID pub fn add_node ( & mut self , node : impl Node < Context , A > + ' static ) -> NodeId { let id = NodeId :: new (); self . nodes . insert ( id , Box :: new ( node )); id } /// Connect two nodes with a directed edge and an action pub fn connect ( & mut self , from : NodeId , action : A , to : NodeId ) -> & mut Self { self . edges . insert (( from , action ), to ); self } /// Execute the workflow with the provided context pub async fn execute ( & self , ctx : & mut Context ) -> Result < (), FloxideError > { let mut current = self . entry_point ; loop { let node = self . nodes . get ( & current ) . ok_or_else ( || FloxideError :: NodeNotFound ( format! ( \"{:?}\" , current ))) ? ; match node . process ( ctx ). await ? { NodeOutcome :: Complete ( _ ) => return Ok (()), NodeOutcome :: Transition ( action , _ ) => { current = * self . edges . get ( & ( current , action . clone ())) . ok_or_else ( || FloxideError :: EdgeNotFound ( format! ( \"{:?}\" , current ), format! ( \"{:?}\" , action ) )) ? ; } } } } }","title":"4. Workflow Structure"},{"location":"adrs/0003-core-framework-abstractions/#5-retry-mechanism","text":"We'll implement retry as a wrapper node that adds retry capability to any other node: /// Strategy for timing retries pub enum BackoffStrategy { /// No delay between retries Immediate , /// Fixed delay between retries Fixed ( Duration ), /// Exponential backoff with optional jitter Exponential { base_delay : Duration , max_delay : Duration , factor : f64 , jitter : bool , }, } /// Node wrapper that adds retry capability pub struct RetryNode < N > { inner : N , max_retries : usize , backoff_strategy : BackoffStrategy , } impl < Context , A , N > Node < Context , A > for RetryNode < N > where N : Node < Context , A > , A : ActionType , { type Output = N :: Output ; async fn process ( & self , ctx : & mut Context ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > { let mut attempts = 0 ; loop { match self . inner . process ( ctx ). await { Ok ( outcome ) => return Ok ( outcome ), Err ( err ) => { attempts += 1 ; if attempts >= self . max_retries { return Err ( err ); } match & self . backoff_strategy { BackoffStrategy :: Immediate => {}, BackoffStrategy :: Fixed ( duration ) => { tokio :: time :: sleep ( * duration ). await ; }, BackoffStrategy :: Exponential { base_delay , max_delay , factor , jitter } => { let backoff = base_delay . mul_f64 ( factor . powi ( attempts as i32 )); let capped_backoff = std :: cmp :: min ( backoff , * max_delay ); let actual_delay = if * jitter { let jitter_factor = rand :: random :: < f64 > () * 0.5 + 0.5 ; // 0.5 to 1.0 capped_backoff . mul_f64 ( jitter_factor ) } else { capped_backoff }; tokio :: time :: sleep ( actual_delay ). await ; } } } } } } } // Helper methods for creating retry nodes impl < N > RetryNode < N > { pub fn new ( inner : N , max_retries : usize ) -> Self { Self { inner , max_retries , backoff_strategy : BackoffStrategy :: Immediate , } } pub fn with_fixed_backoff ( inner : N , max_retries : usize , delay : Duration ) -> Self { Self { inner , max_retries , backoff_strategy : BackoffStrategy :: Fixed ( delay ), } } pub fn with_exponential_backoff ( inner : N , max_retries : usize , base_delay : Duration , max_delay : Duration , factor : f64 , jitter : bool , ) -> Self { Self { inner , max_retries , backoff_strategy : BackoffStrategy :: Exponential { base_delay , max_delay , factor , jitter , }, } } }","title":"5. Retry Mechanism"},{"location":"adrs/0003-core-framework-abstractions/#6-batch-processing","text":"We'll implement batch processing as a specialized node that processes items in parallel: /// A node that processes a collection of items in parallel pub struct BatchNode < ItemNode , ItemType , Context , A = DefaultAction > where ItemNode : Node < Context , A > , A : ActionType , { item_node : ItemNode , parallelism : usize , _phantom : PhantomData < ( ItemType , Context , A ) > , } impl < ItemNode , ItemType , Context , A > BatchNode < ItemNode , ItemType , Context , A > where ItemNode : Node < Context , A > + Clone , A : ActionType , { pub fn new ( item_node : ItemNode , parallelism : usize ) -> Self { Self { item_node , parallelism , _phantom : PhantomData , } } } impl < ItemNode , ItemType , Context , A > Node < Context , A > for BatchNode < ItemNode , ItemType , Context , A > where ItemNode : Node < Context , A > + Clone + Send + Sync + ' static , ItemType : Send + Sync + ' static , Context : BatchContext < ItemType > + Send , A : ActionType , { type Output = Vec < Result < ItemNode :: Output , FloxideError >> ; async fn process ( & self , ctx : & mut Context ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > { let items = ctx . get_batch_items () ? ; let results = process_batch ( items , self . parallelism , | item | { let node = self . item_node . clone (); let mut item_ctx = ctx . create_item_context ( item ) ? ; async move { match node . process ( & mut item_ctx ). await { Ok ( NodeOutcome :: Complete ( output )) => Ok ( output ), Ok ( NodeOutcome :: Transition ( _ , output )) => Ok ( output ), Err ( e ) => Err ( e ), } } } ). await ; Ok ( NodeOutcome :: Complete ( results )) } } /// Helper trait for contexts that support batch processing pub trait BatchContext < T > { fn get_batch_items ( & self ) -> Result < Vec < T > , FloxideError > ; fn create_item_context ( & self , item : T ) -> Result < Self , FloxideError > where Self : Sized ; } async fn process_batch < T , F , Fut , R > ( items : Vec < T > , parallelism : usize , process_fn : F , ) -> Vec < Result < R , FloxideError >> where T : Send + ' static , F : Fn ( T ) -> Fut + Send + Sync + ' static , Fut : Future < Output = Result < R , FloxideError >> + Send , R : Send + ' static , { use futures :: stream ::{ self , StreamExt }; stream :: iter ( items ) . map ( | item | { let process = & process_fn ; async move { process ( item ). await } }) . buffer_unordered ( parallelism ) . collect :: < Vec < _ >> () . await }","title":"6. Batch Processing"},{"location":"adrs/0003-core-framework-abstractions/#7-convenience-node-builders","text":"We'll provide helper functions to create nodes from closures: /// Create a simple node from an async function pub fn node < Context , A , T , F , Fut > ( f : F ) -> impl Node < Context , A , Output = T > where F : Fn ( & mut Context ) -> Fut + Send + Sync + ' static , Fut : Future < Output = Result < NodeOutcome < T , A > , FloxideError >> + Send + ' static , A : ActionType , T : ' static , { struct SimpleNode < F , T , Context , A > { func : F , _phantom : PhantomData < ( T , Context , A ) > , } impl < F , T , Context , A , Fut > Node < Context , A > for SimpleNode < F , T , Context , A > where F : Fn ( & mut Context ) -> Fut + Send + Sync + ' static , Fut : Future < Output = Result < NodeOutcome < T , A > , FloxideError >> + Send + ' static , A : ActionType , { type Output = T ; async fn process ( & self , ctx : & mut Context ) -> Result < NodeOutcome < T , A > , FloxideError > { ( self . func )( ctx ). await } } SimpleNode { func : f , _phantom : PhantomData , } }","title":"7. Convenience Node Builders"},{"location":"adrs/0003-core-framework-abstractions/#type-safety-and-composition","text":"This design emphasizes: Clear separation between the node behavior (the Node trait) and the graph structure (the Workflow struct) A single processing method instead of three lifecycle methods Strong typing with enums for outcomes and actions Composition through node wrappers rather than inheritance Powerful retry strategies with various backoff options Explicit node creation and connection rather than implicit knowledge of successors Type-safe custom action types through user-defined enums that implement the ActionType trait","title":"Type Safety and Composition"},{"location":"adrs/0003-core-framework-abstractions/#consequences","text":"","title":"Consequences"},{"location":"adrs/0003-core-framework-abstractions/#positive","text":"Idiomatic Rust : Uses Rust's strengths like enums, traits, and composition Clear Ownership Model : Explicit about who owns what Simpler API : One method to implement instead of three Strong Type Safety : Makes invalid states unrepresentable Separation of Concerns : Nodes focus on processing, workflow manages connections Composable : Easy to wrap nodes with additional functionality Expressive : Outcome enums clearly express intents Builder Pattern : Fluent API for constructing workflows Type-Safe Actions : No string-based actions that could cause runtime errors","title":"Positive"},{"location":"adrs/0003-core-framework-abstractions/#negative","text":"Learning Curve : Different conceptual model than some may be familiar with Serialization Complexity : Graph structure might be harder to serialize/deserialize Verbose Generics : Some implementations have complex type parameters Runtime Type Information : Still uses dynamic dispatch for heterogeneous nodes Multiple Action Types : Workflows can only use one action type, which might require conversion between different action types","title":"Negative"},{"location":"adrs/0003-core-framework-abstractions/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0003-core-framework-abstractions/#1-multi-method-lifecycle-model","text":"Pros : Separate phases of execution are explicit Familiar pattern for those coming from OOP Cons : More complex to implement correctly Forces a specific execution model Less idiomatic in Rust","title":"1. Multi-method Lifecycle Model"},{"location":"adrs/0003-core-framework-abstractions/#2-self-referential-nodes","text":"Pros : Nodes can directly reference their successors No need for external graph structure Cons : Creates complex ownership issues in Rust Harder to serialize/deserialize Poor separation of concerns","title":"2. Self-referential Nodes"},{"location":"adrs/0003-core-framework-abstractions/#3-static-dispatch-approach","text":"Pros : Better performance No runtime overhead Cons : Much more complex type parameters Harder to compose nodes dynamically Limited heterogeneous collections","title":"3. Static Dispatch Approach"},{"location":"adrs/0003-core-framework-abstractions/#4-channels-based-communication","text":"Pros : More actor-like model Better isolation between nodes Cons : More complex to reason about Harder to debug More overhead","title":"4. Channels-based Communication"},{"location":"adrs/0003-core-framework-abstractions/#5-string-based-custom-actions","text":"Pros : More dynamic and flexible at runtime Easy to serialize/deserialize Cons : No compile-time type checking Prone to runtime errors from typos Less performant due to string comparison Not idiomatic Rust We chose the outcome-based node design with a separate workflow structure and user-defined action types because it provides a good balance of idiomatic Rust, type safety, and usability while maintaining the flexibility needed for a workflow system. The approach emphasizes composition over inheritance and makes excellent use of Rust's strengths in algebraic data types and ownership.","title":"5. String-Based Custom Actions"},{"location":"adrs/0004-async-runtime-selection/","text":"ADR-0004: Async Runtime Selection \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 The floxide framework is designed around asynchronous operations to efficiently handle workflow execution. In Rust, async operations require an explicit runtime to execute futures. There are several viable async runtimes available in the Rust ecosystem, each with different trade-offs: Tokio : Full-featured, production-ready, widely adopted async-std : Similar to the standard library, focused on ergonomics smol : Small and simple runtime Custom runtimes: Roll-our-own or specialized solutions We need to select an appropriate async runtime that will support the framework's execution model, particularly for parallel node execution and orchestration. Decision \u00b6 We will use Tokio as the primary async runtime for the floxide framework implementation, with full feature set enabled. Additionally, we will use the async_trait crate for trait methods that return futures. Implementation Details \u00b6 Core Runtime Usage : // In floxide-transform crate pub fn run_flow < S , F > ( flow : F , shared_state : & mut S ) -> Result < (), FloxideError > where F : BaseNode < S > , { let rt = tokio :: runtime :: Runtime :: new () ? ; rt . block_on ( async { flow . run ( shared_state ). await }) } Async Trait Implementation : use async_trait :: async_trait ; #[async_trait] pub trait Node < Context , A = DefaultAction > where A : ActionType , { type Output ; async fn process ( & self , ctx : & mut Context ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > ; } Parallelism for BatchFlow : // In BatchFlow implementation async fn exec_core ( & self , prep_results : Vec < I > ) -> Result < Vec < Result < (), FloxideError >> , FloxideError > { let mut handles = Vec :: with_capacity ( prep_results . len ()); for item in prep_results { // Create a cloned shared state for each parallel execution let state_clone = /* clone shared state */ ; let start_node = self . flow . get_start_node () ? ; let handle = tokio :: spawn ( async move { start_node . run ( & mut state_clone ). await }); handles . push ( handle ); } let mut results = Vec :: with_capacity ( handles . len ()); for handle in handles { results . push ( handle . await . unwrap_or_else ( | e | Err ( FloxideError :: JoinError ( e . to_string ())))); } Ok ( results ) } Configurable Runtime Options : pub struct FloxideRuntimeConfig { worker_threads : Option < usize > , thread_name_prefix : String , thread_stack_size : Option < usize > , } impl Default for FloxideRuntimeConfig { fn default () -> Self { Self { worker_threads : None , // Use Tokio default thread_name_prefix : \"floxide-worker-\" . to_string (), thread_stack_size : None , // Use Tokio default } } } pub fn create_runtime ( config : FloxideRuntimeConfig ) -> Result < tokio :: runtime :: Runtime , FloxideError > { let mut rt_builder = tokio :: runtime :: Builder :: new_multi_thread (); if let Some ( threads ) = config . worker_threads { rt_builder . worker_threads ( threads ); } rt_builder . thread_name ( config . thread_name_prefix ); if let Some ( stack_size ) = config . thread_stack_size { rt_builder . thread_stack_size ( stack_size ); } rt_builder . enable_all () . build () . map_err ( | e | FloxideError :: RuntimeCreationError ( e . to_string ())) } Abstraction Layer : We will create a runtime abstraction layer in the floxide-transform crate This will allow for potential future runtime switching The public API will remain stable even if the underlying runtime changes Feature Flags \u00b6 We will use feature flags to allow custom runtime configuration: # In floxide-transform/Cargo.toml [features] default = [ \"tokio-full\" ] tokio-full = [ \"tokio/full\" ] tokio-minimal = [ \"tokio/rt\" , \"tokio/sync\" ] custom-runtime = [] [dependencies] tokio = { version = \"1.36\" , features = [ \"full\" ] } async-trait = \"0.1.77\" Consequences \u00b6 Positive \u00b6 Ecosystem Compatibility : Tokio is the most widely used async runtime in Rust, providing compatibility with a large ecosystem of libraries Production-Ready : Tokio is battle-tested and used in many production environments Feature-Rich : Includes timers, I/O utilities, and synchronization primitives that will be useful for the framework Active Maintenance : Actively developed and maintained Scalability : Well-suited for high-performance, concurrent workloads Async Trait Support : Using async_trait simplifies writing async methods in traits Negative \u00b6 Runtime Dependency : Creates a dependency on a specific runtime Binary Size : Tokio with full features adds to the binary size of applications using the framework Learning Curve : Tokio has its own patterns and concepts to learn Opinionated : Some design decisions in Tokio may not align perfectly with all use cases Macro Overhead : async_trait adds some runtime overhead compared to native async traits (which aren't stable yet) Alternatives Considered \u00b6 1. async-std \u00b6 Pros : Familiar API that mirrors the standard library Good documentation Focuses on ergonomics Cons : Less widely adopted than Tokio Smaller ecosystem of compatible libraries Some performance differences compared to Tokio 2. smol \u00b6 Pros : Minimal footprint Simple API Lightweight Cons : Less feature-rich Smaller ecosystem Less battle-tested in large-scale production environments 3. Runtime Agnostic Design \u00b6 Pros : Maximum flexibility for consumers No runtime dependency Cons : Significantly more complex implementation Would require extensive abstraction layers Would limit the use of runtime-specific features 4. Allow Pluggable Runtimes \u00b6 Pros : Flexibility for different environments Could adapt to special requirements Cons : Increased maintenance burden More complex API Testing complexity increases exponentially 5. Wait for native async traits \u00b6 Pros : No need for async_trait macro Better performance More idiomatic Rust Cons : Feature is not stable yet Would delay development Migration cost when the feature stabilizes We chose Tokio with full features and async_trait because it provides the best balance of features, ecosystem compatibility, and production readiness. The abstraction layer will help mitigate some of the downsides by allowing for potential future changes without disrupting the API.","title":"ADR-0004: Async Runtime Selection"},{"location":"adrs/0004-async-runtime-selection/#adr-0004-async-runtime-selection","text":"","title":"ADR-0004: Async Runtime Selection"},{"location":"adrs/0004-async-runtime-selection/#status","text":"Accepted","title":"Status"},{"location":"adrs/0004-async-runtime-selection/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0004-async-runtime-selection/#context","text":"The floxide framework is designed around asynchronous operations to efficiently handle workflow execution. In Rust, async operations require an explicit runtime to execute futures. There are several viable async runtimes available in the Rust ecosystem, each with different trade-offs: Tokio : Full-featured, production-ready, widely adopted async-std : Similar to the standard library, focused on ergonomics smol : Small and simple runtime Custom runtimes: Roll-our-own or specialized solutions We need to select an appropriate async runtime that will support the framework's execution model, particularly for parallel node execution and orchestration.","title":"Context"},{"location":"adrs/0004-async-runtime-selection/#decision","text":"We will use Tokio as the primary async runtime for the floxide framework implementation, with full feature set enabled. Additionally, we will use the async_trait crate for trait methods that return futures.","title":"Decision"},{"location":"adrs/0004-async-runtime-selection/#implementation-details","text":"Core Runtime Usage : // In floxide-transform crate pub fn run_flow < S , F > ( flow : F , shared_state : & mut S ) -> Result < (), FloxideError > where F : BaseNode < S > , { let rt = tokio :: runtime :: Runtime :: new () ? ; rt . block_on ( async { flow . run ( shared_state ). await }) } Async Trait Implementation : use async_trait :: async_trait ; #[async_trait] pub trait Node < Context , A = DefaultAction > where A : ActionType , { type Output ; async fn process ( & self , ctx : & mut Context ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > ; } Parallelism for BatchFlow : // In BatchFlow implementation async fn exec_core ( & self , prep_results : Vec < I > ) -> Result < Vec < Result < (), FloxideError >> , FloxideError > { let mut handles = Vec :: with_capacity ( prep_results . len ()); for item in prep_results { // Create a cloned shared state for each parallel execution let state_clone = /* clone shared state */ ; let start_node = self . flow . get_start_node () ? ; let handle = tokio :: spawn ( async move { start_node . run ( & mut state_clone ). await }); handles . push ( handle ); } let mut results = Vec :: with_capacity ( handles . len ()); for handle in handles { results . push ( handle . await . unwrap_or_else ( | e | Err ( FloxideError :: JoinError ( e . to_string ())))); } Ok ( results ) } Configurable Runtime Options : pub struct FloxideRuntimeConfig { worker_threads : Option < usize > , thread_name_prefix : String , thread_stack_size : Option < usize > , } impl Default for FloxideRuntimeConfig { fn default () -> Self { Self { worker_threads : None , // Use Tokio default thread_name_prefix : \"floxide-worker-\" . to_string (), thread_stack_size : None , // Use Tokio default } } } pub fn create_runtime ( config : FloxideRuntimeConfig ) -> Result < tokio :: runtime :: Runtime , FloxideError > { let mut rt_builder = tokio :: runtime :: Builder :: new_multi_thread (); if let Some ( threads ) = config . worker_threads { rt_builder . worker_threads ( threads ); } rt_builder . thread_name ( config . thread_name_prefix ); if let Some ( stack_size ) = config . thread_stack_size { rt_builder . thread_stack_size ( stack_size ); } rt_builder . enable_all () . build () . map_err ( | e | FloxideError :: RuntimeCreationError ( e . to_string ())) } Abstraction Layer : We will create a runtime abstraction layer in the floxide-transform crate This will allow for potential future runtime switching The public API will remain stable even if the underlying runtime changes","title":"Implementation Details"},{"location":"adrs/0004-async-runtime-selection/#feature-flags","text":"We will use feature flags to allow custom runtime configuration: # In floxide-transform/Cargo.toml [features] default = [ \"tokio-full\" ] tokio-full = [ \"tokio/full\" ] tokio-minimal = [ \"tokio/rt\" , \"tokio/sync\" ] custom-runtime = [] [dependencies] tokio = { version = \"1.36\" , features = [ \"full\" ] } async-trait = \"0.1.77\"","title":"Feature Flags"},{"location":"adrs/0004-async-runtime-selection/#consequences","text":"","title":"Consequences"},{"location":"adrs/0004-async-runtime-selection/#positive","text":"Ecosystem Compatibility : Tokio is the most widely used async runtime in Rust, providing compatibility with a large ecosystem of libraries Production-Ready : Tokio is battle-tested and used in many production environments Feature-Rich : Includes timers, I/O utilities, and synchronization primitives that will be useful for the framework Active Maintenance : Actively developed and maintained Scalability : Well-suited for high-performance, concurrent workloads Async Trait Support : Using async_trait simplifies writing async methods in traits","title":"Positive"},{"location":"adrs/0004-async-runtime-selection/#negative","text":"Runtime Dependency : Creates a dependency on a specific runtime Binary Size : Tokio with full features adds to the binary size of applications using the framework Learning Curve : Tokio has its own patterns and concepts to learn Opinionated : Some design decisions in Tokio may not align perfectly with all use cases Macro Overhead : async_trait adds some runtime overhead compared to native async traits (which aren't stable yet)","title":"Negative"},{"location":"adrs/0004-async-runtime-selection/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0004-async-runtime-selection/#1-async-std","text":"Pros : Familiar API that mirrors the standard library Good documentation Focuses on ergonomics Cons : Less widely adopted than Tokio Smaller ecosystem of compatible libraries Some performance differences compared to Tokio","title":"1. async-std"},{"location":"adrs/0004-async-runtime-selection/#2-smol","text":"Pros : Minimal footprint Simple API Lightweight Cons : Less feature-rich Smaller ecosystem Less battle-tested in large-scale production environments","title":"2. smol"},{"location":"adrs/0004-async-runtime-selection/#3-runtime-agnostic-design","text":"Pros : Maximum flexibility for consumers No runtime dependency Cons : Significantly more complex implementation Would require extensive abstraction layers Would limit the use of runtime-specific features","title":"3. Runtime Agnostic Design"},{"location":"adrs/0004-async-runtime-selection/#4-allow-pluggable-runtimes","text":"Pros : Flexibility for different environments Could adapt to special requirements Cons : Increased maintenance burden More complex API Testing complexity increases exponentially","title":"4. Allow Pluggable Runtimes"},{"location":"adrs/0004-async-runtime-selection/#5-wait-for-native-async-traits","text":"Pros : No need for async_trait macro Better performance More idiomatic Rust Cons : Feature is not stable yet Would delay development Migration cost when the feature stabilizes We chose Tokio with full features and async_trait because it provides the best balance of features, ecosystem compatibility, and production readiness. The abstraction layer will help mitigate some of the downsides by allowing for potential future changes without disrupting the API.","title":"5. Wait for native async traits"},{"location":"adrs/0005-state-serialization-deserialization/","text":"ADR-0005: State Serialization and Deserialization \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 A workflow system often needs to persist its state, either for long-running processes, to support restart after failure, or to enable distributed execution. The floxide framework needs a well-defined approach to serialize and deserialize workflow state, particularly: The workflow configuration (nodes and connections) The execution state (current position, context data) Partial execution results for resumability The design of serialization and deserialization affects: Persistence capabilities Workflow restartability Distributed execution State migration during version upgrades Performance of checkpointing Integration with external storage systems Decision \u00b6 We will implement a comprehensive serialization and deserialization strategy with the following components: 1. Context Serialization Trait \u00b6 We'll define a trait that contexts must implement to be serializable: /// Trait for contexts that can be serialized and deserialized pub trait SerializableContext : Send + Sync + ' static { /// Serialize this context to bytes fn serialize ( & self ) -> Result < Vec < u8 > , FloxideError > ; /// Deserialize from bytes into a context fn deserialize ( bytes : & [ u8 ]) -> Result < Self , FloxideError > where Self : Sized ; /// Get a serialization format identifier fn format_id ( & self ) -> & ' static str { \"json\" // Default implementation } } 2. Automatic Serde Implementation \u00b6 For contexts that use standard types, we'll provide a derive macro: #[derive(Serialize, Deserialize, SerializableContext)] pub struct MyWorkflowContext { // Fields... } The macro will implement SerializableContext using serde_json by default: impl SerializableContext for MyWorkflowContext { fn serialize ( & self ) -> Result < Vec < u8 > , FloxideError > { serde_json :: to_vec ( self ) . map_err ( | e | FloxideError :: SerializationError ( e . to_string ())) } fn deserialize ( bytes : & [ u8 ]) -> Result < Self , FloxideError > { serde_json :: from_slice ( bytes ) . map_err ( | e | FloxideError :: DeserializationError ( e . to_string ())) } } 3. Workflow State Snapshot \u00b6 We'll define a structure to represent the complete workflow state: pub struct WorkflowSnapshot < C , A = DefaultAction > where C : SerializableContext , A : ActionType , { /// The serialized context context_data : Vec < u8 > , /// Format identifier for the serialized context context_format : String , /// Current position in the workflow current_node_id : NodeId , /// Metadata about the snapshot metadata : HashMap < String , String > , /// Version information to support migrations version : String , /// Timestamp when the snapshot was created created_at : DateTime < Utc > , /// Hash of the workflow graph at the time of snapshot workflow_hash : String , } 4. Workflow Serialization and Deserialization \u00b6 The Workflow struct will have methods for serialization and deserialization: impl < C , A > Workflow < C , A > where C : SerializableContext , A : ActionType , { /// Create a snapshot of the current workflow state pub fn create_snapshot ( & self , context : & C ) -> Result < WorkflowSnapshot < C , A > , FloxideError > { // Implementation... } /// Resume execution from a snapshot pub async fn resume_from_snapshot ( & self , snapshot : WorkflowSnapshot < C , A > ) -> Result < (), FloxideError > { // Implementation... } /// Save a snapshot to a storage location pub async fn save_snapshot ( & self , context : & C , storage : & impl SnapshotStorage , ) -> Result < String , FloxideError > { let snapshot = self . create_snapshot ( context ) ? ; storage . store_snapshot ( & snapshot ). await } /// Load a snapshot from a storage location pub async fn load_and_resume ( & self , snapshot_id : & str , storage : & impl SnapshotStorage , ) -> Result < (), FloxideError > { let snapshot = storage . load_snapshot :: < C , A > ( snapshot_id ). await ? ; self . resume_from_snapshot ( snapshot ). await } } 5. Storage Abstraction \u00b6 We'll define a trait for snapshot storage backends: #[async_trait] pub trait SnapshotStorage : Send + Sync { /// Store a snapshot and return a unique identifier async fn store_snapshot < C , A > ( & self , snapshot : & WorkflowSnapshot < C , A > , ) -> Result < String , FloxideError > where C : SerializableContext , A : ActionType ; /// Load a snapshot by its identifier async fn load_snapshot < C , A > ( & self , id : & str , ) -> Result < WorkflowSnapshot < C , A > , FloxideError > where C : SerializableContext , A : ActionType ; /// List available snapshots async fn list_snapshots ( & self ) -> Result < Vec < SnapshotMetadata > , FloxideError > ; /// Delete a snapshot async fn delete_snapshot ( & self , id : & str ) -> Result < (), FloxideError > ; } 6. Built-in Storage Implementations \u00b6 We'll provide several built-in storage implementations: /// File system storage for snapshots pub struct FileSystemStorage { base_path : PathBuf , } /// In-memory storage for testing pub struct InMemoryStorage { snapshots : RwLock < HashMap < String , Vec < u8 >>> , } /// Redis storage implementation #[cfg(feature = \"redis-storage\" )] pub struct RedisStorage { client : redis :: Client , prefix : String , } 7. Checkpointing Support \u00b6 We'll add automatic checkpointing functionality: /// Configuration for automatic checkpointing pub struct CheckpointConfig { /// How often to create checkpoints (by node count or time) frequency : CheckpointFrequency , /// Storage backend to use storage : Box < dyn SnapshotStorage > , /// Whether to keep all checkpoints or only the latest keep_all : bool , } /// Extension to Workflow for checkpointing impl < C , A > Workflow < C , A > where C : SerializableContext , A : ActionType , { /// Enable automatic checkpointing during execution pub fn with_checkpointing ( mut self , config : CheckpointConfig ) -> Self { self . checkpoint_config = Some ( config ); self } } 8. Workflow Graph Serialization \u00b6 For serializing the workflow definition itself: impl < C , A > Workflow < C , A > where C : SerializableContext , A : ActionType + Serialize + DeserializeOwned , { /// Serialize the workflow definition pub fn serialize_definition ( & self ) -> Result < Vec < u8 > , FloxideError > { // Implementation... } /// Create a workflow from a serialized definition pub fn from_serialized_definition ( data : & [ u8 ]) -> Result < Self , FloxideError > { // Implementation... } } Consequences \u00b6 Positive \u00b6 Persistence : Workflows can be saved and resumed across process restarts Reliability : Failed workflows can be restarted from the latest checkpoint Distributed Execution : Workflow state can be transferred between different machines Flexibility : Multiple storage backends allow for different deployment scenarios Versioning : Support for migrations when workflow definitions change Transparency : Clear semantics for what happens during serialization and deserialization Performance Options : Users can choose serialization formats based on their needs Testability : In-memory storage simplifies testing of persistence scenarios Negative \u00b6 Increased Complexity : Adds complexity to the framework and API Serialization Constraints : Contexts must be serializable, which may limit what they can contain Additional Dependencies : May require additional crates like serde, uuid, chrono Performance Impact : Frequent checkpointing could impact workflow execution performance Storage Management : Users need to manage storage for snapshots (cleanup, etc.) Format Compatibility : May require migration tools for format changes Alternatives Considered \u00b6 1. Protocol Buffers / Cap'n Proto \u00b6 Pros : Schema evolution Compact binary format Cross-language compatibility Cons : More complex tooling Less flexible for dynamic types Additional build dependencies 2. Custom Binary Format \u00b6 Pros : Could be optimized for workflow-specific needs Potentially smaller size Cons : Significant implementation effort Limited ecosystem tools Harder to debug serialized data 3. Full Workflow Serialization \u00b6 Pros : Simpler conceptual model Could serialize node implementations too Cons : Much harder to implement correctly Would require significant restrictions on node implementations Less efficient for large workflows with little state 4. Event Sourcing Approach \u00b6 Pros : Complete history of workflow execution Ability to replay workflows Better auditability Cons : More complex implementation Higher storage requirements Potentially slower restoration for long workflows We chose the snapshot-based approach with a storage abstraction because it provides a good balance between flexibility, performance, and implementation complexity. The ability to plug in different storage backends and serialization formats allows users to tailor the persistence behavior to their specific needs.","title":"ADR-0005: State Serialization and Deserialization"},{"location":"adrs/0005-state-serialization-deserialization/#adr-0005-state-serialization-and-deserialization","text":"","title":"ADR-0005: State Serialization and Deserialization"},{"location":"adrs/0005-state-serialization-deserialization/#status","text":"Accepted","title":"Status"},{"location":"adrs/0005-state-serialization-deserialization/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0005-state-serialization-deserialization/#context","text":"A workflow system often needs to persist its state, either for long-running processes, to support restart after failure, or to enable distributed execution. The floxide framework needs a well-defined approach to serialize and deserialize workflow state, particularly: The workflow configuration (nodes and connections) The execution state (current position, context data) Partial execution results for resumability The design of serialization and deserialization affects: Persistence capabilities Workflow restartability Distributed execution State migration during version upgrades Performance of checkpointing Integration with external storage systems","title":"Context"},{"location":"adrs/0005-state-serialization-deserialization/#decision","text":"We will implement a comprehensive serialization and deserialization strategy with the following components:","title":"Decision"},{"location":"adrs/0005-state-serialization-deserialization/#1-context-serialization-trait","text":"We'll define a trait that contexts must implement to be serializable: /// Trait for contexts that can be serialized and deserialized pub trait SerializableContext : Send + Sync + ' static { /// Serialize this context to bytes fn serialize ( & self ) -> Result < Vec < u8 > , FloxideError > ; /// Deserialize from bytes into a context fn deserialize ( bytes : & [ u8 ]) -> Result < Self , FloxideError > where Self : Sized ; /// Get a serialization format identifier fn format_id ( & self ) -> & ' static str { \"json\" // Default implementation } }","title":"1. Context Serialization Trait"},{"location":"adrs/0005-state-serialization-deserialization/#2-automatic-serde-implementation","text":"For contexts that use standard types, we'll provide a derive macro: #[derive(Serialize, Deserialize, SerializableContext)] pub struct MyWorkflowContext { // Fields... } The macro will implement SerializableContext using serde_json by default: impl SerializableContext for MyWorkflowContext { fn serialize ( & self ) -> Result < Vec < u8 > , FloxideError > { serde_json :: to_vec ( self ) . map_err ( | e | FloxideError :: SerializationError ( e . to_string ())) } fn deserialize ( bytes : & [ u8 ]) -> Result < Self , FloxideError > { serde_json :: from_slice ( bytes ) . map_err ( | e | FloxideError :: DeserializationError ( e . to_string ())) } }","title":"2. Automatic Serde Implementation"},{"location":"adrs/0005-state-serialization-deserialization/#3-workflow-state-snapshot","text":"We'll define a structure to represent the complete workflow state: pub struct WorkflowSnapshot < C , A = DefaultAction > where C : SerializableContext , A : ActionType , { /// The serialized context context_data : Vec < u8 > , /// Format identifier for the serialized context context_format : String , /// Current position in the workflow current_node_id : NodeId , /// Metadata about the snapshot metadata : HashMap < String , String > , /// Version information to support migrations version : String , /// Timestamp when the snapshot was created created_at : DateTime < Utc > , /// Hash of the workflow graph at the time of snapshot workflow_hash : String , }","title":"3. Workflow State Snapshot"},{"location":"adrs/0005-state-serialization-deserialization/#4-workflow-serialization-and-deserialization","text":"The Workflow struct will have methods for serialization and deserialization: impl < C , A > Workflow < C , A > where C : SerializableContext , A : ActionType , { /// Create a snapshot of the current workflow state pub fn create_snapshot ( & self , context : & C ) -> Result < WorkflowSnapshot < C , A > , FloxideError > { // Implementation... } /// Resume execution from a snapshot pub async fn resume_from_snapshot ( & self , snapshot : WorkflowSnapshot < C , A > ) -> Result < (), FloxideError > { // Implementation... } /// Save a snapshot to a storage location pub async fn save_snapshot ( & self , context : & C , storage : & impl SnapshotStorage , ) -> Result < String , FloxideError > { let snapshot = self . create_snapshot ( context ) ? ; storage . store_snapshot ( & snapshot ). await } /// Load a snapshot from a storage location pub async fn load_and_resume ( & self , snapshot_id : & str , storage : & impl SnapshotStorage , ) -> Result < (), FloxideError > { let snapshot = storage . load_snapshot :: < C , A > ( snapshot_id ). await ? ; self . resume_from_snapshot ( snapshot ). await } }","title":"4. Workflow Serialization and Deserialization"},{"location":"adrs/0005-state-serialization-deserialization/#5-storage-abstraction","text":"We'll define a trait for snapshot storage backends: #[async_trait] pub trait SnapshotStorage : Send + Sync { /// Store a snapshot and return a unique identifier async fn store_snapshot < C , A > ( & self , snapshot : & WorkflowSnapshot < C , A > , ) -> Result < String , FloxideError > where C : SerializableContext , A : ActionType ; /// Load a snapshot by its identifier async fn load_snapshot < C , A > ( & self , id : & str , ) -> Result < WorkflowSnapshot < C , A > , FloxideError > where C : SerializableContext , A : ActionType ; /// List available snapshots async fn list_snapshots ( & self ) -> Result < Vec < SnapshotMetadata > , FloxideError > ; /// Delete a snapshot async fn delete_snapshot ( & self , id : & str ) -> Result < (), FloxideError > ; }","title":"5. Storage Abstraction"},{"location":"adrs/0005-state-serialization-deserialization/#6-built-in-storage-implementations","text":"We'll provide several built-in storage implementations: /// File system storage for snapshots pub struct FileSystemStorage { base_path : PathBuf , } /// In-memory storage for testing pub struct InMemoryStorage { snapshots : RwLock < HashMap < String , Vec < u8 >>> , } /// Redis storage implementation #[cfg(feature = \"redis-storage\" )] pub struct RedisStorage { client : redis :: Client , prefix : String , }","title":"6. Built-in Storage Implementations"},{"location":"adrs/0005-state-serialization-deserialization/#7-checkpointing-support","text":"We'll add automatic checkpointing functionality: /// Configuration for automatic checkpointing pub struct CheckpointConfig { /// How often to create checkpoints (by node count or time) frequency : CheckpointFrequency , /// Storage backend to use storage : Box < dyn SnapshotStorage > , /// Whether to keep all checkpoints or only the latest keep_all : bool , } /// Extension to Workflow for checkpointing impl < C , A > Workflow < C , A > where C : SerializableContext , A : ActionType , { /// Enable automatic checkpointing during execution pub fn with_checkpointing ( mut self , config : CheckpointConfig ) -> Self { self . checkpoint_config = Some ( config ); self } }","title":"7. Checkpointing Support"},{"location":"adrs/0005-state-serialization-deserialization/#8-workflow-graph-serialization","text":"For serializing the workflow definition itself: impl < C , A > Workflow < C , A > where C : SerializableContext , A : ActionType + Serialize + DeserializeOwned , { /// Serialize the workflow definition pub fn serialize_definition ( & self ) -> Result < Vec < u8 > , FloxideError > { // Implementation... } /// Create a workflow from a serialized definition pub fn from_serialized_definition ( data : & [ u8 ]) -> Result < Self , FloxideError > { // Implementation... } }","title":"8. Workflow Graph Serialization"},{"location":"adrs/0005-state-serialization-deserialization/#consequences","text":"","title":"Consequences"},{"location":"adrs/0005-state-serialization-deserialization/#positive","text":"Persistence : Workflows can be saved and resumed across process restarts Reliability : Failed workflows can be restarted from the latest checkpoint Distributed Execution : Workflow state can be transferred between different machines Flexibility : Multiple storage backends allow for different deployment scenarios Versioning : Support for migrations when workflow definitions change Transparency : Clear semantics for what happens during serialization and deserialization Performance Options : Users can choose serialization formats based on their needs Testability : In-memory storage simplifies testing of persistence scenarios","title":"Positive"},{"location":"adrs/0005-state-serialization-deserialization/#negative","text":"Increased Complexity : Adds complexity to the framework and API Serialization Constraints : Contexts must be serializable, which may limit what they can contain Additional Dependencies : May require additional crates like serde, uuid, chrono Performance Impact : Frequent checkpointing could impact workflow execution performance Storage Management : Users need to manage storage for snapshots (cleanup, etc.) Format Compatibility : May require migration tools for format changes","title":"Negative"},{"location":"adrs/0005-state-serialization-deserialization/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0005-state-serialization-deserialization/#1-protocol-buffers-capn-proto","text":"Pros : Schema evolution Compact binary format Cross-language compatibility Cons : More complex tooling Less flexible for dynamic types Additional build dependencies","title":"1. Protocol Buffers / Cap'n Proto"},{"location":"adrs/0005-state-serialization-deserialization/#2-custom-binary-format","text":"Pros : Could be optimized for workflow-specific needs Potentially smaller size Cons : Significant implementation effort Limited ecosystem tools Harder to debug serialized data","title":"2. Custom Binary Format"},{"location":"adrs/0005-state-serialization-deserialization/#3-full-workflow-serialization","text":"Pros : Simpler conceptual model Could serialize node implementations too Cons : Much harder to implement correctly Would require significant restrictions on node implementations Less efficient for large workflows with little state","title":"3. Full Workflow Serialization"},{"location":"adrs/0005-state-serialization-deserialization/#4-event-sourcing-approach","text":"Pros : Complete history of workflow execution Ability to replay workflows Better auditability Cons : More complex implementation Higher storage requirements Potentially slower restoration for long workflows We chose the snapshot-based approach with a storage abstraction because it provides a good balance between flexibility, performance, and implementation complexity. The ability to plug in different storage backends and serialization formats allows users to tailor the persistence behavior to their specific needs.","title":"4. Event Sourcing Approach"},{"location":"adrs/0006-workflow-observability/","text":"ADR-0006: Workflow Observability \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 Observability is crucial for workflow systems, especially for complex workflows or in production environments. Users need visibility into: What is happening during workflow execution Where a workflow is in its execution lifecycle How long individual steps are taking What errors occurred and why The overall performance characteristics of workflows A comprehensive observability system is essential for: Debugging workflow issues Monitoring production workflows Understanding performance bottlenecks Auditing workflow execution Visualizing workflow execution Alerting on workflow failures or delays Decision \u00b6 We will implement an observability system for the floxide framework with OpenTelemetry as the primary integration point, complemented by additional observability mechanisms. 1. OpenTelemetry as Primary Observability Solution \u00b6 OpenTelemetry will be the main framework for observability: /// Core OpenTelemetry integration for the floxide framework pub struct FloxideOtel { tracer : opentelemetry :: trace :: Tracer , meter : opentelemetry :: metrics :: Meter , attributes : HashMap < String , String > , } impl FloxideOtel { /// Create a new OpenTelemetry integration with the default configuration pub fn new ( service_name : & str ) -> Result < Self , FloxideError > { // Set up OpenTelemetry with appropriate exporters let tracer_provider = opentelemetry_sdk :: trace :: TracerProvider :: builder () . with_simple_processor ( opentelemetry_sdk :: trace :: BatchSpanProcessor :: new ( opentelemetry_otlp :: new_exporter () . tonic () . build () ? , )) . build (); let tracer = tracer_provider . tracer ( service_name ); let meter_provider = opentelemetry_sdk :: metrics :: MeterProvider :: builder () . with_reader ( opentelemetry_sdk :: metrics :: reader :: Builder :: default () . with_exporter ( opentelemetry_otlp :: new_exporter () . tonic () . build () ? , ) . build (), ) . build (); let meter = meter_provider . meter ( service_name ); Ok ( Self { tracer , meter , attributes : HashMap :: new (), }) } /// Add a common attribute to all spans and metrics pub fn with_attribute ( mut self , key : & str , value : & str ) -> Self { self . attributes . insert ( key . to_string (), value . to_string ()); self } } 2. Event Emission System \u00b6 The core of our observability design will be event emission that integrates with OpenTelemetry: /// Events emitted during workflow execution #[derive(Clone, Debug)] pub enum WorkflowEvent < C , A = DefaultAction > where C : ' static , A : ActionType , { /// Workflow execution started WorkflowStarted { workflow_id : String , timestamp : DateTime < Utc > , metadata : HashMap < String , String > , }, /// Workflow execution completed WorkflowCompleted { workflow_id : String , timestamp : DateTime < Utc > , execution_time_ms : u64 , metadata : HashMap < String , String > , }, /// Workflow execution failed WorkflowFailed { workflow_id : String , timestamp : DateTime < Utc > , error : FloxideError , metadata : HashMap < String , String > , }, /// Node processing started NodeStarted { workflow_id : String , node_id : NodeId , node_type : String , timestamp : DateTime < Utc > , metadata : HashMap < String , String > , }, /// Node processing completed NodeCompleted { workflow_id : String , node_id : NodeId , node_type : String , timestamp : DateTime < Utc > , execution_time_ms : u64 , outcome : NodeOutcomeType < A > , metadata : HashMap < String , String > , }, /// Node processing failed NodeFailed { workflow_id : String , node_id : NodeId , node_type : String , timestamp : DateTime < Utc > , error : FloxideError , metadata : HashMap < String , String > , }, /// Transition between nodes Transition { workflow_id : String , from_node_id : NodeId , to_node_id : NodeId , action : A , timestamp : DateTime < Utc > , metadata : HashMap < String , String > , }, /// Retry attempt occurred RetryAttempt { workflow_id : String , node_id : NodeId , attempt_number : usize , reason : String , timestamp : DateTime < Utc > , backoff_ms : u64 , metadata : HashMap < String , String > , }, /// Checkpoint created CheckpointCreated { workflow_id : String , checkpoint_id : String , timestamp : DateTime < Utc > , metadata : HashMap < String , String > , }, /// Custom application event Custom { workflow_id : String , event_type : String , payload : Value , timestamp : DateTime < Utc > , metadata : HashMap < String , String > , }, } /// Type of node outcome for events (without the actual output value) #[derive(Clone, Debug)] pub enum NodeOutcomeType < A > where A : ActionType , { /// Node completed Complete , /// Node transitioned with the specified action Transition ( A ), } 3. OpenTelemetry Observer Implementation \u00b6 The primary observer implementation will use OpenTelemetry: /// Observer that emits events to OpenTelemetry pub struct OpenTelemetryObserver { otel : FloxideOtel , } impl OpenTelemetryObserver { /// Create a new OpenTelemetry observer pub fn new ( otel : FloxideOtel ) -> Self { Self { otel } } } #[async_trait] impl < C , A > WorkflowObserver < C , A > for OpenTelemetryObserver where C : ' static , A : ActionType , { async fn on_event ( & self , event : WorkflowEvent < C , A > ) -> Result < (), FloxideError > { match & event { WorkflowEvent :: WorkflowStarted { workflow_id , metadata , .. } => { let mut span = self . otel . tracer . span_builder ( format! ( \"workflow:{}\" , workflow_id )) . with_kind ( SpanKind :: Internal ) . start ( & self . otel . tracer ); for ( key , value ) in metadata { span . set_attribute ( KeyValue :: new ( key . clone (), value . clone ())); } // Set workflow ID as a global context for child spans let cx = Context :: current_with_span ( span ); cx . attach (); } // Handle other event types with appropriate spans and metrics // ... } Ok (()) } } 4. Additional Observer Implementations \u00b6 We'll also provide complementary observers for specific use cases: /// Observer that logs events using the tracing crate pub struct TracingObserver { min_level : Level , } /// In-memory observer for testing or UI visualization pub struct InMemoryObserver { events : RwLock < Vec < WorkflowEvent >> , max_events : usize , } /// Observer that publishes events to a channel pub struct ChannelObserver < C , A = DefaultAction > where C : ' static , A : ActionType , { sender : mpsc :: Sender < WorkflowEvent < C , A >> , } 5. Observer Registry with OpenTelemetry Default \u00b6 The workflow will have a registry of observers with OpenTelemetry as the default: impl < C , A > Workflow < C , A > where C : ' static , A : ActionType , { /// Add OpenTelemetry observability to this workflow pub fn with_opentelemetry ( mut self , service_name : & str ) -> Result < Self , FloxideError > { let otel = FloxideOtel :: new ( service_name ) ? ; self . observers . push ( Box :: new ( OpenTelemetryObserver :: new ( otel ))); Ok ( self ) } /// Add a custom observer to this workflow pub fn add_observer < O > ( & mut self , observer : O ) -> & mut Self where O : WorkflowObserver < C , A > + ' static , { self . observers . push ( Box :: new ( observer )); self } } 6. Distributed Tracing with OpenTelemetry \u00b6 We'll implement specific tracing support for workflow execution: impl < C , A > Workflow < C , A > where C : ' static , A : ActionType , { async fn execute_with_tracing ( & self , ctx : & mut C ) -> Result < (), FloxideError > { // Create a workflow execution span let tracer = opentelemetry :: global :: tracer ( \"floxide\" ); let workflow_span = tracer . start ( format! ( \"workflow:{}\" , self . id ())); let cx = Context :: current_with_span ( workflow_span ); // Set the current context for propagation let _guard = cx . attach (); let start = Instant :: now (); for observer in & self . observers { observer . on_start ( self , ctx ). await ? ; } let result = self . execute_internal ( ctx ). await ; let execution_time = start . elapsed (). as_millis () as u64 ; // Record metrics let meter = opentelemetry :: global :: meter ( \"floxide\" ); let workflow_duration = meter . f64_histogram ( \"workflow.duration\" ) . with_description ( \"Workflow execution duration in milliseconds\" ) . init (); workflow_duration . record ( execution_time as f64 , & [ KeyValue :: new ( \"workflow_id\" , self . id (). to_string ())], ); match & result { Ok ( _ ) => { workflow_span . set_status ( Status :: Ok ); for observer in & self . observers { observer . on_complete ( self , execution_time ). await ? ; } } Err ( err ) => { workflow_span . set_status ( Status :: Error ); workflow_span . record_error ( err ); for observer in & self . observers { observer . on_failure ( self , err ). await ? ; } } } result } } 7. OpenTelemetry-Based Visualization \u00b6 We'll leverage OpenTelemetry for visualization capabilities: /// Generate a visualization of workflow execution using OpenTelemetry spans pub async fn generate_workflow_visualization ( workflow_id : & str , trace_exporter_endpoint : & str , ) -> Result < String , FloxideError > { // Query OpenTelemetry backend for the trace data // and generate visualization // ... } 8. Context-Aware Logging with OpenTelemetry Integration \u00b6 We'll enhance nodes with OpenTelemetry-integrated logging capabilities: /// Extension trait for context to add observability capabilities pub trait ObservableContext { /// Get a context identifier for observability fn context_id ( & self ) -> String ; /// Get additional context attributes for OpenTelemetry fn otel_attributes ( & self ) -> HashMap < String , String > { HashMap :: new () } } /// OpenTelemetry instrumentation for nodes impl < Context , A > Node < Context , A > for OtelNodeWrapper < N , Context , A > where N : Node < Context , A > , Context : ObservableContext , A : ActionType , { type Output = N :: Output ; async fn process ( & self , ctx : & mut Context ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > { let context_id = ctx . context_id (); let attributes = ctx . otel_attributes (); let tracer = opentelemetry :: global :: tracer ( \"floxide\" ); let mut span_builder = tracer . span_builder ( format! ( \"node:{}\" , self . node_type )); // Add attributes to the span for ( key , value ) in & attributes { span_builder = span_builder . with_attribute ( KeyValue :: new ( key . clone (), value . clone ())); } span_builder = span_builder . with_attribute ( KeyValue :: new ( \"context_id\" , context_id . clone ())); span_builder = span_builder . with_attribute ( KeyValue :: new ( \"node_type\" , self . node_type . clone ())); let span = span_builder . start ( & tracer ); let cx = Context :: current_with_span ( span ); let _guard = cx . attach (); let start = Instant :: now (); let result = self . inner . process ( ctx ). await ; let duration = start . elapsed (); // Record node execution metrics let meter = opentelemetry :: global :: meter ( \"floxide\" ); let node_duration = meter . f64_histogram ( \"node.duration\" ) . with_description ( \"Node execution duration in milliseconds\" ) . init (); node_duration . record ( duration . as_millis () as f64 , & [ KeyValue :: new ( \"node_type\" , self . node_type . clone ())], ); match & result { Ok ( outcome ) => { let outcome_type = match outcome { NodeOutcome :: Complete ( _ ) => \"complete\" , NodeOutcome :: Transition ( action , _ ) => { span . set_attribute ( KeyValue :: new ( \"transition_action\" , format! ( \"{:?}\" , action ))); \"transition\" } }; span . set_attribute ( KeyValue :: new ( \"outcome\" , outcome_type . to_string ())); span . set_attribute ( KeyValue :: new ( \"duration_ms\" , duration . as_millis () as i64 )); span . set_status ( Status :: Ok ); } Err ( err ) => { span . set_attribute ( KeyValue :: new ( \"duration_ms\" , duration . as_millis () as i64 )); span . set_status ( Status :: Error ); span . record_error ( err ); } } result } } Consequences \u00b6 Positive \u00b6 Complete Observability : OpenTelemetry provides a comprehensive solution for traces, metrics, and logs Standard Integration : Follows industry standards for observability Ecosystem Compatibility : OpenTelemetry supports many backends (Jaeger, Prometheus, etc.) Distributed Tracing : Built-in support for tracing across service boundaries Minimal Overhead : OpenTelemetry is designed for production use with minimal impact Visualization Options : Can leverage existing OpenTelemetry visualization tools Context Propagation : Supports propagating context across async boundaries Negative \u00b6 Dependency Size : OpenTelemetry adds significant dependencies to the project Configuration Complexity : Properly configuring OpenTelemetry requires additional expertise Learning Curve : Using OpenTelemetry effectively requires understanding its concepts Backend Requirements : Requires setting up and maintaining OpenTelemetry backends Additional Resource Usage : Collecting and exporting telemetry data uses system resources Alternatives Considered \u00b6 1. Simple Logging Approach \u00b6 Pros : Simpler implementation Fewer dependencies Cons : Limited visibility into workflow execution No standardized way to analyze the data No metrics collection 2. Custom Metrics Solution \u00b6 Pros : Could be more tailored to workflow-specific needs Potentially lower overhead for specific metrics Cons : Would require maintaining a custom solution No standardized integration with other systems Limited ecosystem tools 3. Multiple Separate Systems (logs, metrics, traces) \u00b6 Pros : Could choose best-of-breed for each concern More flexibility in implementation Cons : No unified observability model More complex integration points Harder to correlate data across systems 4. No Built-in Observability \u00b6 Pros : Simpler framework Lower dependency footprint Cons : Users would need to implement their own solutions Inconsistent observability implementations Poor developer experience We chose OpenTelemetry as our primary observability solution because it provides a comprehensive, standards-based approach to observability that covers traces, metrics, and logs. It offers excellent ecosystem compatibility while maintaining reasonable performance characteristics. The event-based observer pattern allows us to integrate OpenTelemetry seamlessly while still supporting additional observability mechanisms for specific use cases.","title":"ADR-0006: Workflow Observability"},{"location":"adrs/0006-workflow-observability/#adr-0006-workflow-observability","text":"","title":"ADR-0006: Workflow Observability"},{"location":"adrs/0006-workflow-observability/#status","text":"Accepted","title":"Status"},{"location":"adrs/0006-workflow-observability/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0006-workflow-observability/#context","text":"Observability is crucial for workflow systems, especially for complex workflows or in production environments. Users need visibility into: What is happening during workflow execution Where a workflow is in its execution lifecycle How long individual steps are taking What errors occurred and why The overall performance characteristics of workflows A comprehensive observability system is essential for: Debugging workflow issues Monitoring production workflows Understanding performance bottlenecks Auditing workflow execution Visualizing workflow execution Alerting on workflow failures or delays","title":"Context"},{"location":"adrs/0006-workflow-observability/#decision","text":"We will implement an observability system for the floxide framework with OpenTelemetry as the primary integration point, complemented by additional observability mechanisms.","title":"Decision"},{"location":"adrs/0006-workflow-observability/#1-opentelemetry-as-primary-observability-solution","text":"OpenTelemetry will be the main framework for observability: /// Core OpenTelemetry integration for the floxide framework pub struct FloxideOtel { tracer : opentelemetry :: trace :: Tracer , meter : opentelemetry :: metrics :: Meter , attributes : HashMap < String , String > , } impl FloxideOtel { /// Create a new OpenTelemetry integration with the default configuration pub fn new ( service_name : & str ) -> Result < Self , FloxideError > { // Set up OpenTelemetry with appropriate exporters let tracer_provider = opentelemetry_sdk :: trace :: TracerProvider :: builder () . with_simple_processor ( opentelemetry_sdk :: trace :: BatchSpanProcessor :: new ( opentelemetry_otlp :: new_exporter () . tonic () . build () ? , )) . build (); let tracer = tracer_provider . tracer ( service_name ); let meter_provider = opentelemetry_sdk :: metrics :: MeterProvider :: builder () . with_reader ( opentelemetry_sdk :: metrics :: reader :: Builder :: default () . with_exporter ( opentelemetry_otlp :: new_exporter () . tonic () . build () ? , ) . build (), ) . build (); let meter = meter_provider . meter ( service_name ); Ok ( Self { tracer , meter , attributes : HashMap :: new (), }) } /// Add a common attribute to all spans and metrics pub fn with_attribute ( mut self , key : & str , value : & str ) -> Self { self . attributes . insert ( key . to_string (), value . to_string ()); self } }","title":"1. OpenTelemetry as Primary Observability Solution"},{"location":"adrs/0006-workflow-observability/#2-event-emission-system","text":"The core of our observability design will be event emission that integrates with OpenTelemetry: /// Events emitted during workflow execution #[derive(Clone, Debug)] pub enum WorkflowEvent < C , A = DefaultAction > where C : ' static , A : ActionType , { /// Workflow execution started WorkflowStarted { workflow_id : String , timestamp : DateTime < Utc > , metadata : HashMap < String , String > , }, /// Workflow execution completed WorkflowCompleted { workflow_id : String , timestamp : DateTime < Utc > , execution_time_ms : u64 , metadata : HashMap < String , String > , }, /// Workflow execution failed WorkflowFailed { workflow_id : String , timestamp : DateTime < Utc > , error : FloxideError , metadata : HashMap < String , String > , }, /// Node processing started NodeStarted { workflow_id : String , node_id : NodeId , node_type : String , timestamp : DateTime < Utc > , metadata : HashMap < String , String > , }, /// Node processing completed NodeCompleted { workflow_id : String , node_id : NodeId , node_type : String , timestamp : DateTime < Utc > , execution_time_ms : u64 , outcome : NodeOutcomeType < A > , metadata : HashMap < String , String > , }, /// Node processing failed NodeFailed { workflow_id : String , node_id : NodeId , node_type : String , timestamp : DateTime < Utc > , error : FloxideError , metadata : HashMap < String , String > , }, /// Transition between nodes Transition { workflow_id : String , from_node_id : NodeId , to_node_id : NodeId , action : A , timestamp : DateTime < Utc > , metadata : HashMap < String , String > , }, /// Retry attempt occurred RetryAttempt { workflow_id : String , node_id : NodeId , attempt_number : usize , reason : String , timestamp : DateTime < Utc > , backoff_ms : u64 , metadata : HashMap < String , String > , }, /// Checkpoint created CheckpointCreated { workflow_id : String , checkpoint_id : String , timestamp : DateTime < Utc > , metadata : HashMap < String , String > , }, /// Custom application event Custom { workflow_id : String , event_type : String , payload : Value , timestamp : DateTime < Utc > , metadata : HashMap < String , String > , }, } /// Type of node outcome for events (without the actual output value) #[derive(Clone, Debug)] pub enum NodeOutcomeType < A > where A : ActionType , { /// Node completed Complete , /// Node transitioned with the specified action Transition ( A ), }","title":"2. Event Emission System"},{"location":"adrs/0006-workflow-observability/#3-opentelemetry-observer-implementation","text":"The primary observer implementation will use OpenTelemetry: /// Observer that emits events to OpenTelemetry pub struct OpenTelemetryObserver { otel : FloxideOtel , } impl OpenTelemetryObserver { /// Create a new OpenTelemetry observer pub fn new ( otel : FloxideOtel ) -> Self { Self { otel } } } #[async_trait] impl < C , A > WorkflowObserver < C , A > for OpenTelemetryObserver where C : ' static , A : ActionType , { async fn on_event ( & self , event : WorkflowEvent < C , A > ) -> Result < (), FloxideError > { match & event { WorkflowEvent :: WorkflowStarted { workflow_id , metadata , .. } => { let mut span = self . otel . tracer . span_builder ( format! ( \"workflow:{}\" , workflow_id )) . with_kind ( SpanKind :: Internal ) . start ( & self . otel . tracer ); for ( key , value ) in metadata { span . set_attribute ( KeyValue :: new ( key . clone (), value . clone ())); } // Set workflow ID as a global context for child spans let cx = Context :: current_with_span ( span ); cx . attach (); } // Handle other event types with appropriate spans and metrics // ... } Ok (()) } }","title":"3. OpenTelemetry Observer Implementation"},{"location":"adrs/0006-workflow-observability/#4-additional-observer-implementations","text":"We'll also provide complementary observers for specific use cases: /// Observer that logs events using the tracing crate pub struct TracingObserver { min_level : Level , } /// In-memory observer for testing or UI visualization pub struct InMemoryObserver { events : RwLock < Vec < WorkflowEvent >> , max_events : usize , } /// Observer that publishes events to a channel pub struct ChannelObserver < C , A = DefaultAction > where C : ' static , A : ActionType , { sender : mpsc :: Sender < WorkflowEvent < C , A >> , }","title":"4. Additional Observer Implementations"},{"location":"adrs/0006-workflow-observability/#5-observer-registry-with-opentelemetry-default","text":"The workflow will have a registry of observers with OpenTelemetry as the default: impl < C , A > Workflow < C , A > where C : ' static , A : ActionType , { /// Add OpenTelemetry observability to this workflow pub fn with_opentelemetry ( mut self , service_name : & str ) -> Result < Self , FloxideError > { let otel = FloxideOtel :: new ( service_name ) ? ; self . observers . push ( Box :: new ( OpenTelemetryObserver :: new ( otel ))); Ok ( self ) } /// Add a custom observer to this workflow pub fn add_observer < O > ( & mut self , observer : O ) -> & mut Self where O : WorkflowObserver < C , A > + ' static , { self . observers . push ( Box :: new ( observer )); self } }","title":"5. Observer Registry with OpenTelemetry Default"},{"location":"adrs/0006-workflow-observability/#6-distributed-tracing-with-opentelemetry","text":"We'll implement specific tracing support for workflow execution: impl < C , A > Workflow < C , A > where C : ' static , A : ActionType , { async fn execute_with_tracing ( & self , ctx : & mut C ) -> Result < (), FloxideError > { // Create a workflow execution span let tracer = opentelemetry :: global :: tracer ( \"floxide\" ); let workflow_span = tracer . start ( format! ( \"workflow:{}\" , self . id ())); let cx = Context :: current_with_span ( workflow_span ); // Set the current context for propagation let _guard = cx . attach (); let start = Instant :: now (); for observer in & self . observers { observer . on_start ( self , ctx ). await ? ; } let result = self . execute_internal ( ctx ). await ; let execution_time = start . elapsed (). as_millis () as u64 ; // Record metrics let meter = opentelemetry :: global :: meter ( \"floxide\" ); let workflow_duration = meter . f64_histogram ( \"workflow.duration\" ) . with_description ( \"Workflow execution duration in milliseconds\" ) . init (); workflow_duration . record ( execution_time as f64 , & [ KeyValue :: new ( \"workflow_id\" , self . id (). to_string ())], ); match & result { Ok ( _ ) => { workflow_span . set_status ( Status :: Ok ); for observer in & self . observers { observer . on_complete ( self , execution_time ). await ? ; } } Err ( err ) => { workflow_span . set_status ( Status :: Error ); workflow_span . record_error ( err ); for observer in & self . observers { observer . on_failure ( self , err ). await ? ; } } } result } }","title":"6. Distributed Tracing with OpenTelemetry"},{"location":"adrs/0006-workflow-observability/#7-opentelemetry-based-visualization","text":"We'll leverage OpenTelemetry for visualization capabilities: /// Generate a visualization of workflow execution using OpenTelemetry spans pub async fn generate_workflow_visualization ( workflow_id : & str , trace_exporter_endpoint : & str , ) -> Result < String , FloxideError > { // Query OpenTelemetry backend for the trace data // and generate visualization // ... }","title":"7. OpenTelemetry-Based Visualization"},{"location":"adrs/0006-workflow-observability/#8-context-aware-logging-with-opentelemetry-integration","text":"We'll enhance nodes with OpenTelemetry-integrated logging capabilities: /// Extension trait for context to add observability capabilities pub trait ObservableContext { /// Get a context identifier for observability fn context_id ( & self ) -> String ; /// Get additional context attributes for OpenTelemetry fn otel_attributes ( & self ) -> HashMap < String , String > { HashMap :: new () } } /// OpenTelemetry instrumentation for nodes impl < Context , A > Node < Context , A > for OtelNodeWrapper < N , Context , A > where N : Node < Context , A > , Context : ObservableContext , A : ActionType , { type Output = N :: Output ; async fn process ( & self , ctx : & mut Context ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > { let context_id = ctx . context_id (); let attributes = ctx . otel_attributes (); let tracer = opentelemetry :: global :: tracer ( \"floxide\" ); let mut span_builder = tracer . span_builder ( format! ( \"node:{}\" , self . node_type )); // Add attributes to the span for ( key , value ) in & attributes { span_builder = span_builder . with_attribute ( KeyValue :: new ( key . clone (), value . clone ())); } span_builder = span_builder . with_attribute ( KeyValue :: new ( \"context_id\" , context_id . clone ())); span_builder = span_builder . with_attribute ( KeyValue :: new ( \"node_type\" , self . node_type . clone ())); let span = span_builder . start ( & tracer ); let cx = Context :: current_with_span ( span ); let _guard = cx . attach (); let start = Instant :: now (); let result = self . inner . process ( ctx ). await ; let duration = start . elapsed (); // Record node execution metrics let meter = opentelemetry :: global :: meter ( \"floxide\" ); let node_duration = meter . f64_histogram ( \"node.duration\" ) . with_description ( \"Node execution duration in milliseconds\" ) . init (); node_duration . record ( duration . as_millis () as f64 , & [ KeyValue :: new ( \"node_type\" , self . node_type . clone ())], ); match & result { Ok ( outcome ) => { let outcome_type = match outcome { NodeOutcome :: Complete ( _ ) => \"complete\" , NodeOutcome :: Transition ( action , _ ) => { span . set_attribute ( KeyValue :: new ( \"transition_action\" , format! ( \"{:?}\" , action ))); \"transition\" } }; span . set_attribute ( KeyValue :: new ( \"outcome\" , outcome_type . to_string ())); span . set_attribute ( KeyValue :: new ( \"duration_ms\" , duration . as_millis () as i64 )); span . set_status ( Status :: Ok ); } Err ( err ) => { span . set_attribute ( KeyValue :: new ( \"duration_ms\" , duration . as_millis () as i64 )); span . set_status ( Status :: Error ); span . record_error ( err ); } } result } }","title":"8. Context-Aware Logging with OpenTelemetry Integration"},{"location":"adrs/0006-workflow-observability/#consequences","text":"","title":"Consequences"},{"location":"adrs/0006-workflow-observability/#positive","text":"Complete Observability : OpenTelemetry provides a comprehensive solution for traces, metrics, and logs Standard Integration : Follows industry standards for observability Ecosystem Compatibility : OpenTelemetry supports many backends (Jaeger, Prometheus, etc.) Distributed Tracing : Built-in support for tracing across service boundaries Minimal Overhead : OpenTelemetry is designed for production use with minimal impact Visualization Options : Can leverage existing OpenTelemetry visualization tools Context Propagation : Supports propagating context across async boundaries","title":"Positive"},{"location":"adrs/0006-workflow-observability/#negative","text":"Dependency Size : OpenTelemetry adds significant dependencies to the project Configuration Complexity : Properly configuring OpenTelemetry requires additional expertise Learning Curve : Using OpenTelemetry effectively requires understanding its concepts Backend Requirements : Requires setting up and maintaining OpenTelemetry backends Additional Resource Usage : Collecting and exporting telemetry data uses system resources","title":"Negative"},{"location":"adrs/0006-workflow-observability/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0006-workflow-observability/#1-simple-logging-approach","text":"Pros : Simpler implementation Fewer dependencies Cons : Limited visibility into workflow execution No standardized way to analyze the data No metrics collection","title":"1. Simple Logging Approach"},{"location":"adrs/0006-workflow-observability/#2-custom-metrics-solution","text":"Pros : Could be more tailored to workflow-specific needs Potentially lower overhead for specific metrics Cons : Would require maintaining a custom solution No standardized integration with other systems Limited ecosystem tools","title":"2. Custom Metrics Solution"},{"location":"adrs/0006-workflow-observability/#3-multiple-separate-systems-logs-metrics-traces","text":"Pros : Could choose best-of-breed for each concern More flexibility in implementation Cons : No unified observability model More complex integration points Harder to correlate data across systems","title":"3. Multiple Separate Systems (logs, metrics, traces)"},{"location":"adrs/0006-workflow-observability/#4-no-built-in-observability","text":"Pros : Simpler framework Lower dependency footprint Cons : Users would need to implement their own solutions Inconsistent observability implementations Poor developer experience We chose OpenTelemetry as our primary observability solution because it provides a comprehensive, standards-based approach to observability that covers traces, metrics, and logs. It offers excellent ecosystem compatibility while maintaining reasonable performance characteristics. The event-based observer pattern allows us to integrate OpenTelemetry seamlessly while still supporting additional observability mechanisms for specific use cases.","title":"4. No Built-in Observability"},{"location":"adrs/0007-batch-processing-implementation/","text":"ADR-0007: Batch Processing Implementation \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 The floxide framework needs batch processing capabilities to efficiently handle parallel execution of workflows on collections of items. We need to design a batch processing system that leverages Rust's ownership model and concurrency features. We need to design a batch processing system that: Efficiently processes collections of items in parallel Respects configurable concurrency limits Provides proper error handling for individual item failures Integrates well with the existing workflow system Follows Rust idioms and best practices Decision \u00b6 We'll implement batch processing with a two-tier approach: A BatchContext trait to define contexts that support batch operations A BatchNode implementation that can process items concurrently A BatchFlow wrapper that provides a simplified API for batch execution BatchContext Trait \u00b6 The BatchContext trait will define how batch-supporting contexts should behave: /// Trait for contexts that support batch processing pub trait BatchContext < T > { /// Get the items to process in batch fn get_batch_items ( & self ) -> Result < Vec < T > , FloxideError > ; /// Create a context for a single item fn create_item_context ( & self , item : T ) -> Result < Self , FloxideError > where Self : Sized ; /// Update the main context with results from item processing fn update_with_results ( & mut self , results : Vec < Result < T , FloxideError >> ) -> Result < (), FloxideError > ; } BatchNode Implementation \u00b6 The BatchNode will implement the Node trait and use Tokio tasks to process items in parallel, with a semaphore to control concurrency: pub struct BatchNode < Context , ItemType , A = crate :: action :: DefaultAction > where Context : BatchContext < ItemType > + Send + Sync + ' static , ItemType : Send + Sync + ' static , A : ActionType + Send + Sync + ' static , { id : NodeId , item_workflow : Arc < Workflow < Context , A >> , parallelism : usize , _phantom : PhantomData < ( Context , ItemType , A ) > , } BatchFlow Implementation \u00b6 The BatchFlow will provide a simpler way to execute batch operations without directly dealing with nodes: pub struct BatchFlow < Context , ItemType , A = crate :: action :: DefaultAction > where Context : BatchContext < ItemType > + Send + Sync + ' static , ItemType : Send + Sync + ' static , A : ActionType + Send + Sync + ' static , { id : NodeId , batch_node : BatchNode < Context , ItemType , A > , } Consequences \u00b6 Advantages \u00b6 Better Parallelism : Rust's async runtime with Tokio provides efficient parallel processing Type Safety : The approach is fully type-safe with no runtime type checking needed Resource Control : Explicit concurrency controls prevent overwhelming system resources Integration : Seamlessly integrates with the existing workflow system Error Isolation : Individual item failures don't stop the entire batch Disadvantages \u00b6 Complexity : Requires implementing the BatchContext trait for contexts that support batch operations Resource Overhead : Each parallel task incurs some overhead for spawning and synchronization Context Cloning : Requires contexts to be clonable, which might be inefficient for large contexts Testing and Verification \u00b6 We've added comprehensive tests for both BatchNode and BatchFlow to verify: Parallel processing capabilities Proper error handling Context updates after processing Integration with the workflow system Alternatives Considered \u00b6 Stream-Based Processing \u00b6 We initially considered a purely stream-based approach using the futures crate StreamExt traits: stream :: iter ( items ) . map ( | item | async { /* process item */ }) . buffer_unordered ( self . parallelism ) . collect :: < Vec < _ >> () . await However, this approach was more limited in handling context updates and didn't provide as much control over error handling. Single-Threaded Processing \u00b6 We considered a simpler, single-threaded approach that processes items sequentially. While simpler, this would not take advantage of multi-core systems for CPU-bound tasks. Implementation Notes \u00b6 We're using Tokio's Semaphore for concurrency control Each item gets its own task, allowing true parallelism for CPU-bound operations Results are collected back in the original context after all items are processed Error handling preserves information about individual item failures","title":"ADR-0007: Batch Processing Implementation"},{"location":"adrs/0007-batch-processing-implementation/#adr-0007-batch-processing-implementation","text":"","title":"ADR-0007: Batch Processing Implementation"},{"location":"adrs/0007-batch-processing-implementation/#status","text":"Accepted","title":"Status"},{"location":"adrs/0007-batch-processing-implementation/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0007-batch-processing-implementation/#context","text":"The floxide framework needs batch processing capabilities to efficiently handle parallel execution of workflows on collections of items. We need to design a batch processing system that leverages Rust's ownership model and concurrency features. We need to design a batch processing system that: Efficiently processes collections of items in parallel Respects configurable concurrency limits Provides proper error handling for individual item failures Integrates well with the existing workflow system Follows Rust idioms and best practices","title":"Context"},{"location":"adrs/0007-batch-processing-implementation/#decision","text":"We'll implement batch processing with a two-tier approach: A BatchContext trait to define contexts that support batch operations A BatchNode implementation that can process items concurrently A BatchFlow wrapper that provides a simplified API for batch execution","title":"Decision"},{"location":"adrs/0007-batch-processing-implementation/#batchcontext-trait","text":"The BatchContext trait will define how batch-supporting contexts should behave: /// Trait for contexts that support batch processing pub trait BatchContext < T > { /// Get the items to process in batch fn get_batch_items ( & self ) -> Result < Vec < T > , FloxideError > ; /// Create a context for a single item fn create_item_context ( & self , item : T ) -> Result < Self , FloxideError > where Self : Sized ; /// Update the main context with results from item processing fn update_with_results ( & mut self , results : Vec < Result < T , FloxideError >> ) -> Result < (), FloxideError > ; }","title":"BatchContext Trait"},{"location":"adrs/0007-batch-processing-implementation/#batchnode-implementation","text":"The BatchNode will implement the Node trait and use Tokio tasks to process items in parallel, with a semaphore to control concurrency: pub struct BatchNode < Context , ItemType , A = crate :: action :: DefaultAction > where Context : BatchContext < ItemType > + Send + Sync + ' static , ItemType : Send + Sync + ' static , A : ActionType + Send + Sync + ' static , { id : NodeId , item_workflow : Arc < Workflow < Context , A >> , parallelism : usize , _phantom : PhantomData < ( Context , ItemType , A ) > , }","title":"BatchNode Implementation"},{"location":"adrs/0007-batch-processing-implementation/#batchflow-implementation","text":"The BatchFlow will provide a simpler way to execute batch operations without directly dealing with nodes: pub struct BatchFlow < Context , ItemType , A = crate :: action :: DefaultAction > where Context : BatchContext < ItemType > + Send + Sync + ' static , ItemType : Send + Sync + ' static , A : ActionType + Send + Sync + ' static , { id : NodeId , batch_node : BatchNode < Context , ItemType , A > , }","title":"BatchFlow Implementation"},{"location":"adrs/0007-batch-processing-implementation/#consequences","text":"","title":"Consequences"},{"location":"adrs/0007-batch-processing-implementation/#advantages","text":"Better Parallelism : Rust's async runtime with Tokio provides efficient parallel processing Type Safety : The approach is fully type-safe with no runtime type checking needed Resource Control : Explicit concurrency controls prevent overwhelming system resources Integration : Seamlessly integrates with the existing workflow system Error Isolation : Individual item failures don't stop the entire batch","title":"Advantages"},{"location":"adrs/0007-batch-processing-implementation/#disadvantages","text":"Complexity : Requires implementing the BatchContext trait for contexts that support batch operations Resource Overhead : Each parallel task incurs some overhead for spawning and synchronization Context Cloning : Requires contexts to be clonable, which might be inefficient for large contexts","title":"Disadvantages"},{"location":"adrs/0007-batch-processing-implementation/#testing-and-verification","text":"We've added comprehensive tests for both BatchNode and BatchFlow to verify: Parallel processing capabilities Proper error handling Context updates after processing Integration with the workflow system","title":"Testing and Verification"},{"location":"adrs/0007-batch-processing-implementation/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0007-batch-processing-implementation/#stream-based-processing","text":"We initially considered a purely stream-based approach using the futures crate StreamExt traits: stream :: iter ( items ) . map ( | item | async { /* process item */ }) . buffer_unordered ( self . parallelism ) . collect :: < Vec < _ >> () . await However, this approach was more limited in handling context updates and didn't provide as much control over error handling.","title":"Stream-Based Processing"},{"location":"adrs/0007-batch-processing-implementation/#single-threaded-processing","text":"We considered a simpler, single-threaded approach that processes items sequentially. While simpler, this would not take advantage of multi-core systems for CPU-bound tasks.","title":"Single-Threaded Processing"},{"location":"adrs/0007-batch-processing-implementation/#implementation-notes","text":"We're using Tokio's Semaphore for concurrency control Each item gets its own task, allowing true parallelism for CPU-bound operations Results are collected back in the original context after all items are processed Error handling preserves information about individual item failures","title":"Implementation Notes"},{"location":"adrs/0008-event-driven-node-extensions/","text":"ADR-0008: Event-Driven Node Extensions \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 The floxide framework is being extended with event-driven capabilities through the floxide-event crate. This crate provides functionality for creating nodes that respond to external events and can be integrated into standard workflows. However, several issues have been identified in the current implementation: The EventProcessor struct has unused type parameters ( Context and Action ) The FloxideError enum lacks methods needed by the event system ( node_not_found and timeout ) The NodeOutcome::RouteToAction variant is being used incorrectly with two arguments when it expects one Type inference issues in several places where trait bounds cannot be properly resolved Missing trait implementations, specifically Default for the Action type parameter Incorrect variant names for DefaultAction in the EventActionExt trait implementation These issues need to be addressed to ensure the event-driven node extensions work correctly within the framework. Decision \u00b6 We will make the following changes to address the issues: Fix EventProcessor type parameters : Update the EventProcessor struct to either use the unused type parameters or remove them, using PhantomData where necessary. Extend FloxideError with required methods : Add methods to the FloxideError type in the core crate for node_not_found and timeout functionality. Fix NodeOutcome::RouteToAction usage : Update the event crate to correctly use this variant with a single argument instead of two. Address type inference issues : Add proper type annotations and function signatures to improve type inference in the event-driven components. Add required trait bounds : Ensure that all generic type parameters have the necessary trait bounds, including Default where required. Correct EventActionExt implementation : Fix the implementation to use the proper function syntax instead of non-existent enum variants. Clean up unused imports : Remove unused imports in both the transform and event crates. Implementation Details \u00b6 The implementation involved the following key changes: Added node_not_found , timeout , and is_timeout methods to FloxideError in the core crate. Fixed EventProcessor by adding PhantomData for unused type parameters and adding proper trait bounds. Updated the EventDrivenNode trait to change wait_for_event from taking an immutable reference to a mutable reference, solving a borrowing issue with the mpsc::Receiver . Replaced all incorrect uses of NodeOutcome::RouteToAction with the correct single-argument syntax. Added the #[async_trait] macro to all implementations of async traits. Used fully-qualified path syntax to address type inference issues in the wait_for_event and id methods. Wrapped the ChannelEventSource in a tokio::sync::Mutex to allow safe mutable access from multiple places. Fixed the EventActionExt trait implementation for DefaultAction to create proper custom actions. Removed unused imports in both crates. Consequences \u00b6 Positive \u00b6 The event-driven extensions now compile and work correctly The code is more type-safe and follows Rust best practices Better error handling with proper error types for event-driven workflows Cleaner code with unused imports removed Improved consistency between the core, transform, and event APIs Negative \u00b6 Small API changes may require updates to existing code that uses these features: EventDrivenNode::wait_for_event now requires a mutable reference The Action type parameter now requires the Default trait Neutral \u00b6 The architecture of the event-driven node system remains fundamentally the same Core framework abstractions are unchanged Future Work \u00b6 Consider adding more comprehensive documentation for the event-driven capabilities Explore adding more event source implementations (e.g., WebSocket, HTTP, etc.) Consider adding a more ergonomic API for creating event-driven workflows","title":"ADR-0008: Event-Driven Node Extensions"},{"location":"adrs/0008-event-driven-node-extensions/#adr-0008-event-driven-node-extensions","text":"","title":"ADR-0008: Event-Driven Node Extensions"},{"location":"adrs/0008-event-driven-node-extensions/#status","text":"Accepted","title":"Status"},{"location":"adrs/0008-event-driven-node-extensions/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0008-event-driven-node-extensions/#context","text":"The floxide framework is being extended with event-driven capabilities through the floxide-event crate. This crate provides functionality for creating nodes that respond to external events and can be integrated into standard workflows. However, several issues have been identified in the current implementation: The EventProcessor struct has unused type parameters ( Context and Action ) The FloxideError enum lacks methods needed by the event system ( node_not_found and timeout ) The NodeOutcome::RouteToAction variant is being used incorrectly with two arguments when it expects one Type inference issues in several places where trait bounds cannot be properly resolved Missing trait implementations, specifically Default for the Action type parameter Incorrect variant names for DefaultAction in the EventActionExt trait implementation These issues need to be addressed to ensure the event-driven node extensions work correctly within the framework.","title":"Context"},{"location":"adrs/0008-event-driven-node-extensions/#decision","text":"We will make the following changes to address the issues: Fix EventProcessor type parameters : Update the EventProcessor struct to either use the unused type parameters or remove them, using PhantomData where necessary. Extend FloxideError with required methods : Add methods to the FloxideError type in the core crate for node_not_found and timeout functionality. Fix NodeOutcome::RouteToAction usage : Update the event crate to correctly use this variant with a single argument instead of two. Address type inference issues : Add proper type annotations and function signatures to improve type inference in the event-driven components. Add required trait bounds : Ensure that all generic type parameters have the necessary trait bounds, including Default where required. Correct EventActionExt implementation : Fix the implementation to use the proper function syntax instead of non-existent enum variants. Clean up unused imports : Remove unused imports in both the transform and event crates.","title":"Decision"},{"location":"adrs/0008-event-driven-node-extensions/#implementation-details","text":"The implementation involved the following key changes: Added node_not_found , timeout , and is_timeout methods to FloxideError in the core crate. Fixed EventProcessor by adding PhantomData for unused type parameters and adding proper trait bounds. Updated the EventDrivenNode trait to change wait_for_event from taking an immutable reference to a mutable reference, solving a borrowing issue with the mpsc::Receiver . Replaced all incorrect uses of NodeOutcome::RouteToAction with the correct single-argument syntax. Added the #[async_trait] macro to all implementations of async traits. Used fully-qualified path syntax to address type inference issues in the wait_for_event and id methods. Wrapped the ChannelEventSource in a tokio::sync::Mutex to allow safe mutable access from multiple places. Fixed the EventActionExt trait implementation for DefaultAction to create proper custom actions. Removed unused imports in both crates.","title":"Implementation Details"},{"location":"adrs/0008-event-driven-node-extensions/#consequences","text":"","title":"Consequences"},{"location":"adrs/0008-event-driven-node-extensions/#positive","text":"The event-driven extensions now compile and work correctly The code is more type-safe and follows Rust best practices Better error handling with proper error types for event-driven workflows Cleaner code with unused imports removed Improved consistency between the core, transform, and event APIs","title":"Positive"},{"location":"adrs/0008-event-driven-node-extensions/#negative","text":"Small API changes may require updates to existing code that uses these features: EventDrivenNode::wait_for_event now requires a mutable reference The Action type parameter now requires the Default trait","title":"Negative"},{"location":"adrs/0008-event-driven-node-extensions/#neutral","text":"The architecture of the event-driven node system remains fundamentally the same Core framework abstractions are unchanged","title":"Neutral"},{"location":"adrs/0008-event-driven-node-extensions/#future-work","text":"Consider adding more comprehensive documentation for the event-driven capabilities Explore adding more event source implementations (e.g., WebSocket, HTTP, etc.) Consider adding a more ergonomic API for creating event-driven workflows","title":"Future Work"},{"location":"adrs/0008-node-lifecycle-methods/","text":"ADR-0008: Node Lifecycle Methods \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 The Flow Framework uses a three-phase lifecycle for nodes: prep : Preparation phase for setup and validation execCore : Core execution with potential retry mechanisms post : Post-processing phase that determines routing This pattern provides clear separation of concerns and allows for specialized behaviors in each phase. We need to implement this pattern in Rust while following Rust idioms. Decision \u00b6 We will introduce a LifecycleNode trait that explicitly models the three-phase lifecycle, while maintaining compatibility with the existing Node trait through adapter patterns. LifecycleNode Trait \u00b6 #[async_trait] pub trait LifecycleNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , Self :: PrepOutput : Clone + Send + Sync + ' static , Self :: ExecOutput : Clone + Send + Sync + ' static , { /// Output type from the preparation phase type PrepOutput ; /// Output type from the execution phase type ExecOutput ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; /// Preparation phase - perform setup and validation async fn prep ( & self , ctx : & mut Context ) -> Result < Self :: PrepOutput , FloxideError > ; /// Execution phase - perform the main work async fn exec ( & self , prep_result : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > ; /// Post-execution phase - determine the next action and update context async fn post ( & self , prep_result : Self :: PrepOutput , exec_result : Self :: ExecOutput , ctx : & mut Context ) -> Result < Action , FloxideError > ; } Adapter Pattern \u00b6 To maintain compatibility with the existing Node trait, we'll implement an adapter that converts LifecycleNodes to Nodes: pub struct LifecycleNodeAdapter < LN , Context , Action > where LN : LifecycleNode < Context , Action > , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { inner : LN , _phantom : PhantomData < ( Context , Action ) > , } #[async_trait] impl < LN , Context , Action > Node < Context , Action > for LifecycleNodeAdapter < LN , Context , Action > where LN : LifecycleNode < Context , Action > + Send + Sync + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , LN :: ExecOutput : Send + Sync + ' static , { type Output = LN :: ExecOutput ; fn id ( & self ) -> NodeId { self . inner . id () } async fn process ( & self , ctx : & mut Context ) -> Result < NodeOutcome < Self :: Output , Action > , FloxideError > { // Run the three-phase lifecycle debug ! ( node_id = % self . id (), \"Starting prep phase\" ); let prep_result = self . inner . prep ( ctx ). await ? ; debug ! ( node_id = % self . id (), \"Starting exec phase\" ); let exec_result = self . inner . exec ( prep_result . clone ()). await ? ; debug ! ( node_id = % self . id (), \"Starting post phase\" ); let next_action = self . inner . post ( prep_result , exec_result . clone (), ctx ). await ? ; // Return the appropriate outcome based on the action Ok ( NodeOutcome :: RouteToAction ( next_action )) } } Builder Function \u00b6 For convenience, we'll provide a closure-based API that makes it easy to create lifecycle nodes: pub fn lifecycle_node < PrepFn , ExecFn , PostFn , Context , Action , PrepOut , ExecOut , PrepFut , ExecFut , PostFut > ( id : Option < String > , prep_fn : PrepFn , exec_fn : ExecFn , post_fn : PostFn , ) -> impl Node < Context , Action , Output = ExecOut > where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , PrepOut : Send + Sync + Clone + ' static , ExecOut : Send + Sync + Clone + ' static , PrepFn : Fn ( & mut Context ) -> PrepFut + Send + Sync + ' static , ExecFn : Fn ( PrepOut ) -> ExecFut + Send + Sync + ' static , PostFn : Fn ( PrepOut , ExecOut , & mut Context ) -> PostFut + Send + Sync + ' static , PrepFut : Future < Output = Result < PrepOut , FloxideError >> + Send + ' static , ExecFut : Future < Output = Result < ExecOut , FloxideError >> + Send + ' static , PostFut : Future < Output = Result < Action , FloxideError >> + Send + ' static , { // Implementation details... } Consequences \u00b6 Advantages \u00b6 Clear Separation : Each phase has a distinct purpose and signature Compatibility : Works with existing Node interface through the adapter Type Safety : Phase outputs are properly typed Flexibility : Different nodes can define their own prep/exec types Consistency : Maintains a clear and structured lifecycle approach for workflow nodes Disadvantages \u00b6 Complexity : More complex than a single process method Clone Requirements : Requires Clone trait on phase outputs Type Complexity : More generic parameters than the simpler Node trait Migration Path \u00b6 Existing nodes using the Node trait can continue to work without changes. New nodes can use the LifecycleNode trait with the adapter, or use the convenient lifecycle_node builder function. Alternatives Considered \u00b6 Single Method with Internal Phases \u00b6 We considered having a single process method that internally calls prep/exec/post methods, but this would make the phase outputs harder to type correctly and require more dynamic typing. Complete Replacement \u00b6 We considered completely replacing the Node trait with LifecycleNode, but this would break compatibility with existing code. Dynamic Function Parameters \u00b6 We evaluated using dynamic function parameters to allow more flexibility in the lifecycle, but this would have required more complex trait bounds and potentially runtime checks. Implementation Notes \u00b6 The LifecycleNode trait requires prep/exec outputs to implement Clone for simplicity The adapter automatically converts to NodeOutcome::RouteToAction Unit tests verify the full lifecycle and error propagation between phases","title":"ADR-0008: Node Lifecycle Methods"},{"location":"adrs/0008-node-lifecycle-methods/#adr-0008-node-lifecycle-methods","text":"","title":"ADR-0008: Node Lifecycle Methods"},{"location":"adrs/0008-node-lifecycle-methods/#status","text":"Accepted","title":"Status"},{"location":"adrs/0008-node-lifecycle-methods/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0008-node-lifecycle-methods/#context","text":"The Flow Framework uses a three-phase lifecycle for nodes: prep : Preparation phase for setup and validation execCore : Core execution with potential retry mechanisms post : Post-processing phase that determines routing This pattern provides clear separation of concerns and allows for specialized behaviors in each phase. We need to implement this pattern in Rust while following Rust idioms.","title":"Context"},{"location":"adrs/0008-node-lifecycle-methods/#decision","text":"We will introduce a LifecycleNode trait that explicitly models the three-phase lifecycle, while maintaining compatibility with the existing Node trait through adapter patterns.","title":"Decision"},{"location":"adrs/0008-node-lifecycle-methods/#lifecyclenode-trait","text":"#[async_trait] pub trait LifecycleNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , Self :: PrepOutput : Clone + Send + Sync + ' static , Self :: ExecOutput : Clone + Send + Sync + ' static , { /// Output type from the preparation phase type PrepOutput ; /// Output type from the execution phase type ExecOutput ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; /// Preparation phase - perform setup and validation async fn prep ( & self , ctx : & mut Context ) -> Result < Self :: PrepOutput , FloxideError > ; /// Execution phase - perform the main work async fn exec ( & self , prep_result : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > ; /// Post-execution phase - determine the next action and update context async fn post ( & self , prep_result : Self :: PrepOutput , exec_result : Self :: ExecOutput , ctx : & mut Context ) -> Result < Action , FloxideError > ; }","title":"LifecycleNode Trait"},{"location":"adrs/0008-node-lifecycle-methods/#adapter-pattern","text":"To maintain compatibility with the existing Node trait, we'll implement an adapter that converts LifecycleNodes to Nodes: pub struct LifecycleNodeAdapter < LN , Context , Action > where LN : LifecycleNode < Context , Action > , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { inner : LN , _phantom : PhantomData < ( Context , Action ) > , } #[async_trait] impl < LN , Context , Action > Node < Context , Action > for LifecycleNodeAdapter < LN , Context , Action > where LN : LifecycleNode < Context , Action > + Send + Sync + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , LN :: ExecOutput : Send + Sync + ' static , { type Output = LN :: ExecOutput ; fn id ( & self ) -> NodeId { self . inner . id () } async fn process ( & self , ctx : & mut Context ) -> Result < NodeOutcome < Self :: Output , Action > , FloxideError > { // Run the three-phase lifecycle debug ! ( node_id = % self . id (), \"Starting prep phase\" ); let prep_result = self . inner . prep ( ctx ). await ? ; debug ! ( node_id = % self . id (), \"Starting exec phase\" ); let exec_result = self . inner . exec ( prep_result . clone ()). await ? ; debug ! ( node_id = % self . id (), \"Starting post phase\" ); let next_action = self . inner . post ( prep_result , exec_result . clone (), ctx ). await ? ; // Return the appropriate outcome based on the action Ok ( NodeOutcome :: RouteToAction ( next_action )) } }","title":"Adapter Pattern"},{"location":"adrs/0008-node-lifecycle-methods/#builder-function","text":"For convenience, we'll provide a closure-based API that makes it easy to create lifecycle nodes: pub fn lifecycle_node < PrepFn , ExecFn , PostFn , Context , Action , PrepOut , ExecOut , PrepFut , ExecFut , PostFut > ( id : Option < String > , prep_fn : PrepFn , exec_fn : ExecFn , post_fn : PostFn , ) -> impl Node < Context , Action , Output = ExecOut > where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , PrepOut : Send + Sync + Clone + ' static , ExecOut : Send + Sync + Clone + ' static , PrepFn : Fn ( & mut Context ) -> PrepFut + Send + Sync + ' static , ExecFn : Fn ( PrepOut ) -> ExecFut + Send + Sync + ' static , PostFn : Fn ( PrepOut , ExecOut , & mut Context ) -> PostFut + Send + Sync + ' static , PrepFut : Future < Output = Result < PrepOut , FloxideError >> + Send + ' static , ExecFut : Future < Output = Result < ExecOut , FloxideError >> + Send + ' static , PostFut : Future < Output = Result < Action , FloxideError >> + Send + ' static , { // Implementation details... }","title":"Builder Function"},{"location":"adrs/0008-node-lifecycle-methods/#consequences","text":"","title":"Consequences"},{"location":"adrs/0008-node-lifecycle-methods/#advantages","text":"Clear Separation : Each phase has a distinct purpose and signature Compatibility : Works with existing Node interface through the adapter Type Safety : Phase outputs are properly typed Flexibility : Different nodes can define their own prep/exec types Consistency : Maintains a clear and structured lifecycle approach for workflow nodes","title":"Advantages"},{"location":"adrs/0008-node-lifecycle-methods/#disadvantages","text":"Complexity : More complex than a single process method Clone Requirements : Requires Clone trait on phase outputs Type Complexity : More generic parameters than the simpler Node trait","title":"Disadvantages"},{"location":"adrs/0008-node-lifecycle-methods/#migration-path","text":"Existing nodes using the Node trait can continue to work without changes. New nodes can use the LifecycleNode trait with the adapter, or use the convenient lifecycle_node builder function.","title":"Migration Path"},{"location":"adrs/0008-node-lifecycle-methods/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0008-node-lifecycle-methods/#single-method-with-internal-phases","text":"We considered having a single process method that internally calls prep/exec/post methods, but this would make the phase outputs harder to type correctly and require more dynamic typing.","title":"Single Method with Internal Phases"},{"location":"adrs/0008-node-lifecycle-methods/#complete-replacement","text":"We considered completely replacing the Node trait with LifecycleNode, but this would break compatibility with existing code.","title":"Complete Replacement"},{"location":"adrs/0008-node-lifecycle-methods/#dynamic-function-parameters","text":"We evaluated using dynamic function parameters to allow more flexibility in the lifecycle, but this would have required more complex trait bounds and potentially runtime checks.","title":"Dynamic Function Parameters"},{"location":"adrs/0008-node-lifecycle-methods/#implementation-notes","text":"The LifecycleNode trait requires prep/exec outputs to implement Clone for simplicity The adapter automatically converts to NodeOutcome::RouteToAction Unit tests verify the full lifecycle and error propagation between phases","title":"Implementation Notes"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/","text":"ADR-0009: Cloneable Types for Batch Processing \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 In implementing batch processing for the floxide framework, we've encountered an ownership challenge: individual items that are processed in parallel tasks need to be accessed in multiple places: When creating an item-specific context When returning the original item as part of the result When updating the batch context with results The Rust borrow checker enforces strict ownership rules, and we need a solution that allows: Processing items in parallel Passing ownership of items into tasks Returning processed items from tasks Avoiding unnecessary copies of potentially large data Decision \u00b6 We will require that item types used in BatchContext<T> must implement the Clone trait. This requirement will be documented and enforced through trait bounds. Updated BatchContext Trait \u00b6 /// Trait for contexts that support batch processing pub trait BatchContext < T > where T : Clone + Send + Sync + ' static , { /// Get the items to process in batch fn get_batch_items ( & self ) -> Result < Vec < T > , FloxideError > ; /// Create a context for a single item fn create_item_context ( & self , item : T ) -> Result < Self , FloxideError > where Self : Sized ; /// Update the main context with results from item processing fn update_with_results ( & mut self , results : & Vec < Result < T , FloxideError >> , ) -> Result < (), FloxideError > ; } BatchNode Implementation \u00b6 The BatchNode implementation will be updated to properly handle cloning: // Create tasks for each item for item in items { let semaphore = semaphore . clone (); let workflow = self . item_workflow . clone (); let ctx_clone = ctx . clone (); // Clone the item for use in the task let item_clone = item . clone (); // Spawn a task for each item let handle = tokio :: spawn ( async move { // Acquire a permit from the semaphore to limit concurrency let _permit = semaphore . acquire (). await . unwrap (); match ctx_clone . create_item_context ( item_clone ) { Ok ( mut item_ctx ) => match workflow . execute ( & mut item_ctx ). await { Ok ( _ ) => Ok ( item ), Err ( e ) => Err ( FloxideError :: batch_processing ( \"Failed to process item\" , Box :: new ( e ), )), }, Err ( e ) => Err ( e ), } }); handles . push ( handle ); } Consequences \u00b6 Advantages \u00b6 Clear Requirements : Users know exactly what constraints apply to item types Type Safety : The compiler enforces the Clone constraint Efficient Processing : Items can be processed in parallel without unsafe code Safe Implementation : No risk of use-after-move errors Disadvantages \u00b6 Constraint on Types : Requires all batch item types to implement Clone Potential Memory Overhead : May result in more copies than strictly necessary Potential Performance Impact : Cloning large items could impact performance Alternatives Considered \u00b6 Require Copy Instead of Clone \u00b6 We considered requiring Copy instead of Clone , which would eliminate the need for explicit cloning. However, this would be too restrictive, as many useful types (like String, Vec, etc.) don't implement Copy. Use References with Lifetime Parameters \u00b6 Another approach would be to use references with explicit lifetimes throughout the batch processing system. While this would avoid cloning, it would significantly complicate the API and make it harder to use, especially with async code and closures. Use Arc for Shared Ownership \u00b6 We could require item types to be wrapped in Arc for shared ownership. This would avoid cloning the actual data but would require users to wrap and unwrap their data, complicating the API. Implementation Notes \u00b6 The Clone constraint will be added to all relevant trait bounds Documentation will clearly state that batch item types must be cloneable Examples will demonstrate best practices for minimizing cloning overhead Unit tests will verify correct behavior with various item types","title":"ADR-0009: Cloneable Types for Batch Processing"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/#adr-0009-cloneable-types-for-batch-processing","text":"","title":"ADR-0009: Cloneable Types for Batch Processing"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/#status","text":"Accepted","title":"Status"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/#context","text":"In implementing batch processing for the floxide framework, we've encountered an ownership challenge: individual items that are processed in parallel tasks need to be accessed in multiple places: When creating an item-specific context When returning the original item as part of the result When updating the batch context with results The Rust borrow checker enforces strict ownership rules, and we need a solution that allows: Processing items in parallel Passing ownership of items into tasks Returning processed items from tasks Avoiding unnecessary copies of potentially large data","title":"Context"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/#decision","text":"We will require that item types used in BatchContext<T> must implement the Clone trait. This requirement will be documented and enforced through trait bounds.","title":"Decision"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/#updated-batchcontext-trait","text":"/// Trait for contexts that support batch processing pub trait BatchContext < T > where T : Clone + Send + Sync + ' static , { /// Get the items to process in batch fn get_batch_items ( & self ) -> Result < Vec < T > , FloxideError > ; /// Create a context for a single item fn create_item_context ( & self , item : T ) -> Result < Self , FloxideError > where Self : Sized ; /// Update the main context with results from item processing fn update_with_results ( & mut self , results : & Vec < Result < T , FloxideError >> , ) -> Result < (), FloxideError > ; }","title":"Updated BatchContext Trait"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/#batchnode-implementation","text":"The BatchNode implementation will be updated to properly handle cloning: // Create tasks for each item for item in items { let semaphore = semaphore . clone (); let workflow = self . item_workflow . clone (); let ctx_clone = ctx . clone (); // Clone the item for use in the task let item_clone = item . clone (); // Spawn a task for each item let handle = tokio :: spawn ( async move { // Acquire a permit from the semaphore to limit concurrency let _permit = semaphore . acquire (). await . unwrap (); match ctx_clone . create_item_context ( item_clone ) { Ok ( mut item_ctx ) => match workflow . execute ( & mut item_ctx ). await { Ok ( _ ) => Ok ( item ), Err ( e ) => Err ( FloxideError :: batch_processing ( \"Failed to process item\" , Box :: new ( e ), )), }, Err ( e ) => Err ( e ), } }); handles . push ( handle ); }","title":"BatchNode Implementation"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/#consequences","text":"","title":"Consequences"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/#advantages","text":"Clear Requirements : Users know exactly what constraints apply to item types Type Safety : The compiler enforces the Clone constraint Efficient Processing : Items can be processed in parallel without unsafe code Safe Implementation : No risk of use-after-move errors","title":"Advantages"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/#disadvantages","text":"Constraint on Types : Requires all batch item types to implement Clone Potential Memory Overhead : May result in more copies than strictly necessary Potential Performance Impact : Cloning large items could impact performance","title":"Disadvantages"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/#require-copy-instead-of-clone","text":"We considered requiring Copy instead of Clone , which would eliminate the need for explicit cloning. However, this would be too restrictive, as many useful types (like String, Vec, etc.) don't implement Copy.","title":"Require Copy Instead of Clone"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/#use-references-with-lifetime-parameters","text":"Another approach would be to use references with explicit lifetimes throughout the batch processing system. While this would avoid cloning, it would significantly complicate the API and make it harder to use, especially with async code and closures.","title":"Use References with Lifetime Parameters"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/#use-arc-for-shared-ownership","text":"We could require item types to be wrapped in Arc for shared ownership. This would avoid cloning the actual data but would require users to wrap and unwrap their data, complicating the API.","title":"Use Arc for Shared Ownership"},{"location":"adrs/0009-cloneable-traits-for-batch-contexts/#implementation-notes","text":"The Clone constraint will be added to all relevant trait bounds Documentation will clearly state that batch item types must be cloneable Examples will demonstrate best practices for minimizing cloning overhead Unit tests will verify correct behavior with various item types","title":"Implementation Notes"},{"location":"adrs/0009-event-driven-workflow-pattern/","text":"ADR-0009: Event-Driven Workflow Pattern \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 The Flow Framework is primarily designed around synchronous, request-response style workflows where nodes are executed in a predefined sequence. However, many real-world systems require handling of asynchronous events where the timing of events is unpredictable. Examples include: IoT sensor monitoring systems Message queue processors User interaction workflows Long-running background processes The framework needs to support event-driven workflows where nodes can wait for external events and then process them according to workflow rules. Decision \u00b6 We will extend the Flow Framework with a dedicated event subsystem by introducing: EventDrivenNode Trait : A specialized node trait for handling events with these key methods: wait_for_event() : Waits for an external event to arrive process_event() : Processes the event and returns an action id() : Returns the node's unique identifier Event Sources : Implementations of event sources such as: ChannelEventSource : Receives events from a Tokio MPSC channel (Future) Other event sources for websockets, HTTP, etc. EventDrivenWorkflow : A specialized workflow that: Contains a collection of event-driven nodes Routes events between nodes based on actions Executes until a termination condition is met Supports timeouts for bounded execution Integration with Standard Workflows : Adapters to: Use event-driven nodes within standard workflows ( EventDrivenNodeAdapter ) Use event-driven workflows as nodes in standard workflows ( NestedEventDrivenWorkflow ) EventActionExt Trait : An extension trait providing common event-related actions: terminate() : Creates a termination action timeout() : Creates a timeout action Workflow Pattern \u00b6 The event-driven workflow follows this pattern: Setup Phase : Create event sources Create event processors Configure the workflow routing Execution Phase : The workflow starts with an initial event source node The source waits for an external event When an event arrives, it's passed to the next node based on routing rules The node processes the event and returns an action The action determines the next node to route to This continues until a termination action is received or a timeout occurs Termination Phase : The workflow terminates when a node returns a designated termination action Alternatively, the workflow can time out if configured with a timeout Key Benefits \u00b6 Non-blocking Event Handling : Allows nodes to wait for events without blocking the thread Dynamic Routing : Routes events based on their content/classification Integration with Standard Workflows : Can be used seamlessly with regular workflows Timeout Support : Prevents workflows from hanging indefinitely Stateful Processing : Maintains context between events Implementation Approach \u00b6 We have implemented this pattern in the floxide-event crate, which provides: The core traits and interfaces for event-driven nodes Basic event source implementations (channel-based) Adapters for integrating with standard workflows Workflows for orchestrating event-driven nodes The implementation has been designed to be: Type-safe : Using Rust's type system to ensure correctness Async-first : Built around Tokio's async ecosystem Composable : Can be used alongside other framework components Extensible : New event sources can be added without modifying the core Example Use Cases \u00b6 IoT Monitoring : Processing temperature/humidity readings from sensors Chat Application : Handling incoming messages in a chat system Job Queue : Processing jobs from a distributed queue User Session : Handling events during a user session Consequences \u00b6 Positive \u00b6 Enables event-driven application patterns within the Flow Framework Allows for integration with external event sources Maintains type safety throughout the event pipeline Fits naturally into the existing node/action framework model Negative \u00b6 Adds complexity to the framework Requires understanding of both workflow and event concepts Might increase the learning curve for new users Neutral \u00b6 Requires additional testing of asynchronous patterns May need refinement as real-world usage patterns emerge","title":"ADR-0009: Event-Driven Workflow Pattern"},{"location":"adrs/0009-event-driven-workflow-pattern/#adr-0009-event-driven-workflow-pattern","text":"","title":"ADR-0009: Event-Driven Workflow Pattern"},{"location":"adrs/0009-event-driven-workflow-pattern/#status","text":"Accepted","title":"Status"},{"location":"adrs/0009-event-driven-workflow-pattern/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0009-event-driven-workflow-pattern/#context","text":"The Flow Framework is primarily designed around synchronous, request-response style workflows where nodes are executed in a predefined sequence. However, many real-world systems require handling of asynchronous events where the timing of events is unpredictable. Examples include: IoT sensor monitoring systems Message queue processors User interaction workflows Long-running background processes The framework needs to support event-driven workflows where nodes can wait for external events and then process them according to workflow rules.","title":"Context"},{"location":"adrs/0009-event-driven-workflow-pattern/#decision","text":"We will extend the Flow Framework with a dedicated event subsystem by introducing: EventDrivenNode Trait : A specialized node trait for handling events with these key methods: wait_for_event() : Waits for an external event to arrive process_event() : Processes the event and returns an action id() : Returns the node's unique identifier Event Sources : Implementations of event sources such as: ChannelEventSource : Receives events from a Tokio MPSC channel (Future) Other event sources for websockets, HTTP, etc. EventDrivenWorkflow : A specialized workflow that: Contains a collection of event-driven nodes Routes events between nodes based on actions Executes until a termination condition is met Supports timeouts for bounded execution Integration with Standard Workflows : Adapters to: Use event-driven nodes within standard workflows ( EventDrivenNodeAdapter ) Use event-driven workflows as nodes in standard workflows ( NestedEventDrivenWorkflow ) EventActionExt Trait : An extension trait providing common event-related actions: terminate() : Creates a termination action timeout() : Creates a timeout action","title":"Decision"},{"location":"adrs/0009-event-driven-workflow-pattern/#workflow-pattern","text":"The event-driven workflow follows this pattern: Setup Phase : Create event sources Create event processors Configure the workflow routing Execution Phase : The workflow starts with an initial event source node The source waits for an external event When an event arrives, it's passed to the next node based on routing rules The node processes the event and returns an action The action determines the next node to route to This continues until a termination action is received or a timeout occurs Termination Phase : The workflow terminates when a node returns a designated termination action Alternatively, the workflow can time out if configured with a timeout","title":"Workflow Pattern"},{"location":"adrs/0009-event-driven-workflow-pattern/#key-benefits","text":"Non-blocking Event Handling : Allows nodes to wait for events without blocking the thread Dynamic Routing : Routes events based on their content/classification Integration with Standard Workflows : Can be used seamlessly with regular workflows Timeout Support : Prevents workflows from hanging indefinitely Stateful Processing : Maintains context between events","title":"Key Benefits"},{"location":"adrs/0009-event-driven-workflow-pattern/#implementation-approach","text":"We have implemented this pattern in the floxide-event crate, which provides: The core traits and interfaces for event-driven nodes Basic event source implementations (channel-based) Adapters for integrating with standard workflows Workflows for orchestrating event-driven nodes The implementation has been designed to be: Type-safe : Using Rust's type system to ensure correctness Async-first : Built around Tokio's async ecosystem Composable : Can be used alongside other framework components Extensible : New event sources can be added without modifying the core","title":"Implementation Approach"},{"location":"adrs/0009-event-driven-workflow-pattern/#example-use-cases","text":"IoT Monitoring : Processing temperature/humidity readings from sensors Chat Application : Handling incoming messages in a chat system Job Queue : Processing jobs from a distributed queue User Session : Handling events during a user session","title":"Example Use Cases"},{"location":"adrs/0009-event-driven-workflow-pattern/#consequences","text":"","title":"Consequences"},{"location":"adrs/0009-event-driven-workflow-pattern/#positive","text":"Enables event-driven application patterns within the Flow Framework Allows for integration with external event sources Maintains type safety throughout the event pipeline Fits naturally into the existing node/action framework model","title":"Positive"},{"location":"adrs/0009-event-driven-workflow-pattern/#negative","text":"Adds complexity to the framework Requires understanding of both workflow and event concepts Might increase the learning curve for new users","title":"Negative"},{"location":"adrs/0009-event-driven-workflow-pattern/#neutral","text":"Requires additional testing of asynchronous patterns May need refinement as real-world usage patterns emerge","title":"Neutral"},{"location":"adrs/0010-workflow-cloning-strategy/","text":"ADR-0010: Workflow Cloning Strategy \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 Our codebase has multiple areas where we need to clone or share workflows: In the Workflow::from_arc method, which attempts to clone a workflow from an Arc<Workflow> In the BatchNode implementation, which stores workflows in an Arc and clones the reference for each worker task However, we're encountering issues because: Box<dyn Node<...>> does not implement Clone , preventing direct cloning of the nodes HashMap We need to preserve the ability to share workflows between tasks efficiently There are logical ownership constraints in async code that prevent simply using references with lifetimes Decision \u00b6 We will take a multi-pronged approach to solve workflow cloning issues: 1. Use Arc for Node Storage \u00b6 Instead of storing nodes directly in Box<dyn Node<...>> , we'll store them in Arc<dyn Node<...>> : pub ( crate ) nodes : HashMap < NodeId , Arc < dyn Node < Context , A , Output = Output >>> , This allows easy cloning of the entire node collection without duplicating the actual node implementations. 2. Implement Clone for Workflow \u00b6 We'll implement a proper Clone implementation for Workflow that clones the structure but shares the node implementations: impl < Context , A , Output > Clone for Workflow < Context , A , Output > where Context : Send + Sync + ' static , A : ActionType + Clone + Send + Sync + ' static , Output : Send + Sync + ' static , { fn clone ( & self ) -> Self { Self { start_node : self . start_node . clone (), nodes : self . nodes . clone (), // Now possible because we're using Arc edges : self . edges . clone (), default_routes : self . default_routes . clone (), } } } 3. Remove from_arc Method \u00b6 The Workflow::from_arc method will be removed since it's no longer necessary - Arc can now be dereferenced and cloned directly. 4. Refactor BatchNode to Leverage Cloning \u00b6 The BatchNode will be updated to use this clone capability rather than wrapping the workflow in an Arc: pub struct BatchNode < Context , ItemType , A = crate :: action :: DefaultAction > where Context : BatchContext < ItemType > + Send + Sync + ' static , ItemType : Clone + Send + Sync + ' static , A : ActionType + Clone + Send + Sync + ' static , { id : NodeId , item_workflow : Workflow < Context , A > , // No longer an Arc parallelism : usize , _phantom : PhantomData < ( Context , ItemType , A ) > , } // In the process method: let workflow = self . item_workflow . clone (); // Now directly cloneable Consequences \u00b6 Advantages \u00b6 Cleaner API : No need for Arc-specific methods Memory Efficiency : Node implementations are shared, not duplicated Thread Safety : Arc provides thread-safe reference counting Type Safety : Cloning is now properly supported at the type level Disadvantages \u00b6 Indirection Cost : Extra indirection through Arc when accessing nodes Memory Overhead : Arc has a small overhead per reference API Changes : Will require changes to code that expects Box Migration Plan \u00b6 First, update the Workflow struct to use Arc<dyn Node> instead of Box<dyn Node> Implement Clone for Workflow Update the BatchNode implementation to leverage this new capability Remove the now-redundant from_arc method Update tests to verify correct cloning behavior Alternatives Considered \u00b6 Use Clone Trait Objects \u00b6 We considered making Node require Clone , but this would be problematic because: Trait objects cannot use clone to return a new trait object It would require all node implementations to implement Clone Keep Arc as the Primary Interface \u00b6 We considered embracing Arc as the primary way to share workflows, but this would make the API more cumbersome and push complexity to the caller. Use Cow (Clone-on-Write) \u00b6 We explored using Cow to defer cloning until mutation, but this added complexity without significant benefits given our usage patterns. Implementation Notes \u00b6 The change to Arc will be backward compatible for most code that consumes nodes We'll need to update node creation code to wrap nodes in Arc instead of Box This change reinforces the immutability of nodes once created","title":"ADR-0010: Workflow Cloning Strategy"},{"location":"adrs/0010-workflow-cloning-strategy/#adr-0010-workflow-cloning-strategy","text":"","title":"ADR-0010: Workflow Cloning Strategy"},{"location":"adrs/0010-workflow-cloning-strategy/#status","text":"Accepted","title":"Status"},{"location":"adrs/0010-workflow-cloning-strategy/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0010-workflow-cloning-strategy/#context","text":"Our codebase has multiple areas where we need to clone or share workflows: In the Workflow::from_arc method, which attempts to clone a workflow from an Arc<Workflow> In the BatchNode implementation, which stores workflows in an Arc and clones the reference for each worker task However, we're encountering issues because: Box<dyn Node<...>> does not implement Clone , preventing direct cloning of the nodes HashMap We need to preserve the ability to share workflows between tasks efficiently There are logical ownership constraints in async code that prevent simply using references with lifetimes","title":"Context"},{"location":"adrs/0010-workflow-cloning-strategy/#decision","text":"We will take a multi-pronged approach to solve workflow cloning issues:","title":"Decision"},{"location":"adrs/0010-workflow-cloning-strategy/#1-use-arc-for-node-storage","text":"Instead of storing nodes directly in Box<dyn Node<...>> , we'll store them in Arc<dyn Node<...>> : pub ( crate ) nodes : HashMap < NodeId , Arc < dyn Node < Context , A , Output = Output >>> , This allows easy cloning of the entire node collection without duplicating the actual node implementations.","title":"1. Use Arc for Node Storage"},{"location":"adrs/0010-workflow-cloning-strategy/#2-implement-clone-for-workflow","text":"We'll implement a proper Clone implementation for Workflow that clones the structure but shares the node implementations: impl < Context , A , Output > Clone for Workflow < Context , A , Output > where Context : Send + Sync + ' static , A : ActionType + Clone + Send + Sync + ' static , Output : Send + Sync + ' static , { fn clone ( & self ) -> Self { Self { start_node : self . start_node . clone (), nodes : self . nodes . clone (), // Now possible because we're using Arc edges : self . edges . clone (), default_routes : self . default_routes . clone (), } } }","title":"2. Implement Clone for Workflow"},{"location":"adrs/0010-workflow-cloning-strategy/#3-remove-from_arc-method","text":"The Workflow::from_arc method will be removed since it's no longer necessary - Arc can now be dereferenced and cloned directly.","title":"3. Remove from_arc Method"},{"location":"adrs/0010-workflow-cloning-strategy/#4-refactor-batchnode-to-leverage-cloning","text":"The BatchNode will be updated to use this clone capability rather than wrapping the workflow in an Arc: pub struct BatchNode < Context , ItemType , A = crate :: action :: DefaultAction > where Context : BatchContext < ItemType > + Send + Sync + ' static , ItemType : Clone + Send + Sync + ' static , A : ActionType + Clone + Send + Sync + ' static , { id : NodeId , item_workflow : Workflow < Context , A > , // No longer an Arc parallelism : usize , _phantom : PhantomData < ( Context , ItemType , A ) > , } // In the process method: let workflow = self . item_workflow . clone (); // Now directly cloneable","title":"4. Refactor BatchNode to Leverage Cloning"},{"location":"adrs/0010-workflow-cloning-strategy/#consequences","text":"","title":"Consequences"},{"location":"adrs/0010-workflow-cloning-strategy/#advantages","text":"Cleaner API : No need for Arc-specific methods Memory Efficiency : Node implementations are shared, not duplicated Thread Safety : Arc provides thread-safe reference counting Type Safety : Cloning is now properly supported at the type level","title":"Advantages"},{"location":"adrs/0010-workflow-cloning-strategy/#disadvantages","text":"Indirection Cost : Extra indirection through Arc when accessing nodes Memory Overhead : Arc has a small overhead per reference API Changes : Will require changes to code that expects Box","title":"Disadvantages"},{"location":"adrs/0010-workflow-cloning-strategy/#migration-plan","text":"First, update the Workflow struct to use Arc<dyn Node> instead of Box<dyn Node> Implement Clone for Workflow Update the BatchNode implementation to leverage this new capability Remove the now-redundant from_arc method Update tests to verify correct cloning behavior","title":"Migration Plan"},{"location":"adrs/0010-workflow-cloning-strategy/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0010-workflow-cloning-strategy/#use-clone-trait-objects","text":"We considered making Node require Clone , but this would be problematic because: Trait objects cannot use clone to return a new trait object It would require all node implementations to implement Clone","title":"Use Clone Trait Objects"},{"location":"adrs/0010-workflow-cloning-strategy/#keep-arc-as-the-primary-interface","text":"We considered embracing Arc as the primary way to share workflows, but this would make the API more cumbersome and push complexity to the caller.","title":"Keep Arc as the Primary Interface"},{"location":"adrs/0010-workflow-cloning-strategy/#use-cow-clone-on-write","text":"We explored using Cow to defer cloning until mutation, but this added complexity without significant benefits given our usage patterns.","title":"Use Cow (Clone-on-Write)"},{"location":"adrs/0010-workflow-cloning-strategy/#implementation-notes","text":"The change to Arc will be backward compatible for most code that consumes nodes We'll need to update node creation code to wrap nodes in Arc instead of Box This change reinforces the immutability of nodes once created","title":"Implementation Notes"},{"location":"adrs/0011-closure-lifetime-management/","text":"ADR-0011: Closure Lifetime Management in Async Contexts \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 We're encountering lifetime issues when using closures in async contexts, particularly in our lifecycle_node function. The problem occurs because: The closures capture references to local variables The async blocks created from these closures must satisfy lifetime bounds The compiler can't guarantee that the references will live long enough when the Future is awaited Specifically, we're seeing errors like: lifetime may not live long enough returning this value requires that `'1` must outlive `'2` This is a common issue in Rust's async ecosystem when closures capture references and are then returned from functions. Decision \u00b6 We will adopt a multi-faceted approach to handling closure lifetimes in async contexts: 1. Use 'static Types for Closure Inputs and Outputs \u00b6 Any data passed into or out of closures used in async contexts will be required to satisfy the 'static lifetime bound. This ensures the data will live for the entire program duration: where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , PrepOut : Send + Sync + Clone + ' static , ExecOut : Send + Sync + Clone + ' static , 2. Explicitly Use move Closures \u00b6 We'll always use the move keyword when defining closures that will be used in async contexts to ensure ownership is transferred into the closure: move | ctx : & mut TestContext | async move { // Closure body } 3. Document Function-specific Lifetime Requirements \u00b6 For functions that take closures, we'll document that the closures: Must be move closures Any captured data must meet the 'static lifetime bound References passed as parameters follow the normal borrowing rules 4. Create Helper Types for Complex Cases \u00b6 In cases where we need to capture references with specific lifetimes, we'll create dedicated structs with explicit lifetime parameters: struct ContextBorrower <' a , T > { context : & ' a mut T , } impl <' a , T > ContextBorrower <' a , T > { async fn process_with_context < F , Fut > ( & mut self , f : F ) -> Fut :: Output where F : FnOnce ( & mut T ) -> Fut , Fut : Future , { f ( self . context ). await } } Consequences \u00b6 Advantages \u00b6 Type Safety : The compiler enforces our lifetime constraints Clear Requirements : Using move consistently makes ownership transfer explicit Reduced Bugs : Avoids subtle lifetime bugs that could manifest at runtime Better Composability : Working with 'static data makes composition easier Disadvantages \u00b6 More Constraints : Requires data to be owned or 'static Additional Complexity : May require additional cloning in some cases Learning Curve : Developers need to understand the reasons for these patterns Alternatives Considered \u00b6 Static Function References Instead of Closures \u00b6 We considered using static function references ( fn() -> ... ) instead of closures, but this would severely limit the expressiveness of our API. Allocating Contexts on the Heap \u00b6 We explored allocating all context data on the heap with Box or Arc , but this complicates the API and introduces unnecessary allocation overhead. Returning Impl Future with Explicit Lifetimes \u00b6 We investigated returning impl Future + 'a with explicit lifetimes, but this propagates lifetime complexity throughout the API and is difficult to implement correctly. Implementation Notes \u00b6 We'll update all tests to use move closures consistently Documentation will explicitly mention the need for move in async contexts Examples will demonstrate correct lifetime handling patterns We'll consider introducing a linter rule to enforce move for async closures","title":"ADR-0011: Closure Lifetime Management in Async Contexts"},{"location":"adrs/0011-closure-lifetime-management/#adr-0011-closure-lifetime-management-in-async-contexts","text":"","title":"ADR-0011: Closure Lifetime Management in Async Contexts"},{"location":"adrs/0011-closure-lifetime-management/#status","text":"Accepted","title":"Status"},{"location":"adrs/0011-closure-lifetime-management/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0011-closure-lifetime-management/#context","text":"We're encountering lifetime issues when using closures in async contexts, particularly in our lifecycle_node function. The problem occurs because: The closures capture references to local variables The async blocks created from these closures must satisfy lifetime bounds The compiler can't guarantee that the references will live long enough when the Future is awaited Specifically, we're seeing errors like: lifetime may not live long enough returning this value requires that `'1` must outlive `'2` This is a common issue in Rust's async ecosystem when closures capture references and are then returned from functions.","title":"Context"},{"location":"adrs/0011-closure-lifetime-management/#decision","text":"We will adopt a multi-faceted approach to handling closure lifetimes in async contexts:","title":"Decision"},{"location":"adrs/0011-closure-lifetime-management/#1-use-static-types-for-closure-inputs-and-outputs","text":"Any data passed into or out of closures used in async contexts will be required to satisfy the 'static lifetime bound. This ensures the data will live for the entire program duration: where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , PrepOut : Send + Sync + Clone + ' static , ExecOut : Send + Sync + Clone + ' static ,","title":"1. Use 'static Types for Closure Inputs and Outputs"},{"location":"adrs/0011-closure-lifetime-management/#2-explicitly-use-move-closures","text":"We'll always use the move keyword when defining closures that will be used in async contexts to ensure ownership is transferred into the closure: move | ctx : & mut TestContext | async move { // Closure body }","title":"2. Explicitly Use move Closures"},{"location":"adrs/0011-closure-lifetime-management/#3-document-function-specific-lifetime-requirements","text":"For functions that take closures, we'll document that the closures: Must be move closures Any captured data must meet the 'static lifetime bound References passed as parameters follow the normal borrowing rules","title":"3. Document Function-specific Lifetime Requirements"},{"location":"adrs/0011-closure-lifetime-management/#4-create-helper-types-for-complex-cases","text":"In cases where we need to capture references with specific lifetimes, we'll create dedicated structs with explicit lifetime parameters: struct ContextBorrower <' a , T > { context : & ' a mut T , } impl <' a , T > ContextBorrower <' a , T > { async fn process_with_context < F , Fut > ( & mut self , f : F ) -> Fut :: Output where F : FnOnce ( & mut T ) -> Fut , Fut : Future , { f ( self . context ). await } }","title":"4. Create Helper Types for Complex Cases"},{"location":"adrs/0011-closure-lifetime-management/#consequences","text":"","title":"Consequences"},{"location":"adrs/0011-closure-lifetime-management/#advantages","text":"Type Safety : The compiler enforces our lifetime constraints Clear Requirements : Using move consistently makes ownership transfer explicit Reduced Bugs : Avoids subtle lifetime bugs that could manifest at runtime Better Composability : Working with 'static data makes composition easier","title":"Advantages"},{"location":"adrs/0011-closure-lifetime-management/#disadvantages","text":"More Constraints : Requires data to be owned or 'static Additional Complexity : May require additional cloning in some cases Learning Curve : Developers need to understand the reasons for these patterns","title":"Disadvantages"},{"location":"adrs/0011-closure-lifetime-management/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0011-closure-lifetime-management/#static-function-references-instead-of-closures","text":"We considered using static function references ( fn() -> ... ) instead of closures, but this would severely limit the expressiveness of our API.","title":"Static Function References Instead of Closures"},{"location":"adrs/0011-closure-lifetime-management/#allocating-contexts-on-the-heap","text":"We explored allocating all context data on the heap with Box or Arc , but this complicates the API and introduces unnecessary allocation overhead.","title":"Allocating Contexts on the Heap"},{"location":"adrs/0011-closure-lifetime-management/#returning-impl-future-with-explicit-lifetimes","text":"We investigated returning impl Future + 'a with explicit lifetimes, but this propagates lifetime complexity throughout the API and is difficult to implement correctly.","title":"Returning Impl Future with Explicit Lifetimes"},{"location":"adrs/0011-closure-lifetime-management/#implementation-notes","text":"We'll update all tests to use move closures consistently Documentation will explicitly mention the need for move in async contexts Examples will demonstrate correct lifetime handling patterns We'll consider introducing a linter rule to enforce move for async closures","title":"Implementation Notes"},{"location":"adrs/0012-test-implementation-patterns/","text":"ADR-0012: Testing Patterns for Async Node Implementations \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 Testing async code in Rust presents several challenges, particularly when it comes to: Lifetime issues with closures that capture variables Type inference complexities with async closures Implementation constraints when using trait objects Testing behavior that relies on future resolution We encountered specific issues when testing our lifecycle node implementation: Lifetime errors when using closures directly in tests Difficulty creating reusable test patterns Readability and maintainability of test code Decision \u00b6 We will adopt a set of testing patterns specifically for our async node implementations: 1. Use Concrete Implementations for Testing \u00b6 Instead of using closures and the helper functions like lifecycle_node() , we'll create concrete test implementations of the traits: // For testing LifecycleNode struct TestLifecycleNode { id : NodeId , } #[async_trait] impl LifecycleNode < TestContext , DefaultAction > for TestLifecycleNode { type PrepOutput = i32 ; type ExecOutput = i32 ; fn id ( & self ) -> NodeId { self . id . clone () } async fn prep ( & self , ctx : & mut TestContext ) -> Result < i32 , FloxideError > { // Test-specific implementation... Ok ( 42 ) } // ... other method implementations } 2. Test Adapters Directly \u00b6 Test adapter implementations directly rather than through helper functions: #[tokio::test] async fn test_lifecycle_node () { let lifecycle_node = TestLifecycleNode { id : \"test-node\" . to_string () }; let node = LifecycleNodeAdapter :: new ( lifecycle_node ); // Test node behavior... } 3. Create Factory Functions for Complex Setup \u00b6 For tests requiring complex setup, use factory functions that return fully initialized test objects: fn create_test_workflow () -> Workflow < TestContext , DefaultAction > { let start_node = TestNode { id : \"start\" . to_string () }; let mut workflow = Workflow :: new ( start_node ); // Add more nodes, configure workflow workflow } 4. Use Type Aliases for Complex Types \u00b6 When working with complex generic types, define type aliases to improve readability: type TestWorkflow = Workflow < TestContext , DefaultAction , i32 > ; type TestBatchNode = BatchNode < TestBatchContext , Item , DefaultAction > ; Consequences \u00b6 Advantages \u00b6 No Lifetime Issues : By using concrete implementations, we avoid the lifetime issues common with closures Clear Test Intent : Tests are more explicit about what they're testing Better Test Organization : Test objects can be reused across multiple tests Easier Debugging : When tests fail, it's clearer where the failure occurs Isolated Test Logic : Each test component has a clear responsibility Disadvantages \u00b6 More Boilerplate : Requires more code to set up tests Lower Test-to-Code Ratio : Tests may be significantly longer than the code they test Learning Curve : New team members need to understand the testing patterns Alternatives Considered \u00b6 Using Box<dyn Fn...> for Lifecycle Closures \u00b6 We considered changing the lifecycle_node function to accept boxed closures: pub fn lifecycle_node < Context , Action , PrepOut , ExecOut > ( id : Option < String > , prep_fn : Box < dyn Fn ( & mut Context ) -> BoxFuture <' _ , Result < PrepOut , FloxideError >> + Send + Sync > , // ... ) This would allow for easier use in tests, but would make the API more cumbersome for normal use. Testing Helper Functions Directly \u00b6 We considered writing tests specifically for helper functions like lifecycle_node , which would allow simpler test cases. However, this wouldn't test the integration with the Node trait. Using Helper Macros for Tests \u00b6 We explored creating test macros that would handle the boilerplate, but this would hide important details and make debugging more difficult. Implementation Notes \u00b6 We will update existing tests to follow these patterns We will document these patterns in the project's testing guidelines Test modules will be organized to match the structure of the code they test Helper modules may be created for shared test components Test coverage should focus on behavior, not implementation details","title":"ADR-0012: Testing Patterns for Async Node Implementations"},{"location":"adrs/0012-test-implementation-patterns/#adr-0012-testing-patterns-for-async-node-implementations","text":"","title":"ADR-0012: Testing Patterns for Async Node Implementations"},{"location":"adrs/0012-test-implementation-patterns/#status","text":"Accepted","title":"Status"},{"location":"adrs/0012-test-implementation-patterns/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0012-test-implementation-patterns/#context","text":"Testing async code in Rust presents several challenges, particularly when it comes to: Lifetime issues with closures that capture variables Type inference complexities with async closures Implementation constraints when using trait objects Testing behavior that relies on future resolution We encountered specific issues when testing our lifecycle node implementation: Lifetime errors when using closures directly in tests Difficulty creating reusable test patterns Readability and maintainability of test code","title":"Context"},{"location":"adrs/0012-test-implementation-patterns/#decision","text":"We will adopt a set of testing patterns specifically for our async node implementations:","title":"Decision"},{"location":"adrs/0012-test-implementation-patterns/#1-use-concrete-implementations-for-testing","text":"Instead of using closures and the helper functions like lifecycle_node() , we'll create concrete test implementations of the traits: // For testing LifecycleNode struct TestLifecycleNode { id : NodeId , } #[async_trait] impl LifecycleNode < TestContext , DefaultAction > for TestLifecycleNode { type PrepOutput = i32 ; type ExecOutput = i32 ; fn id ( & self ) -> NodeId { self . id . clone () } async fn prep ( & self , ctx : & mut TestContext ) -> Result < i32 , FloxideError > { // Test-specific implementation... Ok ( 42 ) } // ... other method implementations }","title":"1. Use Concrete Implementations for Testing"},{"location":"adrs/0012-test-implementation-patterns/#2-test-adapters-directly","text":"Test adapter implementations directly rather than through helper functions: #[tokio::test] async fn test_lifecycle_node () { let lifecycle_node = TestLifecycleNode { id : \"test-node\" . to_string () }; let node = LifecycleNodeAdapter :: new ( lifecycle_node ); // Test node behavior... }","title":"2. Test Adapters Directly"},{"location":"adrs/0012-test-implementation-patterns/#3-create-factory-functions-for-complex-setup","text":"For tests requiring complex setup, use factory functions that return fully initialized test objects: fn create_test_workflow () -> Workflow < TestContext , DefaultAction > { let start_node = TestNode { id : \"start\" . to_string () }; let mut workflow = Workflow :: new ( start_node ); // Add more nodes, configure workflow workflow }","title":"3. Create Factory Functions for Complex Setup"},{"location":"adrs/0012-test-implementation-patterns/#4-use-type-aliases-for-complex-types","text":"When working with complex generic types, define type aliases to improve readability: type TestWorkflow = Workflow < TestContext , DefaultAction , i32 > ; type TestBatchNode = BatchNode < TestBatchContext , Item , DefaultAction > ;","title":"4. Use Type Aliases for Complex Types"},{"location":"adrs/0012-test-implementation-patterns/#consequences","text":"","title":"Consequences"},{"location":"adrs/0012-test-implementation-patterns/#advantages","text":"No Lifetime Issues : By using concrete implementations, we avoid the lifetime issues common with closures Clear Test Intent : Tests are more explicit about what they're testing Better Test Organization : Test objects can be reused across multiple tests Easier Debugging : When tests fail, it's clearer where the failure occurs Isolated Test Logic : Each test component has a clear responsibility","title":"Advantages"},{"location":"adrs/0012-test-implementation-patterns/#disadvantages","text":"More Boilerplate : Requires more code to set up tests Lower Test-to-Code Ratio : Tests may be significantly longer than the code they test Learning Curve : New team members need to understand the testing patterns","title":"Disadvantages"},{"location":"adrs/0012-test-implementation-patterns/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0012-test-implementation-patterns/#using-boxdyn-fn-for-lifecycle-closures","text":"We considered changing the lifecycle_node function to accept boxed closures: pub fn lifecycle_node < Context , Action , PrepOut , ExecOut > ( id : Option < String > , prep_fn : Box < dyn Fn ( & mut Context ) -> BoxFuture <' _ , Result < PrepOut , FloxideError >> + Send + Sync > , // ... ) This would allow for easier use in tests, but would make the API more cumbersome for normal use.","title":"Using Box&lt;dyn Fn...&gt; for Lifecycle Closures"},{"location":"adrs/0012-test-implementation-patterns/#testing-helper-functions-directly","text":"We considered writing tests specifically for helper functions like lifecycle_node , which would allow simpler test cases. However, this wouldn't test the integration with the Node trait.","title":"Testing Helper Functions Directly"},{"location":"adrs/0012-test-implementation-patterns/#using-helper-macros-for-tests","text":"We explored creating test macros that would handle the boilerplate, but this would hide important details and make debugging more difficult.","title":"Using Helper Macros for Tests"},{"location":"adrs/0012-test-implementation-patterns/#implementation-notes","text":"We will update existing tests to follow these patterns We will document these patterns in the project's testing guidelines Test modules will be organized to match the structure of the code they test Helper modules may be created for shared test components Test coverage should focus on behavior, not implementation details","title":"Implementation Notes"},{"location":"adrs/0013-workflow-patterns/","text":"ADR-0013: Workflow Patterns \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 The Floxide framework is designed as a directed graph workflow system that can support various workflow patterns. To provide a comprehensive and flexible framework, we need to define and document the core workflow patterns that the framework will support. These patterns represent common use cases and architectural approaches for building workflows. Users of the framework need clear guidance on how to implement different types of workflows to solve various business problems. By defining these patterns explicitly, we can ensure the framework's design accommodates all these use cases and provides appropriate abstractions. Decision \u00b6 We will support eight core workflow patterns in the Floxide framework, each addressing different workflow requirements: 1. Node \u00b6 Description : A single step operation that processes input and produces output. Implementation : This is the most basic building block, implemented through the Node trait. pub trait Node < Context , A : ActionType > { type Output ; async fn process ( & self , context : & mut Context ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > ; } Example Use Case : A node that summarizes an email as a standalone operation. 2. Chain \u00b6 Description : A sequence of connected nodes where the output of one node becomes the input to the next. Implementation : Implemented through the chain combinator function that connects nodes sequentially. pub fn chain < C , A , N1 , N2 > ( first : N1 , second : N2 ) -> impl Node < C , A > where N1 : Node < C , A > , N2 : Node < C , A > , A : ActionType , { // Implementation details } Example Use Case : A workflow that first summarizes an email and then drafts a reply based on that summary. 3. Batch \u00b6 Description : Repeats the same processing step across multiple inputs in parallel. Implementation : Implemented through the BatchFlow struct that processes multiple contexts with the same node. pub struct BatchFlow < N , C , A > { node : N , _phantom : PhantomData < ( C , A ) > , } impl < N , C , A > BatchFlow < N , C , A > where N : Node < C , A > + Clone , C : Clone , A : ActionType , { pub fn new ( node : N ) -> Self { // Implementation details } pub async fn execute_all ( & self , contexts : & mut [ C ]) -> Vec < Result < N :: Output , FloxideError >> { // Implementation details } } Example Use Case : Summarizing multiple emails simultaneously, improving throughput for repetitive tasks. 4. Async \u00b6 Description : Handles operations that involve waiting for I/O or external events. Implementation : Leverages Rust's async/await syntax and the Tokio runtime for asynchronous execution. pub async fn execute < C , A > ( & self , context : & mut C ) -> Result < Self :: Output , FloxideError > where A : ActionType , { // Implementation details with async/await } Example Use Case : Checking an inbox (which involves I/O wait) and then processing new emails when they arrive. 5. Shared \u00b6 Description : Enables communication between nodes through shared state. Implementation : Uses shared references or Arc > for state that needs to be accessed by multiple nodes. pub struct SharedState < T > { inner : Arc < Mutex < T >> , } impl < T > SharedState < T > { pub fn new ( value : T ) -> Self { // Implementation details } pub async fn read < F , R > ( & self , f : F ) -> R where F : FnOnce ( & T ) -> R , { // Implementation details } pub async fn write < F , R > ( & self , f : F ) -> R where F : FnOnce ( & mut T ) -> R , { // Implementation details } } Example Use Case : The \"Summarize Email\" node writes an email summary to a shared state, and the \"Draft Reply\" node reads from that shared state rather than receiving direct input from the previous node. 6. Branch \u00b6 Description : Implements conditional logic to determine the next step based on certain criteria. Implementation : Uses the ActionType enum to determine which branch to take based on the node outcome. pub fn branch < C , A , N1 , N2 , N3 > ( condition_node : N1 , true_branch : N2 , false_branch : N3 , ) -> impl Node < C , A > where N1 : Node < C , A , Output = bool > , N2 : Node < C , A > , N3 : Node < C , A > , A : ActionType , { // Implementation details } Example Use Case : After summarizing an email, we determine if it needs review. If it does, it goes to the \"Review\" node and then to \"Draft Reply\" after approval. If review is not needed, it goes directly to \"Draft Reply\". 7. Nesting \u00b6 Description : Allows workflows to be composed of other workflows, creating reusable components. Implementation : Workflows can be nested by creating a node that contains another workflow. pub struct NestedWorkflow < W , C , A > { workflow : W , _phantom : PhantomData < ( C , A ) > , } impl < W , C , A > Node < C , A > for NestedWorkflow < W , C , A > where W : Workflow < C , A > , A : ActionType , { type Output = W :: Output ; async fn process ( & self , context : & mut C ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > { // Implementation details } } Example Use Case : A \"Coding Task\" node that triggers a nested workflow for software development. This nested workflow includes writing tests, writing code, verifying code, and analyzing complexity. 8. Looping \u00b6 Description : Implements repetitive processes that continue until a condition is met. Implementation : Uses recursion or a loop combinator to repeat a node until a condition is met. pub fn loop_until < C , A , N , F > ( node : N , condition : F , ) -> impl Node < C , A > where N : Node < C , A > + Clone , F : Fn ( & N :: Output ) -> bool + Send + Sync + ' static , A : ActionType , { // Implementation details } Example Use Case : A long-running workflow that starts with \"Get Question\", proceeds to \"Answer Question\", and then loops back to \"Get Question\" to continue the cycle indefinitely. Pattern Composition \u00b6 A key architectural decision is to support the composition of these patterns to create more complex workflows. The framework is designed to allow these patterns to be combined in various ways: Composition Strategies \u00b6 Hierarchical Composition : Patterns can be nested within each other, such as a Branch pattern within a Loop, or a Chain within a Nested workflow. Sequential Composition : Patterns can be chained together, with the output of one pattern becoming the input to another. Parallel Composition : Multiple patterns can be executed in parallel using the Batch pattern. State-Sharing Composition : Different patterns can communicate through shared state. Common Composition Patterns \u00b6 Based on real-world use cases, we've identified several common composition patterns: Chain + Branch : Create sequential workflows with decision points let workflow = chain ( summarize_email_node , branch ( needs_review_node , review_node , draft_reply_node ) ); Batch + Async : Process multiple items in parallel while handling I/O operations let batch_workflow = BatchFlow :: new ( async_node ( check_inbox_and_process ) ); Nesting + Shared : Build reusable workflow components that communicate through shared state let shared_state = SharedState :: new ( EmailSummary :: default ()); let nested_workflow = NestedWorkflow :: new ( chain ( write_to_shared_state ( shared_state . clone (), summarize_email_node ), read_from_shared_state ( shared_state , draft_reply_node ) ) ); Looping + Branch : Implement iterative processes with exit conditions let workflow = loop_until ( chain ( get_question_node , answer_question_node , branch ( is_final_question_node , complete_node , continue_node ) ), | outcome | outcome . is_complete () ); Implementation Considerations \u00b6 When implementing pattern composition, we need to ensure: Type Safety : Composed patterns should maintain Rust's strong type safety. Error Propagation : Errors should propagate correctly through composed patterns. Performance : Composition should not introduce significant overhead. Readability : The API for composing patterns should be intuitive and readable. Visual Workflow Representation \u00b6 An important aspect of workflow design is the ability to visualize workflows. We've decided to adopt a standardized visual representation for each pattern to help users design and communicate their workflow architectures: Visual Pattern Language \u00b6 Each pattern has a distinct visual representation: Node : Single rectangle labeled with the operation (e.g., \"Summarize Email\") and noted as \"Single step\" Chain : Connected rectangles with directional arrows (e.g., \"Summarize Email\" \u2192 \"Draft Reply\") and noted as \"Multiple steps\" Batch : Multiple stacked rectangles representing parallel instances of the same node (e.g., stacked \"Summarize Email\" boxes) and noted as \"Repeat steps\" Async : Rectangle with a clock icon indicating waiting for events (e.g., \"Check Inbox\" with clock) and noted as \"I/O wait\" Shared : Connected nodes with a dashed rectangle for shared state and dotted arrows indicating read/write operations (e.g., \"Email summary\" state between nodes with write/read arrows) and noted as \"Communication\" Branch : Node with multiple outgoing paths based on conditions (e.g., \"Need review\" vs. direct paths) and noted as \"Conditional step\" Nesting : Node that expands to reveal a contained workflow within a dashed border (e.g., \"Coding Task\" expanding to reveal test/code/verify/analyze steps) and noted as \"Reusable step\" Looping : Nodes with return arrows forming a cycle (e.g., \"Get Question\" \u2192 \"Answer Question\" with return arrow) and noted as \"Long running step\" For paradigm patterns, we use additional visual indicators: Workflow : Simple directed path with sequential nodes, labeled \"Directed Path\" Map-Reduce : Batch + Merge pattern showing data chunking, parallel processing, and result aggregation Chat : Single node with self-loop and shared state for history, labeled \"Loop (+ chat history store)\" RAG : Nodes with vector database as shared state, labeled \"(+ Vector DB store)\" Chain of Thought : Single node with self-loop and history tracking, labeled \"Loop (+ think history store)\" Agent : Nodes with branching and feedback loops, labeled \"Loop + Branching\" Multi-Agent : Complex interconnected graph with shared pub/sub communication, labeled \"Loop + Branching (+ pub/sub)\" Supervisor : Nested workflow with approval/rejection paths, labeled \"Nesting\" Complex Workflow Example \u00b6 Here's an example of a complex email processing workflow that combines multiple patterns: graph TD Start[Start] --> CheckInbox[\"Check Inbox (Async)\"] CheckInbox --> HasEmails{Has New Emails?} HasEmails -->|No| Wait[\"Wait (Async)\"] Wait --> CheckInbox HasEmails -->|Yes| BatchProcess[\"Process Emails (Batch)\"] subgraph \"For Each Email\" BatchProcess --> Summarize[\"Summarize Email\"] Summarize --> NeedsReview{Needs Review?} NeedsReview -->|Yes| Review[\"Review Email\"] NeedsReview -->|No| Draft[\"Draft Reply\"] Review --> Draft subgraph \"Draft Reply Process (Nested)\" Draft --> GenerateTemplate[\"Generate Template\"] GenerateTemplate --> AddPersonalization[\"Add Personalization\"] AddPersonalization --> CheckGrammar[\"Check Grammar\"] end CheckGrammar --> SendReply[\"Send Reply\"] end SendReply --> UpdateStatus[\"Update Status (Shared State)\"] UpdateStatus --> CheckInbox This visualization demonstrates how multiple patterns can be combined to create a comprehensive workflow solution. Benefits of Visual Representation \u00b6 Communication : Facilitates discussion about workflow design among team members Documentation : Provides clear documentation of workflow architecture Design : Helps identify optimization opportunities and potential issues Onboarding : Makes it easier for new developers to understand existing workflows Implementation \u00b6 While the framework itself doesn't include visualization tools, we recommend using Mermaid or similar tools to create workflow diagrams following our visual pattern language. These diagrams can be included in documentation and design discussions. Paradigm Patterns \u00b6 In addition to the core workflow patterns, we recognize that the framework will be used to implement specialized workflow types for AI and automation applications. These specialized workflows can be built by combining the core patterns in specific ways: 1. Standard Workflow \u00b6 Description : Simple directed path workflow with sequential processing. Implementation : Primarily uses the Chain pattern for a direct processing path. Example Use Case : Email processing with \"Summarize Email\" followed by \"Draft Reply\". 2. Chat Workflow \u00b6 Description : Looping conversation flow with state management for chat history. Implementation : Combines Looping pattern with Shared State to maintain conversation context. Example Use Case : Chat system with a single \"Chat\" node that loops back to itself, writing to and reading from a \"chat history\" shared store. 3. RAG (Retrieval Augmented Generation) \u00b6 Description : Workflows that integrate document storage and retrieval for question answering. Implementation : Combines Shared State (for vector database) with processing nodes for upload and question answering. Example Use Case : A system with \"Upload documents\" node that writes to a \"Vector DB\" shared store, and an \"Answer Questions\" node that reads from it. 4. Chain of Thought (CoT) \u00b6 Description : Single \"thinking\" step that loops and maintains reasoning history. Implementation : Combines Looping with Shared State to store thinking steps and intermediate reasoning. Example Use Case : Reasoning system with a single \"Think\" node that loops back to itself, writing to and reading from a \"think history\" shared store. 5. Map Reduce \u00b6 Description : Batch processing of data chunks followed by aggregation of results. Implementation : Combines Batch (for mapping chunks) with a reduction node that aggregates results. Example Use Case : System that uses \"Map Chunks\" to distribute work to multiple \"Summarize Chunk\" operations in parallel, then aggregates results with \"Reduce Summaries\". 6. Agent \u00b6 Description : Autonomous workflows with branching decision logic and feedback loops. Implementation : Combines Looping and Branch patterns to create a decision-action cycle. Example Use Case : Email processing agent that summarizes an email, branches to either \"Review\" or \"Draft Reply\" based on content, and continues processing. 7. Multi-agent \u00b6 Description : Complex interconnected workflows with multiple processing nodes and publish/subscribe communication. Implementation : Uses multiple Node workflows with pub/sub shared state for coordination and communication. Example Use Case : Distributed system where multiple nodes communicate through a shared pub/sub message bus, creating a complex processing graph. 8. Supervisor \u00b6 Description : Nested workflows with oversight that can approve or reject work. Implementation : Combines Nesting pattern with Branch for approval/rejection decisions. Example Use Case : Quality control system where a \"Supervise\" node reviews the output of a nested workflow and can either approve it to continue or reject it back for rework. Consequences \u00b6 Positive \u00b6 Comprehensive Framework : By supporting these eight patterns, Floxide can address a wide range of workflow requirements. Clear Documentation : Explicitly defining these patterns provides clear guidance to users on how to implement different types of workflows. Consistent Design : Having these patterns defined upfront ensures the framework's design is consistent and accommodates all these use cases. Flexibility : The combination of these patterns allows for complex workflow designs that can solve real-world problems. Negative \u00b6 Implementation Complexity : Supporting all these patterns increases the complexity of the framework implementation. Learning Curve : Users need to understand multiple patterns to effectively use the framework. Maintenance Overhead : More patterns mean more code to maintain and test. Neutral \u00b6 Pattern Combinations : These patterns can be combined in various ways, which provides flexibility but also requires careful design. Performance Considerations : Different patterns may have different performance characteristics that users need to be aware of. Alternatives Considered \u00b6 Simplified Pattern Set : We considered supporting fewer patterns for simplicity, but decided that the comprehensive set provides more value to users. Different Implementation Approaches : For each pattern, we considered multiple implementation approaches before settling on the current design. External DSL : We considered creating an external DSL for workflow definition but decided that a code-based approach using Rust's type system provides better safety and flexibility. References \u00b6 Workflow Patterns Initiative Enterprise Integration Patterns Rust Async Book","title":"ADR-0013: Workflow Patterns"},{"location":"adrs/0013-workflow-patterns/#adr-0013-workflow-patterns","text":"","title":"ADR-0013: Workflow Patterns"},{"location":"adrs/0013-workflow-patterns/#status","text":"Accepted","title":"Status"},{"location":"adrs/0013-workflow-patterns/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0013-workflow-patterns/#context","text":"The Floxide framework is designed as a directed graph workflow system that can support various workflow patterns. To provide a comprehensive and flexible framework, we need to define and document the core workflow patterns that the framework will support. These patterns represent common use cases and architectural approaches for building workflows. Users of the framework need clear guidance on how to implement different types of workflows to solve various business problems. By defining these patterns explicitly, we can ensure the framework's design accommodates all these use cases and provides appropriate abstractions.","title":"Context"},{"location":"adrs/0013-workflow-patterns/#decision","text":"We will support eight core workflow patterns in the Floxide framework, each addressing different workflow requirements:","title":"Decision"},{"location":"adrs/0013-workflow-patterns/#1-node","text":"Description : A single step operation that processes input and produces output. Implementation : This is the most basic building block, implemented through the Node trait. pub trait Node < Context , A : ActionType > { type Output ; async fn process ( & self , context : & mut Context ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > ; } Example Use Case : A node that summarizes an email as a standalone operation.","title":"1. Node"},{"location":"adrs/0013-workflow-patterns/#2-chain","text":"Description : A sequence of connected nodes where the output of one node becomes the input to the next. Implementation : Implemented through the chain combinator function that connects nodes sequentially. pub fn chain < C , A , N1 , N2 > ( first : N1 , second : N2 ) -> impl Node < C , A > where N1 : Node < C , A > , N2 : Node < C , A > , A : ActionType , { // Implementation details } Example Use Case : A workflow that first summarizes an email and then drafts a reply based on that summary.","title":"2. Chain"},{"location":"adrs/0013-workflow-patterns/#3-batch","text":"Description : Repeats the same processing step across multiple inputs in parallel. Implementation : Implemented through the BatchFlow struct that processes multiple contexts with the same node. pub struct BatchFlow < N , C , A > { node : N , _phantom : PhantomData < ( C , A ) > , } impl < N , C , A > BatchFlow < N , C , A > where N : Node < C , A > + Clone , C : Clone , A : ActionType , { pub fn new ( node : N ) -> Self { // Implementation details } pub async fn execute_all ( & self , contexts : & mut [ C ]) -> Vec < Result < N :: Output , FloxideError >> { // Implementation details } } Example Use Case : Summarizing multiple emails simultaneously, improving throughput for repetitive tasks.","title":"3. Batch"},{"location":"adrs/0013-workflow-patterns/#4-async","text":"Description : Handles operations that involve waiting for I/O or external events. Implementation : Leverages Rust's async/await syntax and the Tokio runtime for asynchronous execution. pub async fn execute < C , A > ( & self , context : & mut C ) -> Result < Self :: Output , FloxideError > where A : ActionType , { // Implementation details with async/await } Example Use Case : Checking an inbox (which involves I/O wait) and then processing new emails when they arrive.","title":"4. Async"},{"location":"adrs/0013-workflow-patterns/#5-shared","text":"Description : Enables communication between nodes through shared state. Implementation : Uses shared references or Arc > for state that needs to be accessed by multiple nodes. pub struct SharedState < T > { inner : Arc < Mutex < T >> , } impl < T > SharedState < T > { pub fn new ( value : T ) -> Self { // Implementation details } pub async fn read < F , R > ( & self , f : F ) -> R where F : FnOnce ( & T ) -> R , { // Implementation details } pub async fn write < F , R > ( & self , f : F ) -> R where F : FnOnce ( & mut T ) -> R , { // Implementation details } } Example Use Case : The \"Summarize Email\" node writes an email summary to a shared state, and the \"Draft Reply\" node reads from that shared state rather than receiving direct input from the previous node.","title":"5. Shared"},{"location":"adrs/0013-workflow-patterns/#6-branch","text":"Description : Implements conditional logic to determine the next step based on certain criteria. Implementation : Uses the ActionType enum to determine which branch to take based on the node outcome. pub fn branch < C , A , N1 , N2 , N3 > ( condition_node : N1 , true_branch : N2 , false_branch : N3 , ) -> impl Node < C , A > where N1 : Node < C , A , Output = bool > , N2 : Node < C , A > , N3 : Node < C , A > , A : ActionType , { // Implementation details } Example Use Case : After summarizing an email, we determine if it needs review. If it does, it goes to the \"Review\" node and then to \"Draft Reply\" after approval. If review is not needed, it goes directly to \"Draft Reply\".","title":"6. Branch"},{"location":"adrs/0013-workflow-patterns/#7-nesting","text":"Description : Allows workflows to be composed of other workflows, creating reusable components. Implementation : Workflows can be nested by creating a node that contains another workflow. pub struct NestedWorkflow < W , C , A > { workflow : W , _phantom : PhantomData < ( C , A ) > , } impl < W , C , A > Node < C , A > for NestedWorkflow < W , C , A > where W : Workflow < C , A > , A : ActionType , { type Output = W :: Output ; async fn process ( & self , context : & mut C ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > { // Implementation details } } Example Use Case : A \"Coding Task\" node that triggers a nested workflow for software development. This nested workflow includes writing tests, writing code, verifying code, and analyzing complexity.","title":"7. Nesting"},{"location":"adrs/0013-workflow-patterns/#8-looping","text":"Description : Implements repetitive processes that continue until a condition is met. Implementation : Uses recursion or a loop combinator to repeat a node until a condition is met. pub fn loop_until < C , A , N , F > ( node : N , condition : F , ) -> impl Node < C , A > where N : Node < C , A > + Clone , F : Fn ( & N :: Output ) -> bool + Send + Sync + ' static , A : ActionType , { // Implementation details } Example Use Case : A long-running workflow that starts with \"Get Question\", proceeds to \"Answer Question\", and then loops back to \"Get Question\" to continue the cycle indefinitely.","title":"8. Looping"},{"location":"adrs/0013-workflow-patterns/#pattern-composition","text":"A key architectural decision is to support the composition of these patterns to create more complex workflows. The framework is designed to allow these patterns to be combined in various ways:","title":"Pattern Composition"},{"location":"adrs/0013-workflow-patterns/#composition-strategies","text":"Hierarchical Composition : Patterns can be nested within each other, such as a Branch pattern within a Loop, or a Chain within a Nested workflow. Sequential Composition : Patterns can be chained together, with the output of one pattern becoming the input to another. Parallel Composition : Multiple patterns can be executed in parallel using the Batch pattern. State-Sharing Composition : Different patterns can communicate through shared state.","title":"Composition Strategies"},{"location":"adrs/0013-workflow-patterns/#common-composition-patterns","text":"Based on real-world use cases, we've identified several common composition patterns: Chain + Branch : Create sequential workflows with decision points let workflow = chain ( summarize_email_node , branch ( needs_review_node , review_node , draft_reply_node ) ); Batch + Async : Process multiple items in parallel while handling I/O operations let batch_workflow = BatchFlow :: new ( async_node ( check_inbox_and_process ) ); Nesting + Shared : Build reusable workflow components that communicate through shared state let shared_state = SharedState :: new ( EmailSummary :: default ()); let nested_workflow = NestedWorkflow :: new ( chain ( write_to_shared_state ( shared_state . clone (), summarize_email_node ), read_from_shared_state ( shared_state , draft_reply_node ) ) ); Looping + Branch : Implement iterative processes with exit conditions let workflow = loop_until ( chain ( get_question_node , answer_question_node , branch ( is_final_question_node , complete_node , continue_node ) ), | outcome | outcome . is_complete () );","title":"Common Composition Patterns"},{"location":"adrs/0013-workflow-patterns/#implementation-considerations","text":"When implementing pattern composition, we need to ensure: Type Safety : Composed patterns should maintain Rust's strong type safety. Error Propagation : Errors should propagate correctly through composed patterns. Performance : Composition should not introduce significant overhead. Readability : The API for composing patterns should be intuitive and readable.","title":"Implementation Considerations"},{"location":"adrs/0013-workflow-patterns/#visual-workflow-representation","text":"An important aspect of workflow design is the ability to visualize workflows. We've decided to adopt a standardized visual representation for each pattern to help users design and communicate their workflow architectures:","title":"Visual Workflow Representation"},{"location":"adrs/0013-workflow-patterns/#visual-pattern-language","text":"Each pattern has a distinct visual representation: Node : Single rectangle labeled with the operation (e.g., \"Summarize Email\") and noted as \"Single step\" Chain : Connected rectangles with directional arrows (e.g., \"Summarize Email\" \u2192 \"Draft Reply\") and noted as \"Multiple steps\" Batch : Multiple stacked rectangles representing parallel instances of the same node (e.g., stacked \"Summarize Email\" boxes) and noted as \"Repeat steps\" Async : Rectangle with a clock icon indicating waiting for events (e.g., \"Check Inbox\" with clock) and noted as \"I/O wait\" Shared : Connected nodes with a dashed rectangle for shared state and dotted arrows indicating read/write operations (e.g., \"Email summary\" state between nodes with write/read arrows) and noted as \"Communication\" Branch : Node with multiple outgoing paths based on conditions (e.g., \"Need review\" vs. direct paths) and noted as \"Conditional step\" Nesting : Node that expands to reveal a contained workflow within a dashed border (e.g., \"Coding Task\" expanding to reveal test/code/verify/analyze steps) and noted as \"Reusable step\" Looping : Nodes with return arrows forming a cycle (e.g., \"Get Question\" \u2192 \"Answer Question\" with return arrow) and noted as \"Long running step\" For paradigm patterns, we use additional visual indicators: Workflow : Simple directed path with sequential nodes, labeled \"Directed Path\" Map-Reduce : Batch + Merge pattern showing data chunking, parallel processing, and result aggregation Chat : Single node with self-loop and shared state for history, labeled \"Loop (+ chat history store)\" RAG : Nodes with vector database as shared state, labeled \"(+ Vector DB store)\" Chain of Thought : Single node with self-loop and history tracking, labeled \"Loop (+ think history store)\" Agent : Nodes with branching and feedback loops, labeled \"Loop + Branching\" Multi-Agent : Complex interconnected graph with shared pub/sub communication, labeled \"Loop + Branching (+ pub/sub)\" Supervisor : Nested workflow with approval/rejection paths, labeled \"Nesting\"","title":"Visual Pattern Language"},{"location":"adrs/0013-workflow-patterns/#complex-workflow-example","text":"Here's an example of a complex email processing workflow that combines multiple patterns: graph TD Start[Start] --> CheckInbox[\"Check Inbox (Async)\"] CheckInbox --> HasEmails{Has New Emails?} HasEmails -->|No| Wait[\"Wait (Async)\"] Wait --> CheckInbox HasEmails -->|Yes| BatchProcess[\"Process Emails (Batch)\"] subgraph \"For Each Email\" BatchProcess --> Summarize[\"Summarize Email\"] Summarize --> NeedsReview{Needs Review?} NeedsReview -->|Yes| Review[\"Review Email\"] NeedsReview -->|No| Draft[\"Draft Reply\"] Review --> Draft subgraph \"Draft Reply Process (Nested)\" Draft --> GenerateTemplate[\"Generate Template\"] GenerateTemplate --> AddPersonalization[\"Add Personalization\"] AddPersonalization --> CheckGrammar[\"Check Grammar\"] end CheckGrammar --> SendReply[\"Send Reply\"] end SendReply --> UpdateStatus[\"Update Status (Shared State)\"] UpdateStatus --> CheckInbox This visualization demonstrates how multiple patterns can be combined to create a comprehensive workflow solution.","title":"Complex Workflow Example"},{"location":"adrs/0013-workflow-patterns/#benefits-of-visual-representation","text":"Communication : Facilitates discussion about workflow design among team members Documentation : Provides clear documentation of workflow architecture Design : Helps identify optimization opportunities and potential issues Onboarding : Makes it easier for new developers to understand existing workflows","title":"Benefits of Visual Representation"},{"location":"adrs/0013-workflow-patterns/#implementation","text":"While the framework itself doesn't include visualization tools, we recommend using Mermaid or similar tools to create workflow diagrams following our visual pattern language. These diagrams can be included in documentation and design discussions.","title":"Implementation"},{"location":"adrs/0013-workflow-patterns/#paradigm-patterns","text":"In addition to the core workflow patterns, we recognize that the framework will be used to implement specialized workflow types for AI and automation applications. These specialized workflows can be built by combining the core patterns in specific ways:","title":"Paradigm Patterns"},{"location":"adrs/0013-workflow-patterns/#1-standard-workflow","text":"Description : Simple directed path workflow with sequential processing. Implementation : Primarily uses the Chain pattern for a direct processing path. Example Use Case : Email processing with \"Summarize Email\" followed by \"Draft Reply\".","title":"1. Standard Workflow"},{"location":"adrs/0013-workflow-patterns/#2-chat-workflow","text":"Description : Looping conversation flow with state management for chat history. Implementation : Combines Looping pattern with Shared State to maintain conversation context. Example Use Case : Chat system with a single \"Chat\" node that loops back to itself, writing to and reading from a \"chat history\" shared store.","title":"2. Chat Workflow"},{"location":"adrs/0013-workflow-patterns/#3-rag-retrieval-augmented-generation","text":"Description : Workflows that integrate document storage and retrieval for question answering. Implementation : Combines Shared State (for vector database) with processing nodes for upload and question answering. Example Use Case : A system with \"Upload documents\" node that writes to a \"Vector DB\" shared store, and an \"Answer Questions\" node that reads from it.","title":"3. RAG (Retrieval Augmented Generation)"},{"location":"adrs/0013-workflow-patterns/#4-chain-of-thought-cot","text":"Description : Single \"thinking\" step that loops and maintains reasoning history. Implementation : Combines Looping with Shared State to store thinking steps and intermediate reasoning. Example Use Case : Reasoning system with a single \"Think\" node that loops back to itself, writing to and reading from a \"think history\" shared store.","title":"4. Chain of Thought (CoT)"},{"location":"adrs/0013-workflow-patterns/#5-map-reduce","text":"Description : Batch processing of data chunks followed by aggregation of results. Implementation : Combines Batch (for mapping chunks) with a reduction node that aggregates results. Example Use Case : System that uses \"Map Chunks\" to distribute work to multiple \"Summarize Chunk\" operations in parallel, then aggregates results with \"Reduce Summaries\".","title":"5. Map Reduce"},{"location":"adrs/0013-workflow-patterns/#6-agent","text":"Description : Autonomous workflows with branching decision logic and feedback loops. Implementation : Combines Looping and Branch patterns to create a decision-action cycle. Example Use Case : Email processing agent that summarizes an email, branches to either \"Review\" or \"Draft Reply\" based on content, and continues processing.","title":"6. Agent"},{"location":"adrs/0013-workflow-patterns/#7-multi-agent","text":"Description : Complex interconnected workflows with multiple processing nodes and publish/subscribe communication. Implementation : Uses multiple Node workflows with pub/sub shared state for coordination and communication. Example Use Case : Distributed system where multiple nodes communicate through a shared pub/sub message bus, creating a complex processing graph.","title":"7. Multi-agent"},{"location":"adrs/0013-workflow-patterns/#8-supervisor","text":"Description : Nested workflows with oversight that can approve or reject work. Implementation : Combines Nesting pattern with Branch for approval/rejection decisions. Example Use Case : Quality control system where a \"Supervise\" node reviews the output of a nested workflow and can either approve it to continue or reject it back for rework.","title":"8. Supervisor"},{"location":"adrs/0013-workflow-patterns/#consequences","text":"","title":"Consequences"},{"location":"adrs/0013-workflow-patterns/#positive","text":"Comprehensive Framework : By supporting these eight patterns, Floxide can address a wide range of workflow requirements. Clear Documentation : Explicitly defining these patterns provides clear guidance to users on how to implement different types of workflows. Consistent Design : Having these patterns defined upfront ensures the framework's design is consistent and accommodates all these use cases. Flexibility : The combination of these patterns allows for complex workflow designs that can solve real-world problems.","title":"Positive"},{"location":"adrs/0013-workflow-patterns/#negative","text":"Implementation Complexity : Supporting all these patterns increases the complexity of the framework implementation. Learning Curve : Users need to understand multiple patterns to effectively use the framework. Maintenance Overhead : More patterns mean more code to maintain and test.","title":"Negative"},{"location":"adrs/0013-workflow-patterns/#neutral","text":"Pattern Combinations : These patterns can be combined in various ways, which provides flexibility but also requires careful design. Performance Considerations : Different patterns may have different performance characteristics that users need to be aware of.","title":"Neutral"},{"location":"adrs/0013-workflow-patterns/#alternatives-considered","text":"Simplified Pattern Set : We considered supporting fewer patterns for simplicity, but decided that the comprehensive set provides more value to users. Different Implementation Approaches : For each pattern, we considered multiple implementation approaches before settling on the current design. External DSL : We considered creating an external DSL for workflow definition but decided that a code-based approach using Rust's type system provides better safety and flexibility.","title":"Alternatives Considered"},{"location":"adrs/0013-workflow-patterns/#references","text":"Workflow Patterns Initiative Enterprise Integration Patterns Rust Async Book","title":"References"},{"location":"adrs/0014-crate-publishing-and-cicd/","text":"ADR-0014: Crate Publishing and CI/CD Setup \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 As the Floxide framework matures, we need to establish a professional publishing workflow to make the crates available on crates.io. This includes setting up automated CI/CD pipelines through GitHub Actions to ensure quality, run tests, and automate the release process. We need to determine: How to structure our GitHub repository What CI/CD workflows to implement How to manage versioning across the workspace What quality checks to enforce How to automate the release process Decision \u00b6 We will implement a comprehensive CI/CD pipeline using GitHub Actions with the following components: Repository Structure \u00b6 We will create a public GitHub repository with the following structure: Main branch protected and requiring PR reviews Development through feature branches and PRs Releases tagged with semantic versioning CI/CD Workflows \u00b6 We will implement the following GitHub Actions workflows: CI Pipeline : Triggered on pull requests and pushes to main Runs tests, linting, and formatting checks Builds the project on multiple platforms (Linux, macOS, Windows) Generates and publishes test coverage reports Release Pipeline : Triggered on release tags (e.g., v0.1.0) Builds the project Publishes crates to crates.io Generates release notes Documentation Pipeline : Builds and publishes documentation to GitHub Pages Updates on releases and optionally on main branch changes Versioning Strategy \u00b6 We will use: Semantic versioning (MAJOR.MINOR.PATCH) Workspace-level version management through the workspace manifest Automated version bumping through conventional commits Quality Checks \u00b6 The CI pipeline will enforce: All tests passing Code formatting with rustfmt Linting with clippy Documentation coverage No unsafe code without explicit approval Dependency auditing with cargo audit Release Automation \u00b6 The release process will be automated: Create a release PR that bumps versions Upon approval and merge, tag the release GitHub Actions will publish to crates.io Release notes will be generated from commit history Crates.io Publishing \u00b6 For publishing to crates.io, we will: Reserve the crate names early Ensure all metadata is complete and professional Include appropriate keywords and categories Provide comprehensive documentation Use a dedicated API token stored as a GitHub secret Consequences \u00b6 Positive \u00b6 Professional Appearance : A well-maintained CI/CD pipeline demonstrates project quality Quality Assurance : Automated checks ensure consistent code quality Reduced Manual Work : Automation reduces release overhead Consistency : Enforced standards across the codebase Reliability : Predictable and repeatable release process Negative \u00b6 Maintenance Overhead : CI/CD pipelines require maintenance Complexity : More moving parts in the development process Learning Curve : Contributors need to understand the workflow Potential Delays : CI checks may slow down the development process Alternatives Considered \u00b6 Manual Publishing \u00b6 Pros : Simpler setup initially More direct control over the process Cons : Error-prone Time-consuming Less consistent Requires developer access to crates.io tokens We rejected this approach as it doesn't scale well and introduces unnecessary manual steps and potential for errors. Third-Party CI Services \u00b6 Pros : Potentially more features Separation from GitHub Cons : Additional integration points More accounts/services to manage Potential costs We chose GitHub Actions for its tight integration with our repository and sufficient feature set for our needs. Single Crate Publishing vs. Workspace \u00b6 Pros of individual crate publishing: More granular version control Independent release cycles Cons : More complex coordination Potential version mismatches Multiple publishing steps We chose to coordinate versions at the workspace level for simplicity and consistency, while still allowing for independent versioning when necessary.","title":"ADR-0014: Crate Publishing and CI/CD Setup"},{"location":"adrs/0014-crate-publishing-and-cicd/#adr-0014-crate-publishing-and-cicd-setup","text":"","title":"ADR-0014: Crate Publishing and CI/CD Setup"},{"location":"adrs/0014-crate-publishing-and-cicd/#status","text":"Accepted","title":"Status"},{"location":"adrs/0014-crate-publishing-and-cicd/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0014-crate-publishing-and-cicd/#context","text":"As the Floxide framework matures, we need to establish a professional publishing workflow to make the crates available on crates.io. This includes setting up automated CI/CD pipelines through GitHub Actions to ensure quality, run tests, and automate the release process. We need to determine: How to structure our GitHub repository What CI/CD workflows to implement How to manage versioning across the workspace What quality checks to enforce How to automate the release process","title":"Context"},{"location":"adrs/0014-crate-publishing-and-cicd/#decision","text":"We will implement a comprehensive CI/CD pipeline using GitHub Actions with the following components:","title":"Decision"},{"location":"adrs/0014-crate-publishing-and-cicd/#repository-structure","text":"We will create a public GitHub repository with the following structure: Main branch protected and requiring PR reviews Development through feature branches and PRs Releases tagged with semantic versioning","title":"Repository Structure"},{"location":"adrs/0014-crate-publishing-and-cicd/#cicd-workflows","text":"We will implement the following GitHub Actions workflows: CI Pipeline : Triggered on pull requests and pushes to main Runs tests, linting, and formatting checks Builds the project on multiple platforms (Linux, macOS, Windows) Generates and publishes test coverage reports Release Pipeline : Triggered on release tags (e.g., v0.1.0) Builds the project Publishes crates to crates.io Generates release notes Documentation Pipeline : Builds and publishes documentation to GitHub Pages Updates on releases and optionally on main branch changes","title":"CI/CD Workflows"},{"location":"adrs/0014-crate-publishing-and-cicd/#versioning-strategy","text":"We will use: Semantic versioning (MAJOR.MINOR.PATCH) Workspace-level version management through the workspace manifest Automated version bumping through conventional commits","title":"Versioning Strategy"},{"location":"adrs/0014-crate-publishing-and-cicd/#quality-checks","text":"The CI pipeline will enforce: All tests passing Code formatting with rustfmt Linting with clippy Documentation coverage No unsafe code without explicit approval Dependency auditing with cargo audit","title":"Quality Checks"},{"location":"adrs/0014-crate-publishing-and-cicd/#release-automation","text":"The release process will be automated: Create a release PR that bumps versions Upon approval and merge, tag the release GitHub Actions will publish to crates.io Release notes will be generated from commit history","title":"Release Automation"},{"location":"adrs/0014-crate-publishing-and-cicd/#cratesio-publishing","text":"For publishing to crates.io, we will: Reserve the crate names early Ensure all metadata is complete and professional Include appropriate keywords and categories Provide comprehensive documentation Use a dedicated API token stored as a GitHub secret","title":"Crates.io Publishing"},{"location":"adrs/0014-crate-publishing-and-cicd/#consequences","text":"","title":"Consequences"},{"location":"adrs/0014-crate-publishing-and-cicd/#positive","text":"Professional Appearance : A well-maintained CI/CD pipeline demonstrates project quality Quality Assurance : Automated checks ensure consistent code quality Reduced Manual Work : Automation reduces release overhead Consistency : Enforced standards across the codebase Reliability : Predictable and repeatable release process","title":"Positive"},{"location":"adrs/0014-crate-publishing-and-cicd/#negative","text":"Maintenance Overhead : CI/CD pipelines require maintenance Complexity : More moving parts in the development process Learning Curve : Contributors need to understand the workflow Potential Delays : CI checks may slow down the development process","title":"Negative"},{"location":"adrs/0014-crate-publishing-and-cicd/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0014-crate-publishing-and-cicd/#manual-publishing","text":"Pros : Simpler setup initially More direct control over the process Cons : Error-prone Time-consuming Less consistent Requires developer access to crates.io tokens We rejected this approach as it doesn't scale well and introduces unnecessary manual steps and potential for errors.","title":"Manual Publishing"},{"location":"adrs/0014-crate-publishing-and-cicd/#third-party-ci-services","text":"Pros : Potentially more features Separation from GitHub Cons : Additional integration points More accounts/services to manage Potential costs We chose GitHub Actions for its tight integration with our repository and sufficient feature set for our needs.","title":"Third-Party CI Services"},{"location":"adrs/0014-crate-publishing-and-cicd/#single-crate-publishing-vs-workspace","text":"Pros of individual crate publishing: More granular version control Independent release cycles Cons : More complex coordination Potential version mismatches Multiple publishing steps We chose to coordinate versions at the workspace level for simplicity and consistency, while still allowing for independent versioning when necessary.","title":"Single Crate Publishing vs. Workspace"},{"location":"adrs/0015-batch-processing-examples-and-patterns/","text":"ADR-0015: Batch Processing Examples and Best Practices \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 ADR-0007 established the fundamental architecture for batch processing in the Flow Framework, but there are still practical considerations for implementing batch processing in real-world applications. This ADR builds on ADR-0007 to document concrete patterns for batch processing implementations, common pitfalls, and best practices. Specifically, we need guidance on: How to properly implement the BatchContext trait Working with the BatchNode and BatchFlow abstractions Managing parallelism and resource usage Handling type parameters in Rust's generic system Tracking state across parallel executions Decision \u00b6 We've implemented and documented several batch processing patterns: 1. The BatchContext Implementation Pattern \u00b6 A proper implementation of BatchContext should: impl BatchContext < Image > for ImageBatchContext { // Return the complete batch of items fn get_batch_items ( & self ) -> Result < Vec < Image > , FloxideError > { Ok ( self . images . clone ()) } // Create a context for a single item (called for each batch item) fn create_item_context ( & self , item : Image ) -> Result < Self , FloxideError > { let mut ctx = self . clone (); ctx . images = Vec :: new (); ctx . current_image = Some ( item ); Ok ( ctx ) } // Update main context with results after processing fn update_with_results ( & mut self , results : & [ Result < Image , FloxideError > ], ) -> Result < (), FloxideError > { // Update statistics self . processed_count = results . iter (). filter ( | r | r . is_ok ()). count (); self . failed_count = results . iter (). filter ( | r | r . is_err ()). count (); // Update additional statistics if needed for result in results { match result { Ok ( _ ) => self . add_stat ( \"success\" ), Err ( _ ) => self . add_stat ( \"failure\" ), } } Ok (()) } } 2. Handling Type Parameters in BatchFlow \u00b6 Working with generic parameters requires explicit type annotation to ensure proper type inference: // Helper function to create a BatchFlow with the correct generic parameters fn create_batch_flow ( parallelism : usize ) -> BatchFlow < ImageBatchContext , Image , DefaultAction > { let processor = SimpleImageProcessor :: new ( \"image_processor\" ); // Create a workflow for processing a single item let workflow = Workflow :: new ( processor ); // Create a batch flow BatchFlow :: new ( workflow , parallelism ) } 3. Direct Parallel Processing (Alternative Pattern) \u00b6 For simpler cases, we can use Tokio's tasks directly without the full BatchFlow machinery: // Process images in parallel with a given parallelism limit async fn process_batch ( images : Vec < Image > , parallelism : usize ) -> Vec < Result < Image , FloxideError >> { use tokio :: sync :: Semaphore ; use futures :: stream ::{ self , StreamExt }; let semaphore = std :: sync :: Arc :: new ( Semaphore :: new ( parallelism )); let tasks = stream :: iter ( images ) . map ( | image | { let semaphore = semaphore . clone (); async move { let _permit = semaphore . acquire (). await . unwrap (); let result = process_image ( image ). await ; drop ( _permit ); result } }) . buffer_unordered ( parallelism ) . collect :: < Vec < _ >> () . await ; tasks } Consequences \u00b6 Advantages \u00b6 Clear Patterns : Developers have documented patterns to follow for batch processing Type Safety : Strong type checking at compile time prevents runtime errors Flexibility : Both high-level (BatchFlow) and low-level (direct parallelism) approaches are available Resource Control : Explicit concurrency controls with semaphores prevent resource exhaustion Disadvantages \u00b6 Generic Complexity : Rust's generic type system can be challenging when specifying nested generic types Memory Usage : Cloning contexts for each item can lead to increased memory usage Learning Curve : Proper implementation requires understanding both the Node trait and BatchContext traits Alternatives Considered \u00b6 1. Simplified Trait with No Generics \u00b6 We considered simplifying the BatchContext trait to avoid generic type parameters, but this would have decreased type safety and required more runtime type checking. 2. Iterator-Based API \u00b6 We explored using Rust's iterator traits more extensively but found that the async nature of our operations made the streams-based approach more suitable. 3. Shared Mutable State \u00b6 We considered using shared mutable state with synchronization primitives (Mutex, RwLock) instead of cloning contexts, but this introduced more complexity and potential for deadlocks. Implementation Notes \u00b6 The BatchContext trait requires implementing Clone for contexts Careful management of generic type parameters is crucial for type inference Use the helper function pattern to encapsulate type complexity when creating BatchFlow instances Consider direct parallel processing for simpler use cases Utilize context statistics to track processing results Related ADRs \u00b6 ADR-0007: Batch Processing Implementation","title":"ADR-0015: Batch Processing Examples and Best Practices"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#adr-0015-batch-processing-examples-and-best-practices","text":"","title":"ADR-0015: Batch Processing Examples and Best Practices"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#status","text":"Accepted","title":"Status"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#context","text":"ADR-0007 established the fundamental architecture for batch processing in the Flow Framework, but there are still practical considerations for implementing batch processing in real-world applications. This ADR builds on ADR-0007 to document concrete patterns for batch processing implementations, common pitfalls, and best practices. Specifically, we need guidance on: How to properly implement the BatchContext trait Working with the BatchNode and BatchFlow abstractions Managing parallelism and resource usage Handling type parameters in Rust's generic system Tracking state across parallel executions","title":"Context"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#decision","text":"We've implemented and documented several batch processing patterns:","title":"Decision"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#1-the-batchcontext-implementation-pattern","text":"A proper implementation of BatchContext should: impl BatchContext < Image > for ImageBatchContext { // Return the complete batch of items fn get_batch_items ( & self ) -> Result < Vec < Image > , FloxideError > { Ok ( self . images . clone ()) } // Create a context for a single item (called for each batch item) fn create_item_context ( & self , item : Image ) -> Result < Self , FloxideError > { let mut ctx = self . clone (); ctx . images = Vec :: new (); ctx . current_image = Some ( item ); Ok ( ctx ) } // Update main context with results after processing fn update_with_results ( & mut self , results : & [ Result < Image , FloxideError > ], ) -> Result < (), FloxideError > { // Update statistics self . processed_count = results . iter (). filter ( | r | r . is_ok ()). count (); self . failed_count = results . iter (). filter ( | r | r . is_err ()). count (); // Update additional statistics if needed for result in results { match result { Ok ( _ ) => self . add_stat ( \"success\" ), Err ( _ ) => self . add_stat ( \"failure\" ), } } Ok (()) } }","title":"1. The BatchContext Implementation Pattern"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#2-handling-type-parameters-in-batchflow","text":"Working with generic parameters requires explicit type annotation to ensure proper type inference: // Helper function to create a BatchFlow with the correct generic parameters fn create_batch_flow ( parallelism : usize ) -> BatchFlow < ImageBatchContext , Image , DefaultAction > { let processor = SimpleImageProcessor :: new ( \"image_processor\" ); // Create a workflow for processing a single item let workflow = Workflow :: new ( processor ); // Create a batch flow BatchFlow :: new ( workflow , parallelism ) }","title":"2. Handling Type Parameters in BatchFlow"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#3-direct-parallel-processing-alternative-pattern","text":"For simpler cases, we can use Tokio's tasks directly without the full BatchFlow machinery: // Process images in parallel with a given parallelism limit async fn process_batch ( images : Vec < Image > , parallelism : usize ) -> Vec < Result < Image , FloxideError >> { use tokio :: sync :: Semaphore ; use futures :: stream ::{ self , StreamExt }; let semaphore = std :: sync :: Arc :: new ( Semaphore :: new ( parallelism )); let tasks = stream :: iter ( images ) . map ( | image | { let semaphore = semaphore . clone (); async move { let _permit = semaphore . acquire (). await . unwrap (); let result = process_image ( image ). await ; drop ( _permit ); result } }) . buffer_unordered ( parallelism ) . collect :: < Vec < _ >> () . await ; tasks }","title":"3. Direct Parallel Processing (Alternative Pattern)"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#consequences","text":"","title":"Consequences"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#advantages","text":"Clear Patterns : Developers have documented patterns to follow for batch processing Type Safety : Strong type checking at compile time prevents runtime errors Flexibility : Both high-level (BatchFlow) and low-level (direct parallelism) approaches are available Resource Control : Explicit concurrency controls with semaphores prevent resource exhaustion","title":"Advantages"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#disadvantages","text":"Generic Complexity : Rust's generic type system can be challenging when specifying nested generic types Memory Usage : Cloning contexts for each item can lead to increased memory usage Learning Curve : Proper implementation requires understanding both the Node trait and BatchContext traits","title":"Disadvantages"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#1-simplified-trait-with-no-generics","text":"We considered simplifying the BatchContext trait to avoid generic type parameters, but this would have decreased type safety and required more runtime type checking.","title":"1. Simplified Trait with No Generics"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#2-iterator-based-api","text":"We explored using Rust's iterator traits more extensively but found that the async nature of our operations made the streams-based approach more suitable.","title":"2. Iterator-Based API"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#3-shared-mutable-state","text":"We considered using shared mutable state with synchronization primitives (Mutex, RwLock) instead of cloning contexts, but this introduced more complexity and potential for deadlocks.","title":"3. Shared Mutable State"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#implementation-notes","text":"The BatchContext trait requires implementing Clone for contexts Careful management of generic type parameters is crucial for type inference Use the helper function pattern to encapsulate type complexity when creating BatchFlow instances Consider direct parallel processing for simpler use cases Utilize context statistics to track processing results","title":"Implementation Notes"},{"location":"adrs/0015-batch-processing-examples-and-patterns/#related-adrs","text":"ADR-0007: Batch Processing Implementation","title":"Related ADRs"},{"location":"adrs/0015-node-abstraction-hierarchy/","text":"ADR-0015: Node Abstraction Hierarchy \u00b6 Status \u00b6 Proposed Date \u00b6 2025-02-27 Context \u00b6 The Floxide framework provides multiple node abstractions to support different programming models and use cases: The base Node trait with a single process method The LifecycleNode trait with the prep/exec/post lifecycle A planned AsyncNode trait for async-specific workflows This creates confusion about which abstraction to use when implementing workflow nodes. The README currently shows an example using an AsyncNode trait that doesn't match the actual implementation, while the examples use the base Node trait directly. We need to clarify the relationship between these abstractions, their intended use cases, and provide clear guidance on when to use each approach. Decision \u00b6 We will establish a clear hierarchy of node abstractions with well-defined relationships: 1. Base Node Trait \u00b6 The Node trait will remain the core abstraction that all workflow nodes must implement: #[async_trait] pub trait Node < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , Self :: Output : Send + Sync + ' static , { /// The output type produced by this node type Output ; /// Get the unique identifier for this node fn id ( & self ) -> NodeId ; /// Process the node asynchronously async fn process ( & self , ctx : & mut Context , ) -> Result < NodeOutcome < Self :: Output , Action > , FloxideError > ; } This trait is the foundation of the workflow system and is used by the Workflow struct to execute nodes. 2. LifecycleNode Trait \u00b6 The LifecycleNode trait provides a more structured approach with three distinct phases: #[async_trait] pub trait LifecycleNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , Self :: PrepOutput : Clone + Send + Sync + ' static , Self :: ExecOutput : Clone + Send + Sync + ' static , { /// Output type from the preparation phase type PrepOutput ; /// Output type from the execution phase type ExecOutput ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; /// Preparation phase - perform setup and validation async fn prep ( & self , ctx : & mut Context ) -> Result < Self :: PrepOutput , FloxideError > ; /// Execution phase - perform the main work async fn exec ( & self , prep_result : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > ; /// Post-execution phase - determine the next action and update context async fn post ( & self , prep_result : Self :: PrepOutput , exec_result : Self :: ExecOutput , ctx : & mut Context , ) -> Result < Action , FloxideError > ; } The LifecycleNode trait is adapted to the base Node trait using the LifecycleNodeAdapter struct, which implements the process method by calling the three lifecycle methods in sequence. 3. TransformNode Trait (Formerly AsyncNode) \u00b6 As detailed in ADR-0016: TransformNode Renaming and Async Extension Patterns , we will rename the AsyncNode trait to TransformNode to better reflect its actual purpose - providing a functional transformation interface: #[async_trait] pub trait TransformNode < Input , Output , Error > : Send + Sync where Input : Send + ' static , Output : Send + ' static , Error : std :: error :: Error + Send + Sync + ' static , { /// Preparation phase async fn prep ( & self , input : Input ) -> Result < Input , Error > ; /// Execution phase async fn exec ( & self , input : Input ) -> Result < Output , Error > ; /// Post-execution phase async fn post ( & self , output : Output ) -> Result < Output , Error > ; } This trait will be adapted to the LifecycleNode trait, which in turn adapts to the base Node trait. Adapter Pattern \u00b6 We will use the adapter pattern to convert between these abstractions: LifecycleNodeAdapter : Converts a LifecycleNode to a Node TransformNodeAdapter (formerly AsyncNodeAdapter ): Converts a TransformNode to a LifecycleNode This approach allows users to choose the abstraction that best fits their use case while maintaining compatibility with the core workflow system. Usage Guidelines \u00b6 We will provide clear guidelines on when to use each abstraction: Base Node : Use when you need complete control over the node execution process or when implementing custom node types that don't fit the lifecycle pattern. LifecycleNode : Use for most workflow nodes that benefit from the clear separation of concerns provided by the prep/exec/post lifecycle. TransformNode : Use for simple transformations where the input and output types are known and consistent, and you prefer a functional programming style. Consequences \u00b6 Advantages \u00b6 Clear Hierarchy : Establishes a clear relationship between the different node abstractions Flexibility : Allows users to choose the abstraction that best fits their use case Compatibility : Maintains compatibility with existing code through the adapter pattern Separation of Concerns : The lifecycle pattern provides clear separation of concerns for node implementation Disadvantages \u00b6 Complexity : Multiple abstractions increase the learning curve for new users Adapter Overhead : The adapter pattern introduces some runtime overhead Documentation Burden : Requires clear documentation to explain the different abstractions Migration Path \u00b6 Existing code using the base Node trait can continue to work without changes. For new code, we recommend: Use the LifecycleNode trait for most workflow nodes Use the base Node trait for custom node types that don't fit the lifecycle pattern Use the TransformNode trait for simple transformations (previously called AsyncNode ) Alternatives Considered \u00b6 1. Single Node Trait \u00b6 We considered having a single Node trait with optional lifecycle methods, but this would make the API less clear and harder to implement correctly. 2. Complete Replacement \u00b6 We considered completely replacing the base Node trait with the LifecycleNode trait, but this would break compatibility with existing code. 3. Macro-Based Approach \u00b6 We evaluated using macros to generate the appropriate trait implementations, but this would make the code harder to understand and debug. Implementation Notes \u00b6 The LifecycleNode trait requires prep/exec outputs to implement Clone for simplicity The adapter automatically converts to NodeOutcome::RouteToAction Unit tests verify the full lifecycle and error propagation between phases Related ADRs \u00b6 ADR-0003: Core Framework Abstractions ADR-0004: Async Runtime Selection ADR-0008: Node Lifecycle Methods ADR-0016: TransformNode Renaming and Async Extension Patterns References \u00b6 Adapter Pattern Rust Async Book","title":"ADR-0015: Node Abstraction Hierarchy"},{"location":"adrs/0015-node-abstraction-hierarchy/#adr-0015-node-abstraction-hierarchy","text":"","title":"ADR-0015: Node Abstraction Hierarchy"},{"location":"adrs/0015-node-abstraction-hierarchy/#status","text":"Proposed","title":"Status"},{"location":"adrs/0015-node-abstraction-hierarchy/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0015-node-abstraction-hierarchy/#context","text":"The Floxide framework provides multiple node abstractions to support different programming models and use cases: The base Node trait with a single process method The LifecycleNode trait with the prep/exec/post lifecycle A planned AsyncNode trait for async-specific workflows This creates confusion about which abstraction to use when implementing workflow nodes. The README currently shows an example using an AsyncNode trait that doesn't match the actual implementation, while the examples use the base Node trait directly. We need to clarify the relationship between these abstractions, their intended use cases, and provide clear guidance on when to use each approach.","title":"Context"},{"location":"adrs/0015-node-abstraction-hierarchy/#decision","text":"We will establish a clear hierarchy of node abstractions with well-defined relationships:","title":"Decision"},{"location":"adrs/0015-node-abstraction-hierarchy/#1-base-node-trait","text":"The Node trait will remain the core abstraction that all workflow nodes must implement: #[async_trait] pub trait Node < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , Self :: Output : Send + Sync + ' static , { /// The output type produced by this node type Output ; /// Get the unique identifier for this node fn id ( & self ) -> NodeId ; /// Process the node asynchronously async fn process ( & self , ctx : & mut Context , ) -> Result < NodeOutcome < Self :: Output , Action > , FloxideError > ; } This trait is the foundation of the workflow system and is used by the Workflow struct to execute nodes.","title":"1. Base Node Trait"},{"location":"adrs/0015-node-abstraction-hierarchy/#2-lifecyclenode-trait","text":"The LifecycleNode trait provides a more structured approach with three distinct phases: #[async_trait] pub trait LifecycleNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , Self :: PrepOutput : Clone + Send + Sync + ' static , Self :: ExecOutput : Clone + Send + Sync + ' static , { /// Output type from the preparation phase type PrepOutput ; /// Output type from the execution phase type ExecOutput ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; /// Preparation phase - perform setup and validation async fn prep ( & self , ctx : & mut Context ) -> Result < Self :: PrepOutput , FloxideError > ; /// Execution phase - perform the main work async fn exec ( & self , prep_result : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > ; /// Post-execution phase - determine the next action and update context async fn post ( & self , prep_result : Self :: PrepOutput , exec_result : Self :: ExecOutput , ctx : & mut Context , ) -> Result < Action , FloxideError > ; } The LifecycleNode trait is adapted to the base Node trait using the LifecycleNodeAdapter struct, which implements the process method by calling the three lifecycle methods in sequence.","title":"2. LifecycleNode Trait"},{"location":"adrs/0015-node-abstraction-hierarchy/#3-transformnode-trait-formerly-asyncnode","text":"As detailed in ADR-0016: TransformNode Renaming and Async Extension Patterns , we will rename the AsyncNode trait to TransformNode to better reflect its actual purpose - providing a functional transformation interface: #[async_trait] pub trait TransformNode < Input , Output , Error > : Send + Sync where Input : Send + ' static , Output : Send + ' static , Error : std :: error :: Error + Send + Sync + ' static , { /// Preparation phase async fn prep ( & self , input : Input ) -> Result < Input , Error > ; /// Execution phase async fn exec ( & self , input : Input ) -> Result < Output , Error > ; /// Post-execution phase async fn post ( & self , output : Output ) -> Result < Output , Error > ; } This trait will be adapted to the LifecycleNode trait, which in turn adapts to the base Node trait.","title":"3. TransformNode Trait (Formerly AsyncNode)"},{"location":"adrs/0015-node-abstraction-hierarchy/#adapter-pattern","text":"We will use the adapter pattern to convert between these abstractions: LifecycleNodeAdapter : Converts a LifecycleNode to a Node TransformNodeAdapter (formerly AsyncNodeAdapter ): Converts a TransformNode to a LifecycleNode This approach allows users to choose the abstraction that best fits their use case while maintaining compatibility with the core workflow system.","title":"Adapter Pattern"},{"location":"adrs/0015-node-abstraction-hierarchy/#usage-guidelines","text":"We will provide clear guidelines on when to use each abstraction: Base Node : Use when you need complete control over the node execution process or when implementing custom node types that don't fit the lifecycle pattern. LifecycleNode : Use for most workflow nodes that benefit from the clear separation of concerns provided by the prep/exec/post lifecycle. TransformNode : Use for simple transformations where the input and output types are known and consistent, and you prefer a functional programming style.","title":"Usage Guidelines"},{"location":"adrs/0015-node-abstraction-hierarchy/#consequences","text":"","title":"Consequences"},{"location":"adrs/0015-node-abstraction-hierarchy/#advantages","text":"Clear Hierarchy : Establishes a clear relationship between the different node abstractions Flexibility : Allows users to choose the abstraction that best fits their use case Compatibility : Maintains compatibility with existing code through the adapter pattern Separation of Concerns : The lifecycle pattern provides clear separation of concerns for node implementation","title":"Advantages"},{"location":"adrs/0015-node-abstraction-hierarchy/#disadvantages","text":"Complexity : Multiple abstractions increase the learning curve for new users Adapter Overhead : The adapter pattern introduces some runtime overhead Documentation Burden : Requires clear documentation to explain the different abstractions","title":"Disadvantages"},{"location":"adrs/0015-node-abstraction-hierarchy/#migration-path","text":"Existing code using the base Node trait can continue to work without changes. For new code, we recommend: Use the LifecycleNode trait for most workflow nodes Use the base Node trait for custom node types that don't fit the lifecycle pattern Use the TransformNode trait for simple transformations (previously called AsyncNode )","title":"Migration Path"},{"location":"adrs/0015-node-abstraction-hierarchy/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0015-node-abstraction-hierarchy/#1-single-node-trait","text":"We considered having a single Node trait with optional lifecycle methods, but this would make the API less clear and harder to implement correctly.","title":"1. Single Node Trait"},{"location":"adrs/0015-node-abstraction-hierarchy/#2-complete-replacement","text":"We considered completely replacing the base Node trait with the LifecycleNode trait, but this would break compatibility with existing code.","title":"2. Complete Replacement"},{"location":"adrs/0015-node-abstraction-hierarchy/#3-macro-based-approach","text":"We evaluated using macros to generate the appropriate trait implementations, but this would make the code harder to understand and debug.","title":"3. Macro-Based Approach"},{"location":"adrs/0015-node-abstraction-hierarchy/#implementation-notes","text":"The LifecycleNode trait requires prep/exec outputs to implement Clone for simplicity The adapter automatically converts to NodeOutcome::RouteToAction Unit tests verify the full lifecycle and error propagation between phases","title":"Implementation Notes"},{"location":"adrs/0015-node-abstraction-hierarchy/#related-adrs","text":"ADR-0003: Core Framework Abstractions ADR-0004: Async Runtime Selection ADR-0008: Node Lifecycle Methods ADR-0016: TransformNode Renaming and Async Extension Patterns","title":"Related ADRs"},{"location":"adrs/0015-node-abstraction-hierarchy/#references","text":"Adapter Pattern Rust Async Book","title":"References"},{"location":"adrs/0016-transform-node-and-async-extensions/","text":"ADR-0016: TransformNode Renaming and Async Extension Patterns \u00b6 Status \u00b6 Proposed Date \u00b6 2025-02-27 Context \u00b6 The Floxide framework currently has an AsyncNode abstraction that doesn't actually provide unique asynchronous capabilities beyond what's already available in other node types ( Node and LifecycleNode ). All node methods in the framework are already async , so the name \"AsyncNode\" is potentially misleading. What our current AsyncNode actually provides is: A simplified input/output model (vs. context modification) Direct error types specific to the node A more functional programming style At the same time, the framework lacks node types that truly leverage asynchronous programming patterns like event-driven programming, time-based triggering, and reactive patterns. These patterns would enable workflows to: Wait for external events without blocking or polling Execute nodes based on time schedules or conditions Handle long-running processes with checkpoints React to changes in external systems Decision \u00b6 We will implement two key changes: 1. Rename AsyncNode to TransformNode \u00b6 We will rename the current AsyncNode trait to TransformNode to better reflect its actual purpose - providing a functional transformation interface rather than special async capabilities: #[async_trait] pub trait TransformNode < Input , Output , Error > : Send + Sync where Input : Send + ' static , Output : Send + ' static , Error : std :: error :: Error + Send + Sync + ' static , { /// Preparation phase async fn prep ( & self , input : Input ) -> Result < Input , Error > ; /// Execution phase async fn exec ( & self , input : Input ) -> Result < Output , Error > ; /// Post-execution phase async fn post ( & self , output : Output ) -> Result < Output , Error > ; } All related types and functions will also be renamed: AsyncNodeAdapter \u2192 TransformNodeAdapter AsyncContext \u2192 TransformContext async_node \u2192 transform_node to_lifecycle_node \u2192 Remains the same but will work with TransformNode 2. Introduce True Async Extension Patterns \u00b6 We will introduce new node traits that enable truly async-specific patterns: A. EventDrivenNode \u00b6 A node that waits for external events: #[async_trait] pub trait EventDrivenNode < Event , Context , Action > : Send + Sync where Event : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { /// Wait for an external event to occur async fn wait_for_event ( & self ) -> Result < Event , FloxideError > ; /// Process the received event and update context async fn process_event ( & self , event : Event , ctx : & mut Context ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; } B. TimerNode \u00b6 A node that executes based on time schedules: /// Represents a time schedule for execution pub enum Schedule { Once ( DateTime < Utc > ), Interval ( Duration ), Daily ( u32 , u32 ), // Hour, minute Weekly ( Weekday , u32 , u32 ), // Day of week, hour, minute Monthly ( u32 , u32 , u32 ), // Day of month, hour, minute Cron ( String ), // Cron expression } #[async_trait] pub trait TimerNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { /// Define the execution schedule fn schedule ( & self ) -> Schedule ; /// Execute the node on schedule async fn execute_on_schedule ( & self , ctx : & mut Context ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; } C. LongRunningNode \u00b6 A node that handles long-running processes with checkpoints: pub enum LongRunningOutcome < T , S > { /// Processing is complete with result Complete ( T ), /// Processing needs to be suspended with saved state Suspend ( S ), } #[async_trait] pub trait LongRunningNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , Self :: State : Serialize + Deserialize + Send + Sync + ' static , Self :: Output : Send + ' static , { /// Type representing the node's processing state type State ; /// Type representing the final output type Output ; /// Process the next step, potentially suspending execution async fn process ( & self , state : Option < Self :: State > , ctx : & mut Context ) -> Result < LongRunningOutcome < Self :: Output , Self :: State > , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; } D. ReactiveNode \u00b6 A node that reacts to changes in external data sources: #[async_trait] pub trait ReactiveNode < Change , Context , Action > : Send + Sync where Change : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { /// Set up a stream of changes to watch async fn watch ( & self ) -> impl Stream < Item = Change > + Send ; /// React to a detected change async fn react_to_change ( & self , change : Change , ctx : & mut Context ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; } Consequences \u00b6 Advantages \u00b6 Clearer Naming : TransformNode better reflects the actual purpose of the abstraction New Capabilities : The new async node traits enable workflow patterns not possible before Better Resource Usage : Event-driven and reactive nodes prevent wasting resources on polling Extended Use Cases : Enable integration with external systems, time-based execution, and long-running workflows Richer Model : The framework can represent more real-world workflow patterns Disadvantages \u00b6 Increased Complexity : Adding more node types increases the conceptual overhead Implementation Effort : Building the infrastructure for these patterns requires significant work Integration Challenges : Integrating these patterns with the existing workflow engine will require careful design Migration Path \u00b6 For TransformNode : Migration will be straightforward via a deprecation period Mark AsyncNode as deprecated with a note to use TransformNode instead Keep both traits for a transitional period Eventually remove AsyncNode in a future breaking release For Async Extensions : These are new capabilities, so no migration is needed Alternatives Considered \u00b6 1. Keep AsyncNode As Is \u00b6 We could keep the current AsyncNode as is and just improve documentation to clarify its purpose. However, this would perpetuate the confused naming and miss the opportunity to add truly valuable async capabilities. 2. Add Async Capabilities to Existing Abstractions \u00b6 We could add async capabilities (events, timers, etc.) to the existing node abstractions instead of creating new traits. This would reduce the number of abstractions but would make the existing ones more complex and harder to implement correctly. 3. Create a Single AsyncExtension Trait \u00b6 We could create a single trait that covers all async extension patterns. This would be simpler conceptually but would force nodes to implement capabilities they don't need. Implementation Notes \u00b6 The async extensions will be implemented in a new floxide-async-ext crate Each async pattern will include adapter types to integrate with the core workflow engine Examples will be provided for each pattern to demonstrate proper usage Time-based execution will require a scheduler component Event-driven and reactive nodes will require integration with relevant async primitives (channels, streams, etc.) Related ADRs \u00b6 ADR-0003: Core Framework Abstractions ADR-0004: Async Runtime Selection ADR-0015: Node Abstraction Hierarchy References \u00b6 Rust Async Book Tokio Documentation Reactive Programming Patterns Event-Driven Architecture","title":"ADR-0016: TransformNode Renaming and Async Extension Patterns"},{"location":"adrs/0016-transform-node-and-async-extensions/#adr-0016-transformnode-renaming-and-async-extension-patterns","text":"","title":"ADR-0016: TransformNode Renaming and Async Extension Patterns"},{"location":"adrs/0016-transform-node-and-async-extensions/#status","text":"Proposed","title":"Status"},{"location":"adrs/0016-transform-node-and-async-extensions/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0016-transform-node-and-async-extensions/#context","text":"The Floxide framework currently has an AsyncNode abstraction that doesn't actually provide unique asynchronous capabilities beyond what's already available in other node types ( Node and LifecycleNode ). All node methods in the framework are already async , so the name \"AsyncNode\" is potentially misleading. What our current AsyncNode actually provides is: A simplified input/output model (vs. context modification) Direct error types specific to the node A more functional programming style At the same time, the framework lacks node types that truly leverage asynchronous programming patterns like event-driven programming, time-based triggering, and reactive patterns. These patterns would enable workflows to: Wait for external events without blocking or polling Execute nodes based on time schedules or conditions Handle long-running processes with checkpoints React to changes in external systems","title":"Context"},{"location":"adrs/0016-transform-node-and-async-extensions/#decision","text":"We will implement two key changes:","title":"Decision"},{"location":"adrs/0016-transform-node-and-async-extensions/#1-rename-asyncnode-to-transformnode","text":"We will rename the current AsyncNode trait to TransformNode to better reflect its actual purpose - providing a functional transformation interface rather than special async capabilities: #[async_trait] pub trait TransformNode < Input , Output , Error > : Send + Sync where Input : Send + ' static , Output : Send + ' static , Error : std :: error :: Error + Send + Sync + ' static , { /// Preparation phase async fn prep ( & self , input : Input ) -> Result < Input , Error > ; /// Execution phase async fn exec ( & self , input : Input ) -> Result < Output , Error > ; /// Post-execution phase async fn post ( & self , output : Output ) -> Result < Output , Error > ; } All related types and functions will also be renamed: AsyncNodeAdapter \u2192 TransformNodeAdapter AsyncContext \u2192 TransformContext async_node \u2192 transform_node to_lifecycle_node \u2192 Remains the same but will work with TransformNode","title":"1. Rename AsyncNode to TransformNode"},{"location":"adrs/0016-transform-node-and-async-extensions/#2-introduce-true-async-extension-patterns","text":"We will introduce new node traits that enable truly async-specific patterns:","title":"2. Introduce True Async Extension Patterns"},{"location":"adrs/0016-transform-node-and-async-extensions/#a-eventdrivennode","text":"A node that waits for external events: #[async_trait] pub trait EventDrivenNode < Event , Context , Action > : Send + Sync where Event : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { /// Wait for an external event to occur async fn wait_for_event ( & self ) -> Result < Event , FloxideError > ; /// Process the received event and update context async fn process_event ( & self , event : Event , ctx : & mut Context ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; }","title":"A. EventDrivenNode"},{"location":"adrs/0016-transform-node-and-async-extensions/#b-timernode","text":"A node that executes based on time schedules: /// Represents a time schedule for execution pub enum Schedule { Once ( DateTime < Utc > ), Interval ( Duration ), Daily ( u32 , u32 ), // Hour, minute Weekly ( Weekday , u32 , u32 ), // Day of week, hour, minute Monthly ( u32 , u32 , u32 ), // Day of month, hour, minute Cron ( String ), // Cron expression } #[async_trait] pub trait TimerNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { /// Define the execution schedule fn schedule ( & self ) -> Schedule ; /// Execute the node on schedule async fn execute_on_schedule ( & self , ctx : & mut Context ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; }","title":"B. TimerNode"},{"location":"adrs/0016-transform-node-and-async-extensions/#c-longrunningnode","text":"A node that handles long-running processes with checkpoints: pub enum LongRunningOutcome < T , S > { /// Processing is complete with result Complete ( T ), /// Processing needs to be suspended with saved state Suspend ( S ), } #[async_trait] pub trait LongRunningNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , Self :: State : Serialize + Deserialize + Send + Sync + ' static , Self :: Output : Send + ' static , { /// Type representing the node's processing state type State ; /// Type representing the final output type Output ; /// Process the next step, potentially suspending execution async fn process ( & self , state : Option < Self :: State > , ctx : & mut Context ) -> Result < LongRunningOutcome < Self :: Output , Self :: State > , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; }","title":"C. LongRunningNode"},{"location":"adrs/0016-transform-node-and-async-extensions/#d-reactivenode","text":"A node that reacts to changes in external data sources: #[async_trait] pub trait ReactiveNode < Change , Context , Action > : Send + Sync where Change : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { /// Set up a stream of changes to watch async fn watch ( & self ) -> impl Stream < Item = Change > + Send ; /// React to a detected change async fn react_to_change ( & self , change : Change , ctx : & mut Context ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; }","title":"D. ReactiveNode"},{"location":"adrs/0016-transform-node-and-async-extensions/#consequences","text":"","title":"Consequences"},{"location":"adrs/0016-transform-node-and-async-extensions/#advantages","text":"Clearer Naming : TransformNode better reflects the actual purpose of the abstraction New Capabilities : The new async node traits enable workflow patterns not possible before Better Resource Usage : Event-driven and reactive nodes prevent wasting resources on polling Extended Use Cases : Enable integration with external systems, time-based execution, and long-running workflows Richer Model : The framework can represent more real-world workflow patterns","title":"Advantages"},{"location":"adrs/0016-transform-node-and-async-extensions/#disadvantages","text":"Increased Complexity : Adding more node types increases the conceptual overhead Implementation Effort : Building the infrastructure for these patterns requires significant work Integration Challenges : Integrating these patterns with the existing workflow engine will require careful design","title":"Disadvantages"},{"location":"adrs/0016-transform-node-and-async-extensions/#migration-path","text":"For TransformNode : Migration will be straightforward via a deprecation period Mark AsyncNode as deprecated with a note to use TransformNode instead Keep both traits for a transitional period Eventually remove AsyncNode in a future breaking release For Async Extensions : These are new capabilities, so no migration is needed","title":"Migration Path"},{"location":"adrs/0016-transform-node-and-async-extensions/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0016-transform-node-and-async-extensions/#1-keep-asyncnode-as-is","text":"We could keep the current AsyncNode as is and just improve documentation to clarify its purpose. However, this would perpetuate the confused naming and miss the opportunity to add truly valuable async capabilities.","title":"1. Keep AsyncNode As Is"},{"location":"adrs/0016-transform-node-and-async-extensions/#2-add-async-capabilities-to-existing-abstractions","text":"We could add async capabilities (events, timers, etc.) to the existing node abstractions instead of creating new traits. This would reduce the number of abstractions but would make the existing ones more complex and harder to implement correctly.","title":"2. Add Async Capabilities to Existing Abstractions"},{"location":"adrs/0016-transform-node-and-async-extensions/#3-create-a-single-asyncextension-trait","text":"We could create a single trait that covers all async extension patterns. This would be simpler conceptually but would force nodes to implement capabilities they don't need.","title":"3. Create a Single AsyncExtension Trait"},{"location":"adrs/0016-transform-node-and-async-extensions/#implementation-notes","text":"The async extensions will be implemented in a new floxide-async-ext crate Each async pattern will include adapter types to integrate with the core workflow engine Examples will be provided for each pattern to demonstrate proper usage Time-based execution will require a scheduler component Event-driven and reactive nodes will require integration with relevant async primitives (channels, streams, etc.)","title":"Implementation Notes"},{"location":"adrs/0016-transform-node-and-async-extensions/#related-adrs","text":"ADR-0003: Core Framework Abstractions ADR-0004: Async Runtime Selection ADR-0015: Node Abstraction Hierarchy","title":"Related ADRs"},{"location":"adrs/0016-transform-node-and-async-extensions/#references","text":"Rust Async Book Tokio Documentation Reactive Programming Patterns Event-Driven Architecture","title":"References"},{"location":"adrs/0017-event-driven-node-pattern/","text":"ADR-0017: Event-Driven Node Pattern \u00b6 Status \u00b6 Proposed Date \u00b6 2025-02-27 Context \u00b6 The Floxide framework currently supports synchronous workflow execution where each node processes input and produces output through a direct call chain. While this model works well for deterministic, sequential workflows, it lacks support for: Workflows that need to wait for external events without blocking system resources Long-running workflows that respond to events as they arrive Integration with event sources like message queues, webhooks, or system signals Building reactive systems that can respond to changes in real-time The existing node abstractions ( Node , LifecycleNode , and TransformNode ) all assume that processing is initiated by the workflow engine and completes within a single execution context. They don't provide a natural way to express nodes that wait for events or react to external stimuli. Additionally, since the framework operates within a single process, we need an event-driven pattern that: Works efficiently within a shared memory space Can be nested within other workflows Provides clean integration with the existing workflow engine Maintains type safety and the functional approach of the framework Decision \u00b6 We will implement the EventDrivenNode trait to enable event-driven workflows within the Floxide framework. This trait will be designed to: Allow nodes to wait for events without blocking threads Process events as they arrive Integrate smoothly with the existing workflow system Support nesting within larger workflows EventDrivenNode Trait \u00b6 #[async_trait] pub trait EventDrivenNode < Event , Context , Action > : Send + Sync where Event : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { /// Wait for an external event to occur async fn wait_for_event ( & self ) -> Result < Event , FloxideError > ; /// Process the received event and update context async fn process_event ( & self , event : Event , ctx : & mut Context ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; } Event Sources \u00b6 To provide flexibility in event sources, we'll define common event source adapters: Channel-based Events : Using Tokio channels for in-process communication Timer Events : For time-based triggers using interval or cron expressions External Source Adapters : For connecting to external systems like message queues // Channel-based event source pub struct ChannelEventSource < Event > { receiver : mpsc :: Receiver < Event > , id : NodeId , } impl < Event > ChannelEventSource < Event > where Event : Send + ' static , { pub fn new ( capacity : usize ) -> ( Self , mpsc :: Sender < Event > ) { let ( sender , receiver ) = mpsc :: channel ( capacity ); let id = Uuid :: new_v4 (). to_string (); ( Self { receiver , id }, sender ) } } #[async_trait] impl < Event , Context , Action > EventDrivenNode < Event , Context , Action > for ChannelEventSource < Event > where Event : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { async fn wait_for_event ( & self ) -> Result < Event , FloxideError > { self . receiver . recv (). await . ok_or_else ( || FloxideError :: event_source ( self . id (), \"Event channel closed\" )) } async fn process_event ( & self , event : Event , _ctx : & mut Context ) -> Result < Action , FloxideError > { // Default implementation just forwards the event // Users should implement their own event processors Ok ( Action :: default ()) } fn id ( & self ) -> NodeId { self . id . clone () } } EventDrivenWorkflow \u00b6 To execute event-driven nodes, we'll introduce an EventDrivenWorkflow that: Manages the event loop for event-driven nodes Handles routing between event-driven nodes Provides graceful shutdown capabilities Integrates with the standard workflow system pub struct EventDrivenWorkflow < Event , Context , Action > where Event : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { nodes : HashMap < NodeId , Arc < dyn EventDrivenNode < Event , Context , Action >>> , routes : HashMap < ( NodeId , ActionType ), NodeId > , initial_node : NodeId , } impl < Event , Context , Action > EventDrivenWorkflow < Event , Context , Action > where Event : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { pub fn new ( initial_node : Arc < dyn EventDrivenNode < Event , Context , Action >> ) -> Self { let initial_id = initial_node . id (); let mut nodes = HashMap :: new (); nodes . insert ( initial_id . clone (), initial_node ); Self { nodes , routes : HashMap :: new (), initial_node : initial_id , } } pub fn add_node ( & mut self , node : Arc < dyn EventDrivenNode < Event , Context , Action >> ) { let id = node . id (); self . nodes . insert ( id , node ); } pub fn set_route ( & mut self , from_id : & NodeId , action : & Action , to_id : & NodeId ) { self . routes . insert (( from_id . clone (), action . clone ()), to_id . clone ()); } pub async fn execute ( & self , ctx : & mut Context ) -> Result < (), FloxideError > { let mut current_node_id = self . initial_node . clone (); loop { let node = self . nodes . get ( & current_node_id ). ok_or_else ( || { FloxideError :: node_not_found ( current_node_id . clone ()) }) ? ; // Wait for an event let event = node . wait_for_event (). await ? ; // Process the event let action = node . process_event ( event , ctx ). await ? ; // Check for termination action if action == Action :: terminate () { break ; } // Route to the next node if let Some ( next_node_id ) = self . routes . get ( & ( current_node_id . clone (), action )) { current_node_id = next_node_id . clone (); } else { // Stay on the same node if no route is defined for this action } } Ok (()) } pub async fn execute_with_timeout ( & self , ctx : & mut Context , timeout : Duration ) -> Result < (), FloxideError > { tokio :: select ! { result = self . execute ( ctx ) => result , _ = tokio :: time :: sleep ( timeout ) => { Err ( FloxideError :: timeout ( \"Event-driven workflow execution timed out\" )) } } } } Integration with Standard Workflows \u00b6 We'll provide adapter patterns to integrate event-driven nodes with standard workflows: pub struct EventDrivenNodeAdapter < E , C , A > where E : Send + ' static , C : Send + Sync + ' static , A : ActionType + Send + Sync + ' static , { node : Arc < dyn EventDrivenNode < E , C , A >> , id : NodeId , timeout : Duration , } #[async_trait] impl < E , C , A > Node < C , A > for EventDrivenNodeAdapter < E , C , A > where E : Send + ' static , C : Send + Sync + ' static , A : ActionType + Send + Sync + ' static , { type Output = (); fn id ( & self ) -> NodeId { self . id . clone () } async fn process ( & self , ctx : & mut C ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > { // Wait for one event with timeout tokio :: select ! { result = self . node . wait_for_event () => { match result { Ok ( event ) => { let action = self . node . process_event ( event , ctx ). await ? ; Ok ( NodeOutcome :: RouteToAction ((), action )) }, Err ( e ) => Err ( e ), } } _ = tokio :: time :: sleep ( self . timeout ) => { Ok ( NodeOutcome :: RouteToAction ((), A :: timeout ())) } } } } Nested Event-Driven Workflows \u00b6 To support nested event-driven workflows within larger workflows, we'll provide: pub struct NestedEventDrivenWorkflow < E , C , A > where E : Send + ' static , C : Send + Sync + ' static , A : ActionType + Send + Sync + ' static , { workflow : Arc < EventDrivenWorkflow < E , C , A >> , id : NodeId , timeout : Option < Duration > , } #[async_trait] impl < E , C , A > Node < C , A > for NestedEventDrivenWorkflow < E , C , A > where E : Send + ' static , C : Send + Sync + ' static , A : ActionType + Send + Sync + ' static , { type Output = (); fn id ( & self ) -> NodeId { self . id . clone () } async fn process ( & self , ctx : & mut C ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > { // Execute the workflow, optionally with a timeout let result = if let Some ( timeout ) = self . timeout { self . workflow . execute_with_timeout ( ctx , timeout ). await } else { self . workflow . execute ( ctx ). await }; match result { Ok (()) => Ok ( NodeOutcome :: RouteToAction ((), A :: complete ())), Err ( e ) => { if e . is_timeout () { Ok ( NodeOutcome :: RouteToAction ((), A :: timeout ())) } else { Err ( e ) } } } } } Consequences \u00b6 Advantages \u00b6 Event-Driven Workflows : Enables building reactive systems that respond to events Resource Efficiency : Nodes can wait for events without blocking threads Integration : Clean integration with the existing workflow engine Nesting Support : Event-driven workflows can be nested within larger workflows Type Safety : Maintains the type safety of the framework Flexibility : Works with various event sources (channels, timers, external systems) Composability : Event-driven nodes can be combined with other node types Disadvantages \u00b6 Complexity : Adds more abstractions to the framework Concurrency Challenges : Event-driven code can be harder to reason about State Management : Stateful event-driven nodes require careful management Learning Curve : Users need to understand both workflows and event handling Implementation Plan \u00b6 The implementation will be phased: Phase 1: Core EventDrivenNode trait and basic channel-based event sources Phase 2: Integration with standard workflows and nesting support Phase 3: Advanced event sources (timers, external system adapters) Phase 4: Expanded examples and documentation Alternatives Considered \u00b6 1. Using Actors Instead of Event-Driven Nodes \u00b6 We considered using an actor model (similar to Akka or Actix) where each node is an actor that processes messages. This would provide a different concurrency model but would add significant complexity to the framework. 2. Event Loops in Standard Nodes \u00b6 We evaluated adding event loop capabilities to standard nodes, allowing them to optionally wait for events. This would avoid adding a new trait but would make the existing traits more complex and harder to use correctly. 3. External Event Processing \u00b6 Another approach would be to handle events externally and feed them into the workflow through standard inputs. This would be simpler but would not provide the full reactivity benefits of true event-driven nodes. 4. Workflow-level Event Handling \u00b6 Instead of event-driven nodes, we considered adding event handling at the workflow level only. This would be simpler but would not allow the fine-grained control of having event handling at the node level. Implementation Notes \u00b6 The EventDrivenNode trait will be implemented in the new floxide-event crate Channel-based event sources will use Tokio's MPSC channels for efficiency Event-driven workflows will be cancelable and support graceful shutdown Type parameters on event-driven nodes allow for custom event types per workflow Events must be Send + 'static to support async processing across threads Related ADRs \u00b6 ADR-0003: Core Framework Abstractions ADR-0004: Async Runtime Selection ADR-0015: Node Abstraction Hierarchy ADR-0016: TransformNode Renaming and Async Extension Patterns References \u00b6 Tokio: Asynchronous Programming for Rust The Reactive Manifesto Designing event-driven systems Event-driven architecture","title":"ADR-0017: Event-Driven Node Pattern"},{"location":"adrs/0017-event-driven-node-pattern/#adr-0017-event-driven-node-pattern","text":"","title":"ADR-0017: Event-Driven Node Pattern"},{"location":"adrs/0017-event-driven-node-pattern/#status","text":"Proposed","title":"Status"},{"location":"adrs/0017-event-driven-node-pattern/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0017-event-driven-node-pattern/#context","text":"The Floxide framework currently supports synchronous workflow execution where each node processes input and produces output through a direct call chain. While this model works well for deterministic, sequential workflows, it lacks support for: Workflows that need to wait for external events without blocking system resources Long-running workflows that respond to events as they arrive Integration with event sources like message queues, webhooks, or system signals Building reactive systems that can respond to changes in real-time The existing node abstractions ( Node , LifecycleNode , and TransformNode ) all assume that processing is initiated by the workflow engine and completes within a single execution context. They don't provide a natural way to express nodes that wait for events or react to external stimuli. Additionally, since the framework operates within a single process, we need an event-driven pattern that: Works efficiently within a shared memory space Can be nested within other workflows Provides clean integration with the existing workflow engine Maintains type safety and the functional approach of the framework","title":"Context"},{"location":"adrs/0017-event-driven-node-pattern/#decision","text":"We will implement the EventDrivenNode trait to enable event-driven workflows within the Floxide framework. This trait will be designed to: Allow nodes to wait for events without blocking threads Process events as they arrive Integrate smoothly with the existing workflow system Support nesting within larger workflows","title":"Decision"},{"location":"adrs/0017-event-driven-node-pattern/#eventdrivennode-trait","text":"#[async_trait] pub trait EventDrivenNode < Event , Context , Action > : Send + Sync where Event : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { /// Wait for an external event to occur async fn wait_for_event ( & self ) -> Result < Event , FloxideError > ; /// Process the received event and update context async fn process_event ( & self , event : Event , ctx : & mut Context ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; }","title":"EventDrivenNode Trait"},{"location":"adrs/0017-event-driven-node-pattern/#event-sources","text":"To provide flexibility in event sources, we'll define common event source adapters: Channel-based Events : Using Tokio channels for in-process communication Timer Events : For time-based triggers using interval or cron expressions External Source Adapters : For connecting to external systems like message queues // Channel-based event source pub struct ChannelEventSource < Event > { receiver : mpsc :: Receiver < Event > , id : NodeId , } impl < Event > ChannelEventSource < Event > where Event : Send + ' static , { pub fn new ( capacity : usize ) -> ( Self , mpsc :: Sender < Event > ) { let ( sender , receiver ) = mpsc :: channel ( capacity ); let id = Uuid :: new_v4 (). to_string (); ( Self { receiver , id }, sender ) } } #[async_trait] impl < Event , Context , Action > EventDrivenNode < Event , Context , Action > for ChannelEventSource < Event > where Event : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { async fn wait_for_event ( & self ) -> Result < Event , FloxideError > { self . receiver . recv (). await . ok_or_else ( || FloxideError :: event_source ( self . id (), \"Event channel closed\" )) } async fn process_event ( & self , event : Event , _ctx : & mut Context ) -> Result < Action , FloxideError > { // Default implementation just forwards the event // Users should implement their own event processors Ok ( Action :: default ()) } fn id ( & self ) -> NodeId { self . id . clone () } }","title":"Event Sources"},{"location":"adrs/0017-event-driven-node-pattern/#eventdrivenworkflow","text":"To execute event-driven nodes, we'll introduce an EventDrivenWorkflow that: Manages the event loop for event-driven nodes Handles routing between event-driven nodes Provides graceful shutdown capabilities Integrates with the standard workflow system pub struct EventDrivenWorkflow < Event , Context , Action > where Event : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { nodes : HashMap < NodeId , Arc < dyn EventDrivenNode < Event , Context , Action >>> , routes : HashMap < ( NodeId , ActionType ), NodeId > , initial_node : NodeId , } impl < Event , Context , Action > EventDrivenWorkflow < Event , Context , Action > where Event : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { pub fn new ( initial_node : Arc < dyn EventDrivenNode < Event , Context , Action >> ) -> Self { let initial_id = initial_node . id (); let mut nodes = HashMap :: new (); nodes . insert ( initial_id . clone (), initial_node ); Self { nodes , routes : HashMap :: new (), initial_node : initial_id , } } pub fn add_node ( & mut self , node : Arc < dyn EventDrivenNode < Event , Context , Action >> ) { let id = node . id (); self . nodes . insert ( id , node ); } pub fn set_route ( & mut self , from_id : & NodeId , action : & Action , to_id : & NodeId ) { self . routes . insert (( from_id . clone (), action . clone ()), to_id . clone ()); } pub async fn execute ( & self , ctx : & mut Context ) -> Result < (), FloxideError > { let mut current_node_id = self . initial_node . clone (); loop { let node = self . nodes . get ( & current_node_id ). ok_or_else ( || { FloxideError :: node_not_found ( current_node_id . clone ()) }) ? ; // Wait for an event let event = node . wait_for_event (). await ? ; // Process the event let action = node . process_event ( event , ctx ). await ? ; // Check for termination action if action == Action :: terminate () { break ; } // Route to the next node if let Some ( next_node_id ) = self . routes . get ( & ( current_node_id . clone (), action )) { current_node_id = next_node_id . clone (); } else { // Stay on the same node if no route is defined for this action } } Ok (()) } pub async fn execute_with_timeout ( & self , ctx : & mut Context , timeout : Duration ) -> Result < (), FloxideError > { tokio :: select ! { result = self . execute ( ctx ) => result , _ = tokio :: time :: sleep ( timeout ) => { Err ( FloxideError :: timeout ( \"Event-driven workflow execution timed out\" )) } } } }","title":"EventDrivenWorkflow"},{"location":"adrs/0017-event-driven-node-pattern/#integration-with-standard-workflows","text":"We'll provide adapter patterns to integrate event-driven nodes with standard workflows: pub struct EventDrivenNodeAdapter < E , C , A > where E : Send + ' static , C : Send + Sync + ' static , A : ActionType + Send + Sync + ' static , { node : Arc < dyn EventDrivenNode < E , C , A >> , id : NodeId , timeout : Duration , } #[async_trait] impl < E , C , A > Node < C , A > for EventDrivenNodeAdapter < E , C , A > where E : Send + ' static , C : Send + Sync + ' static , A : ActionType + Send + Sync + ' static , { type Output = (); fn id ( & self ) -> NodeId { self . id . clone () } async fn process ( & self , ctx : & mut C ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > { // Wait for one event with timeout tokio :: select ! { result = self . node . wait_for_event () => { match result { Ok ( event ) => { let action = self . node . process_event ( event , ctx ). await ? ; Ok ( NodeOutcome :: RouteToAction ((), action )) }, Err ( e ) => Err ( e ), } } _ = tokio :: time :: sleep ( self . timeout ) => { Ok ( NodeOutcome :: RouteToAction ((), A :: timeout ())) } } } }","title":"Integration with Standard Workflows"},{"location":"adrs/0017-event-driven-node-pattern/#nested-event-driven-workflows","text":"To support nested event-driven workflows within larger workflows, we'll provide: pub struct NestedEventDrivenWorkflow < E , C , A > where E : Send + ' static , C : Send + Sync + ' static , A : ActionType + Send + Sync + ' static , { workflow : Arc < EventDrivenWorkflow < E , C , A >> , id : NodeId , timeout : Option < Duration > , } #[async_trait] impl < E , C , A > Node < C , A > for NestedEventDrivenWorkflow < E , C , A > where E : Send + ' static , C : Send + Sync + ' static , A : ActionType + Send + Sync + ' static , { type Output = (); fn id ( & self ) -> NodeId { self . id . clone () } async fn process ( & self , ctx : & mut C ) -> Result < NodeOutcome < Self :: Output , A > , FloxideError > { // Execute the workflow, optionally with a timeout let result = if let Some ( timeout ) = self . timeout { self . workflow . execute_with_timeout ( ctx , timeout ). await } else { self . workflow . execute ( ctx ). await }; match result { Ok (()) => Ok ( NodeOutcome :: RouteToAction ((), A :: complete ())), Err ( e ) => { if e . is_timeout () { Ok ( NodeOutcome :: RouteToAction ((), A :: timeout ())) } else { Err ( e ) } } } } }","title":"Nested Event-Driven Workflows"},{"location":"adrs/0017-event-driven-node-pattern/#consequences","text":"","title":"Consequences"},{"location":"adrs/0017-event-driven-node-pattern/#advantages","text":"Event-Driven Workflows : Enables building reactive systems that respond to events Resource Efficiency : Nodes can wait for events without blocking threads Integration : Clean integration with the existing workflow engine Nesting Support : Event-driven workflows can be nested within larger workflows Type Safety : Maintains the type safety of the framework Flexibility : Works with various event sources (channels, timers, external systems) Composability : Event-driven nodes can be combined with other node types","title":"Advantages"},{"location":"adrs/0017-event-driven-node-pattern/#disadvantages","text":"Complexity : Adds more abstractions to the framework Concurrency Challenges : Event-driven code can be harder to reason about State Management : Stateful event-driven nodes require careful management Learning Curve : Users need to understand both workflows and event handling","title":"Disadvantages"},{"location":"adrs/0017-event-driven-node-pattern/#implementation-plan","text":"The implementation will be phased: Phase 1: Core EventDrivenNode trait and basic channel-based event sources Phase 2: Integration with standard workflows and nesting support Phase 3: Advanced event sources (timers, external system adapters) Phase 4: Expanded examples and documentation","title":"Implementation Plan"},{"location":"adrs/0017-event-driven-node-pattern/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0017-event-driven-node-pattern/#1-using-actors-instead-of-event-driven-nodes","text":"We considered using an actor model (similar to Akka or Actix) where each node is an actor that processes messages. This would provide a different concurrency model but would add significant complexity to the framework.","title":"1. Using Actors Instead of Event-Driven Nodes"},{"location":"adrs/0017-event-driven-node-pattern/#2-event-loops-in-standard-nodes","text":"We evaluated adding event loop capabilities to standard nodes, allowing them to optionally wait for events. This would avoid adding a new trait but would make the existing traits more complex and harder to use correctly.","title":"2. Event Loops in Standard Nodes"},{"location":"adrs/0017-event-driven-node-pattern/#3-external-event-processing","text":"Another approach would be to handle events externally and feed them into the workflow through standard inputs. This would be simpler but would not provide the full reactivity benefits of true event-driven nodes.","title":"3. External Event Processing"},{"location":"adrs/0017-event-driven-node-pattern/#4-workflow-level-event-handling","text":"Instead of event-driven nodes, we considered adding event handling at the workflow level only. This would be simpler but would not allow the fine-grained control of having event handling at the node level.","title":"4. Workflow-level Event Handling"},{"location":"adrs/0017-event-driven-node-pattern/#implementation-notes","text":"The EventDrivenNode trait will be implemented in the new floxide-event crate Channel-based event sources will use Tokio's MPSC channels for efficiency Event-driven workflows will be cancelable and support graceful shutdown Type parameters on event-driven nodes allow for custom event types per workflow Events must be Send + 'static to support async processing across threads","title":"Implementation Notes"},{"location":"adrs/0017-event-driven-node-pattern/#related-adrs","text":"ADR-0003: Core Framework Abstractions ADR-0004: Async Runtime Selection ADR-0015: Node Abstraction Hierarchy ADR-0016: TransformNode Renaming and Async Extension Patterns","title":"Related ADRs"},{"location":"adrs/0017-event-driven-node-pattern/#references","text":"Tokio: Asynchronous Programming for Rust The Reactive Manifesto Designing event-driven systems Event-driven architecture","title":"References"},{"location":"adrs/0017-reactive-node-implementation/","text":"ADR-0017: ReactiveNode Implementation \u00b6 Status \u00b6 Proposed Date \u00b6 2025-02-27 Context \u00b6 As part of the implementation of async extension patterns outlined in ADR-0016 , we need to implement a ReactiveNode that can respond to changes in external data sources using a stream-based approach. This pattern is valuable for workflows that need to monitor and react to external changes without constant polling. Reactive programming is a paradigm that deals with asynchronous data streams and the propagation of changes. In the context of our workflow system, we need a node type that can: Watch external data sources for changes React to those changes by executing business logic Produce appropriate routing actions based on the changes Integrate with the core workflow engine The challenges include: Managing long-lived connections to data sources Handling connection failures and retries Converting external change events into workflow actions Maintaining state between change events Providing a clean abstraction over various types of data sources Decision \u00b6 We will implement a ReactiveNode trait and supporting infrastructure in a new floxide-reactive crate with the following components: 1. Core Trait and Types \u00b6 The main ReactiveNode trait will provide a stream-based interface for watching and reacting to changes: #[async_trait] pub trait ReactiveNode < Change , Context , Action > : Send + Sync where Change : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static + Debug , { /// Set up a stream of changes to watch async fn watch ( & self ) -> Result < impl Stream < Item = Change > + Send , FloxideError > ; /// React to a detected change async fn react_to_change ( & self , change : Change , ctx : & mut Context , ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; } 2. Adapter for Core Node Integration \u00b6 A ReactiveNodeAdapter that allows a ReactiveNode to be used as a standard Node : pub struct ReactiveNodeAdapter < R , Change , Context , Action > where R : ReactiveNode < Change , Context , Action > , Change : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static + Debug , { node : Arc < R > , buffer_size : usize , _phantom : PhantomData < ( Change , Context , Action ) > , } 3. Concrete Implementations \u00b6 We will provide two concrete implementations: FileWatcherNode : A simple reactive node that watches a file system path for changes CustomReactiveNode : A flexible implementation that allows using closures to define watch and react behavior 4. Extension Trait for Actions \u00b6 A ReactiveActionExt trait to define common reactive actions: pub trait ReactiveActionExt : ActionType { fn change_detected () -> Self ; fn no_change () -> Self ; fn is_change_detected ( & self ) -> bool ; fn is_no_change ( & self ) -> bool ; } 5. Error Types \u00b6 Specific error types for reactive operations: pub enum ReactiveError { WatchError ( String ), StreamClosed , ConnectionError ( String ), ResourceNotFound ( String ), } Consequences \u00b6 Advantages \u00b6 Event-Driven Architecture : Enables truly event-driven workflows that respond to external changes Resource Efficiency : Avoids constant polling for changes by using reactive streams Separation of Concerns : Clear separation between watching for changes and reacting to them Flexibility : The generic design allows reacting to any type of change from any data source Integration : Seamless integration with the core workflow engine via the adapter pattern Disadvantages \u00b6 Complexity : Introduces additional complexity with stream management and background tasks Resource Management : Long-lived connections require careful resource management Error Handling : More complex error handling is needed for connection issues and recovery Multiple Executions : ReactiveNode may execute multiple times in response to rapid changes Testing Challenges : Testing reactive code is more complex than synchronous code Implementation Details \u00b6 Background Tasks and Resource Management \u00b6 The ReactiveNodeAdapter will spawn background tasks to watch for changes, requiring careful management of task lifetimes and proper cleanup: // Start a background task to watch for changes and process them tokio :: spawn ( async move { match node_clone . watch (). await { Ok ( mut change_stream ) => { while let Some ( change ) = change_stream . next (). await { // Process change and send action... } } Err ( e ) => { warn ! ( \"Failed to set up watch stream: {}\" , e ); } } }); Buffering and Backpressure \u00b6 The implementation will include configurable buffering to handle backpressure when changes occur more rapidly than they can be processed: pub fn with_buffer_size ( mut self , size : usize ) -> Self { self . buffer_size = size ; self } State Handling \u00b6 Contexts will need to maintain state between change events. The implementation will support cloneable contexts to allow state sharing with background tasks: // Context is cloned for the background task let ctx_clone = ctx . clone (); Alternatives Considered \u00b6 1. Using Callbacks Instead of Streams \u00b6 We could have used a callback-based approach instead of streams: async fn on_change ( & self , callback : impl Fn ( Change ) -> Result < Action , FloxideError > ); This would be simpler in some ways but less flexible and harder to integrate with other async code. Streams provide better composition and more control over backpressure. 2. Polling-Based Approach \u00b6 We could have used a polling-based approach instead of reactive streams: async fn check_for_changes ( & self ) -> Result < Option < Change > , FloxideError > ; This would be simpler to implement but less efficient and less idiomatic for truly reactive patterns. 3. Using Event Emitters \u00b6 We could have used an event emitter pattern instead of streams: fn subscribe ( & self , event_emitter : & EventEmitter < Change > ); This approach is common in many event-driven frameworks but would require building a custom event emitter system and doesn't leverage Rust's existing stream ecosystem as well. Related ADRs \u00b6 ADR-0003: Core Framework Abstractions ADR-0004: Async Runtime Selection ADR-0015: Node Abstraction Hierarchy ADR-0016: TransformNode Renaming and Async Extension Patterns References \u00b6 Tokio Documentation futures-rs Documentation Reactive Streams Specification Reactive Programming Patterns","title":"ADR-0017: ReactiveNode Implementation"},{"location":"adrs/0017-reactive-node-implementation/#adr-0017-reactivenode-implementation","text":"","title":"ADR-0017: ReactiveNode Implementation"},{"location":"adrs/0017-reactive-node-implementation/#status","text":"Proposed","title":"Status"},{"location":"adrs/0017-reactive-node-implementation/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0017-reactive-node-implementation/#context","text":"As part of the implementation of async extension patterns outlined in ADR-0016 , we need to implement a ReactiveNode that can respond to changes in external data sources using a stream-based approach. This pattern is valuable for workflows that need to monitor and react to external changes without constant polling. Reactive programming is a paradigm that deals with asynchronous data streams and the propagation of changes. In the context of our workflow system, we need a node type that can: Watch external data sources for changes React to those changes by executing business logic Produce appropriate routing actions based on the changes Integrate with the core workflow engine The challenges include: Managing long-lived connections to data sources Handling connection failures and retries Converting external change events into workflow actions Maintaining state between change events Providing a clean abstraction over various types of data sources","title":"Context"},{"location":"adrs/0017-reactive-node-implementation/#decision","text":"We will implement a ReactiveNode trait and supporting infrastructure in a new floxide-reactive crate with the following components:","title":"Decision"},{"location":"adrs/0017-reactive-node-implementation/#1-core-trait-and-types","text":"The main ReactiveNode trait will provide a stream-based interface for watching and reacting to changes: #[async_trait] pub trait ReactiveNode < Change , Context , Action > : Send + Sync where Change : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static + Debug , { /// Set up a stream of changes to watch async fn watch ( & self ) -> Result < impl Stream < Item = Change > + Send , FloxideError > ; /// React to a detected change async fn react_to_change ( & self , change : Change , ctx : & mut Context , ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; }","title":"1. Core Trait and Types"},{"location":"adrs/0017-reactive-node-implementation/#2-adapter-for-core-node-integration","text":"A ReactiveNodeAdapter that allows a ReactiveNode to be used as a standard Node : pub struct ReactiveNodeAdapter < R , Change , Context , Action > where R : ReactiveNode < Change , Context , Action > , Change : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static + Debug , { node : Arc < R > , buffer_size : usize , _phantom : PhantomData < ( Change , Context , Action ) > , }","title":"2. Adapter for Core Node Integration"},{"location":"adrs/0017-reactive-node-implementation/#3-concrete-implementations","text":"We will provide two concrete implementations: FileWatcherNode : A simple reactive node that watches a file system path for changes CustomReactiveNode : A flexible implementation that allows using closures to define watch and react behavior","title":"3. Concrete Implementations"},{"location":"adrs/0017-reactive-node-implementation/#4-extension-trait-for-actions","text":"A ReactiveActionExt trait to define common reactive actions: pub trait ReactiveActionExt : ActionType { fn change_detected () -> Self ; fn no_change () -> Self ; fn is_change_detected ( & self ) -> bool ; fn is_no_change ( & self ) -> bool ; }","title":"4. Extension Trait for Actions"},{"location":"adrs/0017-reactive-node-implementation/#5-error-types","text":"Specific error types for reactive operations: pub enum ReactiveError { WatchError ( String ), StreamClosed , ConnectionError ( String ), ResourceNotFound ( String ), }","title":"5. Error Types"},{"location":"adrs/0017-reactive-node-implementation/#consequences","text":"","title":"Consequences"},{"location":"adrs/0017-reactive-node-implementation/#advantages","text":"Event-Driven Architecture : Enables truly event-driven workflows that respond to external changes Resource Efficiency : Avoids constant polling for changes by using reactive streams Separation of Concerns : Clear separation between watching for changes and reacting to them Flexibility : The generic design allows reacting to any type of change from any data source Integration : Seamless integration with the core workflow engine via the adapter pattern","title":"Advantages"},{"location":"adrs/0017-reactive-node-implementation/#disadvantages","text":"Complexity : Introduces additional complexity with stream management and background tasks Resource Management : Long-lived connections require careful resource management Error Handling : More complex error handling is needed for connection issues and recovery Multiple Executions : ReactiveNode may execute multiple times in response to rapid changes Testing Challenges : Testing reactive code is more complex than synchronous code","title":"Disadvantages"},{"location":"adrs/0017-reactive-node-implementation/#implementation-details","text":"","title":"Implementation Details"},{"location":"adrs/0017-reactive-node-implementation/#background-tasks-and-resource-management","text":"The ReactiveNodeAdapter will spawn background tasks to watch for changes, requiring careful management of task lifetimes and proper cleanup: // Start a background task to watch for changes and process them tokio :: spawn ( async move { match node_clone . watch (). await { Ok ( mut change_stream ) => { while let Some ( change ) = change_stream . next (). await { // Process change and send action... } } Err ( e ) => { warn ! ( \"Failed to set up watch stream: {}\" , e ); } } });","title":"Background Tasks and Resource Management"},{"location":"adrs/0017-reactive-node-implementation/#buffering-and-backpressure","text":"The implementation will include configurable buffering to handle backpressure when changes occur more rapidly than they can be processed: pub fn with_buffer_size ( mut self , size : usize ) -> Self { self . buffer_size = size ; self }","title":"Buffering and Backpressure"},{"location":"adrs/0017-reactive-node-implementation/#state-handling","text":"Contexts will need to maintain state between change events. The implementation will support cloneable contexts to allow state sharing with background tasks: // Context is cloned for the background task let ctx_clone = ctx . clone ();","title":"State Handling"},{"location":"adrs/0017-reactive-node-implementation/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0017-reactive-node-implementation/#1-using-callbacks-instead-of-streams","text":"We could have used a callback-based approach instead of streams: async fn on_change ( & self , callback : impl Fn ( Change ) -> Result < Action , FloxideError > ); This would be simpler in some ways but less flexible and harder to integrate with other async code. Streams provide better composition and more control over backpressure.","title":"1. Using Callbacks Instead of Streams"},{"location":"adrs/0017-reactive-node-implementation/#2-polling-based-approach","text":"We could have used a polling-based approach instead of reactive streams: async fn check_for_changes ( & self ) -> Result < Option < Change > , FloxideError > ; This would be simpler to implement but less efficient and less idiomatic for truly reactive patterns.","title":"2. Polling-Based Approach"},{"location":"adrs/0017-reactive-node-implementation/#3-using-event-emitters","text":"We could have used an event emitter pattern instead of streams: fn subscribe ( & self , event_emitter : & EventEmitter < Change > ); This approach is common in many event-driven frameworks but would require building a custom event emitter system and doesn't leverage Rust's existing stream ecosystem as well.","title":"3. Using Event Emitters"},{"location":"adrs/0017-reactive-node-implementation/#related-adrs","text":"ADR-0003: Core Framework Abstractions ADR-0004: Async Runtime Selection ADR-0015: Node Abstraction Hierarchy ADR-0016: TransformNode Renaming and Async Extension Patterns","title":"Related ADRs"},{"location":"adrs/0017-reactive-node-implementation/#references","text":"Tokio Documentation futures-rs Documentation Reactive Streams Specification Reactive Programming Patterns","title":"References"},{"location":"adrs/0018-rename-floxide-async-to-floxide-transform/","text":"ADR-0018: Rename floxide-async Crate to floxide-transform \u00b6 Status \u00b6 Implemented Date \u00b6 2025-02-27 Context \u00b6 In ADR-0016, we decided to rename the AsyncNode trait to TransformNode to better reflect its purpose, as the trait is primarily focused on data transformation rather than providing unique asynchronous capabilities beyond what's already available in other node types. While we've successfully renamed the trait and related types within the codebase, the containing crate is still named floxide-async , which creates a terminology inconsistency with the renamed components it contains. Package and module names should accurately reflect their primary purpose and the abstractions they provide. The current mismatch between the crate name (floxide-async) and its primary component (TransformNode) can lead to confusion for users of the framework. Decision \u00b6 We will rename the floxide-async crate to floxide-transform to maintain consistency with the renamed TransformNode trait and related types. This change will involve: Creating a new crate named floxide-transform with the same content as floxide-async Updating all imports in the codebase from floxide_async to floxide_transform Updating the Cargo.toml files to reflect these changes Removing the floxide-async crate entirely to avoid confusion Consequences \u00b6 Advantages \u00b6 Consistent Terminology : The crate name will now match the primary abstraction it provides Better Clarity : Developers will have a clearer understanding of the crate's purpose Forward Compatible : The rename aligns with our ongoing effort to clarify the framework's abstractions Disadvantages \u00b6 Migration Effort : Existing code will need to update imports Documentation Updates : Documentation references will need to be updated Migration Path \u00b6 Create the new floxide-transform crate with identical functionality Update all internal framework code to use the new imports Document the change clearly in the changelog Remove the old crate entirely to avoid confusion and maintenance overhead Alternatives Considered \u00b6 1. Keep the Existing Crate Name \u00b6 We could maintain the existing floxide-async crate name despite the internal renaming of types. This would require less immediate change but would perpetuate the inconsistency between the crate name and its primary abstractions. 2. Create a New Crate but Maintain Both \u00b6 We could create the new floxide-transform crate while keeping floxide-async indefinitely as a thin wrapper that re-exports everything from floxide-transform . This would avoid breaking changes but would add maintenance overhead and potential confusion. Related ADRs \u00b6 ADR-0016: TransformNode Renaming and Async Extension Patterns ADR-0002: Project Structure and Crate Organization","title":"ADR-0018: Rename floxide-async Crate to floxide-transform"},{"location":"adrs/0018-rename-floxide-async-to-floxide-transform/#adr-0018-rename-floxide-async-crate-to-floxide-transform","text":"","title":"ADR-0018: Rename floxide-async Crate to floxide-transform"},{"location":"adrs/0018-rename-floxide-async-to-floxide-transform/#status","text":"Implemented","title":"Status"},{"location":"adrs/0018-rename-floxide-async-to-floxide-transform/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0018-rename-floxide-async-to-floxide-transform/#context","text":"In ADR-0016, we decided to rename the AsyncNode trait to TransformNode to better reflect its purpose, as the trait is primarily focused on data transformation rather than providing unique asynchronous capabilities beyond what's already available in other node types. While we've successfully renamed the trait and related types within the codebase, the containing crate is still named floxide-async , which creates a terminology inconsistency with the renamed components it contains. Package and module names should accurately reflect their primary purpose and the abstractions they provide. The current mismatch between the crate name (floxide-async) and its primary component (TransformNode) can lead to confusion for users of the framework.","title":"Context"},{"location":"adrs/0018-rename-floxide-async-to-floxide-transform/#decision","text":"We will rename the floxide-async crate to floxide-transform to maintain consistency with the renamed TransformNode trait and related types. This change will involve: Creating a new crate named floxide-transform with the same content as floxide-async Updating all imports in the codebase from floxide_async to floxide_transform Updating the Cargo.toml files to reflect these changes Removing the floxide-async crate entirely to avoid confusion","title":"Decision"},{"location":"adrs/0018-rename-floxide-async-to-floxide-transform/#consequences","text":"","title":"Consequences"},{"location":"adrs/0018-rename-floxide-async-to-floxide-transform/#advantages","text":"Consistent Terminology : The crate name will now match the primary abstraction it provides Better Clarity : Developers will have a clearer understanding of the crate's purpose Forward Compatible : The rename aligns with our ongoing effort to clarify the framework's abstractions","title":"Advantages"},{"location":"adrs/0018-rename-floxide-async-to-floxide-transform/#disadvantages","text":"Migration Effort : Existing code will need to update imports Documentation Updates : Documentation references will need to be updated","title":"Disadvantages"},{"location":"adrs/0018-rename-floxide-async-to-floxide-transform/#migration-path","text":"Create the new floxide-transform crate with identical functionality Update all internal framework code to use the new imports Document the change clearly in the changelog Remove the old crate entirely to avoid confusion and maintenance overhead","title":"Migration Path"},{"location":"adrs/0018-rename-floxide-async-to-floxide-transform/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0018-rename-floxide-async-to-floxide-transform/#1-keep-the-existing-crate-name","text":"We could maintain the existing floxide-async crate name despite the internal renaming of types. This would require less immediate change but would perpetuate the inconsistency between the crate name and its primary abstractions.","title":"1. Keep the Existing Crate Name"},{"location":"adrs/0018-rename-floxide-async-to-floxide-transform/#2-create-a-new-crate-but-maintain-both","text":"We could create the new floxide-transform crate while keeping floxide-async indefinitely as a thin wrapper that re-exports everything from floxide-transform . This would avoid breaking changes but would add maintenance overhead and potential confusion.","title":"2. Create a New Crate but Maintain Both"},{"location":"adrs/0018-rename-floxide-async-to-floxide-transform/#related-adrs","text":"ADR-0016: TransformNode Renaming and Async Extension Patterns ADR-0002: Project Structure and Crate Organization","title":"Related ADRs"},{"location":"adrs/0019-examples-structure-standardization/","text":"ADR-0019: Examples Structure Standardization \u00b6 Status \u00b6 Implemented Date \u00b6 2025-02-27 Context \u00b6 Previously, the examples in the floxide framework were organized as binary applications in the examples/src/bin/ directory. This structure required users to run examples using the cargo run --bin example_name command, which diverges from the standard Rust convention for example code. Additionally, example filenames had a redundant _example suffix (e.g., transform_node.rs ), which doesn't align with conventional Rust naming practices for examples. Decision \u00b6 We will reorganize our examples to follow the standard Rust convention for examples: Move examples from examples/src/bin/ to examples/examples/ Remove the redundant _example suffix from the filenames Update the examples/Cargo.toml to properly define examples using the [[example]] format Enable examples to be run using the standard cargo run --example example_name command Consequences \u00b6 Advantages \u00b6 Follows Rust Conventions : Aligns with the standard Rust practice for organizing and running examples Improved Discoverability : Makes examples easier to discover and run for new users Cleaner Naming : Removes redundancy in filename suffixes Better Documentation : Examples serve more clearly as documentation rather than just binaries Disadvantages \u00b6 Migration Effort : Requires updating all examples and references to them Potential Backward Compatibility : Users familiar with the old structure will need to adjust Implementation Details \u00b6 The implementation involves: Creating an examples/examples/ directory Moving example files from examples/src/bin/ to examples/examples/ with renamed files: transform_node.rs \u2192 transform_node.rs lifecycle_node_example.rs \u2192 lifecycle_node.rs order_processing.rs remains as is (no suffix to remove) Updating examples/Cargo.toml to define examples with their paths: [[example]] name = \"transform_node\" path = \"examples/transform_node.rs\" [[example]] name = \"lifecycle_node\" path = \"examples/lifecycle_node.rs\" [[example]] name = \"order_processing\" path = \"examples/order_processing.rs\" Removing the unused examples/src/bin/ directory Related ADRs \u00b6 ADR-0002: Project Structure and Crate Organization","title":"ADR-0019: Examples Structure Standardization"},{"location":"adrs/0019-examples-structure-standardization/#adr-0019-examples-structure-standardization","text":"","title":"ADR-0019: Examples Structure Standardization"},{"location":"adrs/0019-examples-structure-standardization/#status","text":"Implemented","title":"Status"},{"location":"adrs/0019-examples-structure-standardization/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0019-examples-structure-standardization/#context","text":"Previously, the examples in the floxide framework were organized as binary applications in the examples/src/bin/ directory. This structure required users to run examples using the cargo run --bin example_name command, which diverges from the standard Rust convention for example code. Additionally, example filenames had a redundant _example suffix (e.g., transform_node.rs ), which doesn't align with conventional Rust naming practices for examples.","title":"Context"},{"location":"adrs/0019-examples-structure-standardization/#decision","text":"We will reorganize our examples to follow the standard Rust convention for examples: Move examples from examples/src/bin/ to examples/examples/ Remove the redundant _example suffix from the filenames Update the examples/Cargo.toml to properly define examples using the [[example]] format Enable examples to be run using the standard cargo run --example example_name command","title":"Decision"},{"location":"adrs/0019-examples-structure-standardization/#consequences","text":"","title":"Consequences"},{"location":"adrs/0019-examples-structure-standardization/#advantages","text":"Follows Rust Conventions : Aligns with the standard Rust practice for organizing and running examples Improved Discoverability : Makes examples easier to discover and run for new users Cleaner Naming : Removes redundancy in filename suffixes Better Documentation : Examples serve more clearly as documentation rather than just binaries","title":"Advantages"},{"location":"adrs/0019-examples-structure-standardization/#disadvantages","text":"Migration Effort : Requires updating all examples and references to them Potential Backward Compatibility : Users familiar with the old structure will need to adjust","title":"Disadvantages"},{"location":"adrs/0019-examples-structure-standardization/#implementation-details","text":"The implementation involves: Creating an examples/examples/ directory Moving example files from examples/src/bin/ to examples/examples/ with renamed files: transform_node.rs \u2192 transform_node.rs lifecycle_node_example.rs \u2192 lifecycle_node.rs order_processing.rs remains as is (no suffix to remove) Updating examples/Cargo.toml to define examples with their paths: [[example]] name = \"transform_node\" path = \"examples/transform_node.rs\" [[example]] name = \"lifecycle_node\" path = \"examples/lifecycle_node.rs\" [[example]] name = \"order_processing\" path = \"examples/order_processing.rs\" Removing the unused examples/src/bin/ directory","title":"Implementation Details"},{"location":"adrs/0019-examples-structure-standardization/#related-adrs","text":"ADR-0002: Project Structure and Crate Organization","title":"Related ADRs"},{"location":"adrs/0020-event-driven-workflow-routing-guidelines/","text":"ADR-0020: Event-Driven Workflow Routing Guidelines \u00b6 Status \u00b6 Proposed Date \u00b6 2025-02-27 Context \u00b6 In implementing event-driven workflows with the Flow Framework, we've encountered challenges with proper routing between event sources and processing nodes. Specifically, the event_driven_workflow.rs example exhibits an error during execution: \"TemperatureClassifier is not an event source\" when the workflow routing attempts to use a processor node as an event source. The current event-driven workflow pattern allows for connections between nodes using the set_route method, but doesn't clearly distinguish between nodes that can generate events (event sources) and nodes that can only process them (processors). This can lead to execution errors when the workflow engine attempts to wait for events from a node that cannot generate them. Key issues observed: Processing nodes incorrectly being treated as event sources Confusion about the correct routing patterns for different node types Unclear execution flow for event-driven workflows with multiple node types Decision \u00b6 We will establish clear routing guidelines for event-driven workflows to ensure proper distinction between event sources and processor nodes: Source-Processor Routing Pattern : Event sources (implementing wait_for_event ) should be the only nodes that generate new events Processor nodes should only receive events for processing and return actions After a processor node completes, the workflow should always return to an event source Routing Rules : All action routes from processor nodes must point to valid event sources When a processor returns a non-terminating action, execution must be routed to an event source Processors should never route to other processors without going through an event source first Workflow Validation : Add validation in EventDrivenWorkflow to ensure routes from processors lead to valid event sources Provide clear error messages when invalid routing is detected during workflow construction Add debugging facilities to trace the execution flow of event-driven workflows Documentation Updates : Clearly document the distinction between event sources and processors in API docs Provide examples of correct routing patterns in the examples Consequences \u00b6 Positive \u00b6 Clearer understanding of the roles of different node types in event-driven workflows Reduced likelihood of runtime errors due to invalid routing More predictable workflow execution paths Better diagnostic information for debugging workflows Negative \u00b6 Additional validation logic adds complexity to the workflow engine May require refactoring of existing workflows to comply with the new guidelines Stricter routing rules might feel constraining for some use cases Neutral \u00b6 This formalization makes explicit what was previously implicit in the design Implementation \u00b6 The implementation will involve: Adding validation checks in the set_route method of EventDrivenWorkflow to verify that routes from processors point to event sources Enhancing error messages to provide more context about routing issues Updating the event-driven workflow example to clearly demonstrate correct routing patterns Adding documentation comments that explain the routing rules and patterns Related ADRs \u00b6 ADR-0009: Event-Driven Workflow Pattern ADR-0017: Event-Driven Node Pattern","title":"ADR-0020: Event-Driven Workflow Routing Guidelines"},{"location":"adrs/0020-event-driven-workflow-routing-guidelines/#adr-0020-event-driven-workflow-routing-guidelines","text":"","title":"ADR-0020: Event-Driven Workflow Routing Guidelines"},{"location":"adrs/0020-event-driven-workflow-routing-guidelines/#status","text":"Proposed","title":"Status"},{"location":"adrs/0020-event-driven-workflow-routing-guidelines/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0020-event-driven-workflow-routing-guidelines/#context","text":"In implementing event-driven workflows with the Flow Framework, we've encountered challenges with proper routing between event sources and processing nodes. Specifically, the event_driven_workflow.rs example exhibits an error during execution: \"TemperatureClassifier is not an event source\" when the workflow routing attempts to use a processor node as an event source. The current event-driven workflow pattern allows for connections between nodes using the set_route method, but doesn't clearly distinguish between nodes that can generate events (event sources) and nodes that can only process them (processors). This can lead to execution errors when the workflow engine attempts to wait for events from a node that cannot generate them. Key issues observed: Processing nodes incorrectly being treated as event sources Confusion about the correct routing patterns for different node types Unclear execution flow for event-driven workflows with multiple node types","title":"Context"},{"location":"adrs/0020-event-driven-workflow-routing-guidelines/#decision","text":"We will establish clear routing guidelines for event-driven workflows to ensure proper distinction between event sources and processor nodes: Source-Processor Routing Pattern : Event sources (implementing wait_for_event ) should be the only nodes that generate new events Processor nodes should only receive events for processing and return actions After a processor node completes, the workflow should always return to an event source Routing Rules : All action routes from processor nodes must point to valid event sources When a processor returns a non-terminating action, execution must be routed to an event source Processors should never route to other processors without going through an event source first Workflow Validation : Add validation in EventDrivenWorkflow to ensure routes from processors lead to valid event sources Provide clear error messages when invalid routing is detected during workflow construction Add debugging facilities to trace the execution flow of event-driven workflows Documentation Updates : Clearly document the distinction between event sources and processors in API docs Provide examples of correct routing patterns in the examples","title":"Decision"},{"location":"adrs/0020-event-driven-workflow-routing-guidelines/#consequences","text":"","title":"Consequences"},{"location":"adrs/0020-event-driven-workflow-routing-guidelines/#positive","text":"Clearer understanding of the roles of different node types in event-driven workflows Reduced likelihood of runtime errors due to invalid routing More predictable workflow execution paths Better diagnostic information for debugging workflows","title":"Positive"},{"location":"adrs/0020-event-driven-workflow-routing-guidelines/#negative","text":"Additional validation logic adds complexity to the workflow engine May require refactoring of existing workflows to comply with the new guidelines Stricter routing rules might feel constraining for some use cases","title":"Negative"},{"location":"adrs/0020-event-driven-workflow-routing-guidelines/#neutral","text":"This formalization makes explicit what was previously implicit in the design","title":"Neutral"},{"location":"adrs/0020-event-driven-workflow-routing-guidelines/#implementation","text":"The implementation will involve: Adding validation checks in the set_route method of EventDrivenWorkflow to verify that routes from processors point to event sources Enhancing error messages to provide more context about routing issues Updating the event-driven workflow example to clearly demonstrate correct routing patterns Adding documentation comments that explain the routing rules and patterns","title":"Implementation"},{"location":"adrs/0020-event-driven-workflow-routing-guidelines/#related-adrs","text":"ADR-0009: Event-Driven Workflow Pattern ADR-0017: Event-Driven Node Pattern","title":"Related ADRs"},{"location":"adrs/0021-timer-node-implementation/","text":"ADR-0021: Timer Node Implementation \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 In ADR-0016, we outlined the need for an asynchronous extension pattern for timer-based execution of nodes. This extension would allow workflows to execute nodes based on time schedules, providing capabilities for periodic tasks, delayed execution, and time-based triggering without constant polling. The workflow system currently has event-driven nodes implemented, which allow nodes to wait for external events. Timer nodes extend this pattern by introducing time-based scheduling as another form of event trigger. Key requirements for timer nodes include: Support for various scheduling patterns (one-time, intervals, daily, weekly, monthly) Integration with the existing workflow engine Ability to use timer nodes as standard nodes in workflows Support for nested timer workflows Proper error handling and timeout mechanisms Decision \u00b6 We will implement a new crate floxide-timer that provides the TimerNode trait and related implementations as described in ADR-0016. The implementation will follow these design decisions: 1. Core Schedule Enum \u00b6 We will implement a Schedule enum that represents different scheduling patterns: pub enum Schedule { Once ( DateTime < Utc > ), Interval ( Duration ), Daily ( u32 , u32 ), // Hour, minute Weekly ( Weekday , u32 , u32 ), // Day of week, hour, minute Monthly ( u32 , u32 , u32 ), // Day of month, hour, minute Cron ( String ), // Cron expression (placeholder for future implementation) } The Schedule type will provide methods to calculate the next execution time and the duration until that time. 2. TimerNode Trait \u00b6 We will implement the TimerNode trait as outlined in ADR-0016: #[async_trait] pub trait TimerNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static + Default , { /// Define the execution schedule fn schedule ( & self ) -> Schedule ; /// Execute the node on schedule async fn execute_on_schedule ( & self , ctx : & mut Context ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; } 3. Basic Implementations \u00b6 We will provide these concrete implementations: SimpleTimer : A timer node that executes a function on a schedule TimerWorkflow : A workflow that orchestrates execution of timer nodes TimerNodeAdapter : An adapter to use a timer node as a standard node NestedTimerWorkflow : A nested timer workflow that can be used as a standard node 4. Integration with Core Workflow Engine \u00b6 The TimerNodeAdapter will implement the Node trait, allowing timer nodes to be used in standard workflows. This adapter will handle the wait period before executing the node. 5. Utility Extension Traits \u00b6 We will provide a TimerActionExt trait that extends ActionType with timer-specific actions: pub trait TimerActionExt : ActionType { /// Create a complete action for timer nodes fn complete () -> Self ; /// Create a retry action for timer nodes fn retry () -> Self ; } Consequences \u00b6 Advantages \u00b6 Time-Based Execution : The framework can now execute nodes based on time schedules. Resource Efficiency : Timer nodes eliminate the need for polling-based implementations. Flexibility : Different scheduling patterns support a wide range of use cases. Integration : Timer nodes can be used alongside existing node types in workflows. Composability : Timer workflows can be nested within standard workflows. Disadvantages \u00b6 Complexity : Adds another node type to the framework, increasing complexity. Maintenance : Additional code to maintain and test. Scheduling Edge Cases : Time-based scheduling has many edge cases (time zones, DST changes, etc.). Resource Consumption : Long-running timer workflows may consume resources while waiting. Implementation Notes \u00b6 The implementation uses Tokio's sleep function for time-based waiting. The Cron schedule type is a placeholder for future implementation. Proper error handling is implemented for invalid schedules. Unit tests are provided to validate schedule calculations and timer node execution. Alternatives Considered \u00b6 1. Use External Scheduling Libraries \u00b6 We considered using external scheduling libraries like cron or job_scheduler , but decided to implement our own scheduling to maintain control over the implementation and to ensure seamless integration with our workflow engine. 2. Implement as Part of Event-Driven Nodes \u00b6 We considered implementing timers as a special case of event-driven nodes, but decided that a separate abstraction would be clearer and more maintainable, especially given the specialized scheduling logic required. 3. Operating System Level Scheduling \u00b6 We considered integrating with OS-level scheduling (cron jobs, Windows Task Scheduler), but this would limit portability and would not integrate well with in-process workflows. Related ADRs \u00b6 ADR-0016: TransformNode Renaming and Async Extension Patterns ADR-0017: Event-Driven Node Pattern References \u00b6 Tokio time Chrono crate","title":"ADR-0021: Timer Node Implementation"},{"location":"adrs/0021-timer-node-implementation/#adr-0021-timer-node-implementation","text":"","title":"ADR-0021: Timer Node Implementation"},{"location":"adrs/0021-timer-node-implementation/#status","text":"Accepted","title":"Status"},{"location":"adrs/0021-timer-node-implementation/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0021-timer-node-implementation/#context","text":"In ADR-0016, we outlined the need for an asynchronous extension pattern for timer-based execution of nodes. This extension would allow workflows to execute nodes based on time schedules, providing capabilities for periodic tasks, delayed execution, and time-based triggering without constant polling. The workflow system currently has event-driven nodes implemented, which allow nodes to wait for external events. Timer nodes extend this pattern by introducing time-based scheduling as another form of event trigger. Key requirements for timer nodes include: Support for various scheduling patterns (one-time, intervals, daily, weekly, monthly) Integration with the existing workflow engine Ability to use timer nodes as standard nodes in workflows Support for nested timer workflows Proper error handling and timeout mechanisms","title":"Context"},{"location":"adrs/0021-timer-node-implementation/#decision","text":"We will implement a new crate floxide-timer that provides the TimerNode trait and related implementations as described in ADR-0016. The implementation will follow these design decisions:","title":"Decision"},{"location":"adrs/0021-timer-node-implementation/#1-core-schedule-enum","text":"We will implement a Schedule enum that represents different scheduling patterns: pub enum Schedule { Once ( DateTime < Utc > ), Interval ( Duration ), Daily ( u32 , u32 ), // Hour, minute Weekly ( Weekday , u32 , u32 ), // Day of week, hour, minute Monthly ( u32 , u32 , u32 ), // Day of month, hour, minute Cron ( String ), // Cron expression (placeholder for future implementation) } The Schedule type will provide methods to calculate the next execution time and the duration until that time.","title":"1. Core Schedule Enum"},{"location":"adrs/0021-timer-node-implementation/#2-timernode-trait","text":"We will implement the TimerNode trait as outlined in ADR-0016: #[async_trait] pub trait TimerNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static + Default , { /// Define the execution schedule fn schedule ( & self ) -> Schedule ; /// Execute the node on schedule async fn execute_on_schedule ( & self , ctx : & mut Context ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; }","title":"2. TimerNode Trait"},{"location":"adrs/0021-timer-node-implementation/#3-basic-implementations","text":"We will provide these concrete implementations: SimpleTimer : A timer node that executes a function on a schedule TimerWorkflow : A workflow that orchestrates execution of timer nodes TimerNodeAdapter : An adapter to use a timer node as a standard node NestedTimerWorkflow : A nested timer workflow that can be used as a standard node","title":"3. Basic Implementations"},{"location":"adrs/0021-timer-node-implementation/#4-integration-with-core-workflow-engine","text":"The TimerNodeAdapter will implement the Node trait, allowing timer nodes to be used in standard workflows. This adapter will handle the wait period before executing the node.","title":"4. Integration with Core Workflow Engine"},{"location":"adrs/0021-timer-node-implementation/#5-utility-extension-traits","text":"We will provide a TimerActionExt trait that extends ActionType with timer-specific actions: pub trait TimerActionExt : ActionType { /// Create a complete action for timer nodes fn complete () -> Self ; /// Create a retry action for timer nodes fn retry () -> Self ; }","title":"5. Utility Extension Traits"},{"location":"adrs/0021-timer-node-implementation/#consequences","text":"","title":"Consequences"},{"location":"adrs/0021-timer-node-implementation/#advantages","text":"Time-Based Execution : The framework can now execute nodes based on time schedules. Resource Efficiency : Timer nodes eliminate the need for polling-based implementations. Flexibility : Different scheduling patterns support a wide range of use cases. Integration : Timer nodes can be used alongside existing node types in workflows. Composability : Timer workflows can be nested within standard workflows.","title":"Advantages"},{"location":"adrs/0021-timer-node-implementation/#disadvantages","text":"Complexity : Adds another node type to the framework, increasing complexity. Maintenance : Additional code to maintain and test. Scheduling Edge Cases : Time-based scheduling has many edge cases (time zones, DST changes, etc.). Resource Consumption : Long-running timer workflows may consume resources while waiting.","title":"Disadvantages"},{"location":"adrs/0021-timer-node-implementation/#implementation-notes","text":"The implementation uses Tokio's sleep function for time-based waiting. The Cron schedule type is a placeholder for future implementation. Proper error handling is implemented for invalid schedules. Unit tests are provided to validate schedule calculations and timer node execution.","title":"Implementation Notes"},{"location":"adrs/0021-timer-node-implementation/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0021-timer-node-implementation/#1-use-external-scheduling-libraries","text":"We considered using external scheduling libraries like cron or job_scheduler , but decided to implement our own scheduling to maintain control over the implementation and to ensure seamless integration with our workflow engine.","title":"1. Use External Scheduling Libraries"},{"location":"adrs/0021-timer-node-implementation/#2-implement-as-part-of-event-driven-nodes","text":"We considered implementing timers as a special case of event-driven nodes, but decided that a separate abstraction would be clearer and more maintainable, especially given the specialized scheduling logic required.","title":"2. Implement as Part of Event-Driven Nodes"},{"location":"adrs/0021-timer-node-implementation/#3-operating-system-level-scheduling","text":"We considered integrating with OS-level scheduling (cron jobs, Windows Task Scheduler), but this would limit portability and would not integrate well with in-process workflows.","title":"3. Operating System Level Scheduling"},{"location":"adrs/0021-timer-node-implementation/#related-adrs","text":"ADR-0016: TransformNode Renaming and Async Extension Patterns ADR-0017: Event-Driven Node Pattern","title":"Related ADRs"},{"location":"adrs/0021-timer-node-implementation/#references","text":"Tokio time Chrono crate","title":"References"},{"location":"adrs/0022-documentation-structure-and-cross-references/","text":"ADR-0022: Documentation Structure and Cross-References \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 The Floxide framework's documentation needed to be more cohesive and better organized to help users understand and effectively use the framework. The documentation was spread across multiple locations with inconsistent cross-referencing and varying levels of detail. Key issues that needed to be addressed: 1. Inconsistent documentation formats across different components 2. Missing or outdated architectural documentation 3. Lack of clear cross-references between related documents 4. Incomplete API documentation for some components 5. Examples that didn't fully demonstrate best practices Decision \u00b6 We have decided to implement a comprehensive documentation structure with the following components: API Documentation Detailed documentation for each crate Clear examples showing proper usage Links to relevant architectural docs Consistent format across all APIs Architecture Documentation Core Framework Abstractions Node Lifecycle Methods Event-Driven Workflow Pattern Batch Processing Implementation Async Runtime Selection Examples Basic workflow demonstrating core concepts Specialized examples for each pattern Error handling demonstrations Best practices implementation Cross-References Clear links between related documents References to relevant ADRs Links to example implementations References to external resources Consequences \u00b6 Positive \u00b6 Improved Usability Easier for new users to understand the framework Clear path from basic to advanced usage Better understanding of architectural decisions Better Maintainability Consistent documentation format Clear relationships between components Easier to identify missing documentation Enhanced Quality Examples demonstrate best practices Error handling is clearly documented Architecture decisions are well-explained Negative \u00b6 Maintenance Overhead More documentation to keep updated Need to maintain cross-references Regular reviews required Potential Inconsistencies Risk of docs becoming outdated Need to coordinate updates across files More places to check when making changes Implementation \u00b6 The documentation has been updated to follow this structure: API Documentation Updated floxide-reactive.md Updated floxide-timer.md Updated floxide-core.md Added comprehensive examples Architecture Documentation Created/updated core architecture docs Added cross-references Improved consistency Examples Updated basic-workflow.md Added error handling examples Demonstrated best practices Cross-References Added links between related docs Referenced relevant ADRs Updated navigation structure References \u00b6 ADR-0017: ReactiveNode Implementation ADR-0021: Timer Node Implementation Core Framework Abstractions Node Lifecycle Methods","title":"ADR-0022: Documentation Structure and Cross-References"},{"location":"adrs/0022-documentation-structure-and-cross-references/#adr-0022-documentation-structure-and-cross-references","text":"","title":"ADR-0022: Documentation Structure and Cross-References"},{"location":"adrs/0022-documentation-structure-and-cross-references/#status","text":"Accepted","title":"Status"},{"location":"adrs/0022-documentation-structure-and-cross-references/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0022-documentation-structure-and-cross-references/#context","text":"The Floxide framework's documentation needed to be more cohesive and better organized to help users understand and effectively use the framework. The documentation was spread across multiple locations with inconsistent cross-referencing and varying levels of detail. Key issues that needed to be addressed: 1. Inconsistent documentation formats across different components 2. Missing or outdated architectural documentation 3. Lack of clear cross-references between related documents 4. Incomplete API documentation for some components 5. Examples that didn't fully demonstrate best practices","title":"Context"},{"location":"adrs/0022-documentation-structure-and-cross-references/#decision","text":"We have decided to implement a comprehensive documentation structure with the following components: API Documentation Detailed documentation for each crate Clear examples showing proper usage Links to relevant architectural docs Consistent format across all APIs Architecture Documentation Core Framework Abstractions Node Lifecycle Methods Event-Driven Workflow Pattern Batch Processing Implementation Async Runtime Selection Examples Basic workflow demonstrating core concepts Specialized examples for each pattern Error handling demonstrations Best practices implementation Cross-References Clear links between related documents References to relevant ADRs Links to example implementations References to external resources","title":"Decision"},{"location":"adrs/0022-documentation-structure-and-cross-references/#consequences","text":"","title":"Consequences"},{"location":"adrs/0022-documentation-structure-and-cross-references/#positive","text":"Improved Usability Easier for new users to understand the framework Clear path from basic to advanced usage Better understanding of architectural decisions Better Maintainability Consistent documentation format Clear relationships between components Easier to identify missing documentation Enhanced Quality Examples demonstrate best practices Error handling is clearly documented Architecture decisions are well-explained","title":"Positive"},{"location":"adrs/0022-documentation-structure-and-cross-references/#negative","text":"Maintenance Overhead More documentation to keep updated Need to maintain cross-references Regular reviews required Potential Inconsistencies Risk of docs becoming outdated Need to coordinate updates across files More places to check when making changes","title":"Negative"},{"location":"adrs/0022-documentation-structure-and-cross-references/#implementation","text":"The documentation has been updated to follow this structure: API Documentation Updated floxide-reactive.md Updated floxide-timer.md Updated floxide-core.md Added comprehensive examples Architecture Documentation Created/updated core architecture docs Added cross-references Improved consistency Examples Updated basic-workflow.md Added error handling examples Demonstrated best practices Cross-References Added links between related docs Referenced relevant ADRs Updated navigation structure","title":"Implementation"},{"location":"adrs/0022-documentation-structure-and-cross-references/#references","text":"ADR-0017: ReactiveNode Implementation ADR-0021: Timer Node Implementation Core Framework Abstractions Node Lifecycle Methods","title":"References"},{"location":"adrs/0022-longrunning-node-implementation/","text":"ADR-0022: LongRunning Node Implementation \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 In ADR-0016, we outlined the need for a node type that can handle long-running processes with checkpoints. These long-running nodes enable workflows to: Process work incrementally over multiple sessions Save state between executions Resume from the last checkpoint Handle complex multi-step processes that may be paused and resumed Long-running nodes are particularly useful for: Processing that spans multiple sessions Workflows that need to wait for external events or human interaction Resource-intensive tasks that should be broken into manageable chunks Processes that may be interrupted but need to resume from a checkpoint Decision \u00b6 We will implement a new crate floxide-longrunning that provides the LongRunningNode trait and related implementations as described in ADR-0016. The implementation will follow these design decisions: 1. Core LongRunningOutcome Enum \u00b6 We will implement a LongRunningOutcome enum that represents the two possible outcomes of a long-running process: pub enum LongRunningOutcome < T , S > { /// Processing is complete with result Complete ( T ), /// Processing needs to be suspended with saved state Suspend ( S ), } 2. LongRunningNode Trait \u00b6 We will implement the LongRunningNode trait as outlined in ADR-0016: #[async_trait] pub trait LongRunningNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , Self :: State : Serialize + Deserialize <' static > + Send + Sync + ' static , Self :: Output : Send + ' static , { /// Type representing the node's processing state type State ; /// Type representing the final output type Output ; /// Process the next step, potentially suspending execution async fn process ( & self , state : Option < Self :: State > , ctx : & mut Context , ) -> Result < LongRunningOutcome < Self :: Output , Self :: State > , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; } 3. LongRunningActionExt Extension Trait \u00b6 We will provide a LongRunningActionExt trait to extend ActionType with long-running specific actions: pub trait LongRunningActionExt : ActionType { /// Create a suspend action for long-running nodes fn suspend () -> Self ; /// Create a resume action for long-running nodes fn resume () -> Self ; /// Create a complete action for long-running nodes fn complete () -> Self ; /// Check if this is a suspend action fn is_suspend ( & self ) -> bool ; /// Check if this is a resume action fn is_resume ( & self ) -> bool ; /// Check if this is a complete action fn is_complete ( & self ) -> bool ; } 4. Concrete Implementations \u00b6 We will provide these concrete implementations: SimpleLongRunningNode : A long-running node that uses a closure for processing LongRunningNodeAdapter : An adapter to use a long-running node as a standard node StateStore : A trait for storing and retrieving node states InMemoryStateStore : A simple in-memory implementation of StateStore for testing 5. Workflow Integration \u00b6 The LongRunningNodeAdapter will implement the Node trait, allowing long-running nodes to be used in standard workflows. This adapter will handle state management and action conversion. Consequences \u00b6 Advantages \u00b6 State Persistence : Enables workflows to save and resume state across multiple executions. Incremental Processing : Allows breaking down large tasks into manageable chunks. Checkpoint Recovery : Provides a mechanism for resuming from the last successful checkpoint. Workflow Suspension : Supports pausing workflows for external events or human interaction. Resource Efficiency : Prevents long-running tasks from blocking workflow execution. Disadvantages \u00b6 Complexity : Adds another node type, increasing the conceptual overhead. State Management : Requires proper state serialization and storage infrastructure. Debugging Challenges : Stateful workflows can be more difficult to debug and reason about. Implementation Overhead : Users need to implement proper state management. Implementation Notes \u00b6 States must be serializable and deserializable to be properly stored between executions. The actual storage mechanism (database, file system, etc.) is left to the implementation. The LongRunningNodeAdapter allows using long-running nodes seamlessly in standard workflows. Integration tests demonstrate proper state management and resumption. Alternatives Considered \u00b6 1. Using Event-Driven Nodes for Long-Running Processes \u00b6 We considered implementing long-running process support as a special case of event-driven nodes. However, the state management requirements are sufficiently different to warrant a separate abstraction. 2. Implicit State Management in the Workflow Engine \u00b6 We considered building state management directly into the workflow engine rather than in the nodes. While this would simplify the node implementation, it would make the workflow engine more complex and limit flexibility. 3. Framework-Provided Storage Backend \u00b6 We considered implementing a standard storage backend for states. However, we decided that providing a trait-based interface allows users to implement storage that best fits their needs. Related ADRs \u00b6 ADR-0016: TransformNode Renaming and Async Extension Patterns ADR-0021: Timer Node Implementation References \u00b6 Serde for Rust Tokio Documentation","title":"ADR-0022: LongRunning Node Implementation"},{"location":"adrs/0022-longrunning-node-implementation/#adr-0022-longrunning-node-implementation","text":"","title":"ADR-0022: LongRunning Node Implementation"},{"location":"adrs/0022-longrunning-node-implementation/#status","text":"Accepted","title":"Status"},{"location":"adrs/0022-longrunning-node-implementation/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0022-longrunning-node-implementation/#context","text":"In ADR-0016, we outlined the need for a node type that can handle long-running processes with checkpoints. These long-running nodes enable workflows to: Process work incrementally over multiple sessions Save state between executions Resume from the last checkpoint Handle complex multi-step processes that may be paused and resumed Long-running nodes are particularly useful for: Processing that spans multiple sessions Workflows that need to wait for external events or human interaction Resource-intensive tasks that should be broken into manageable chunks Processes that may be interrupted but need to resume from a checkpoint","title":"Context"},{"location":"adrs/0022-longrunning-node-implementation/#decision","text":"We will implement a new crate floxide-longrunning that provides the LongRunningNode trait and related implementations as described in ADR-0016. The implementation will follow these design decisions:","title":"Decision"},{"location":"adrs/0022-longrunning-node-implementation/#1-core-longrunningoutcome-enum","text":"We will implement a LongRunningOutcome enum that represents the two possible outcomes of a long-running process: pub enum LongRunningOutcome < T , S > { /// Processing is complete with result Complete ( T ), /// Processing needs to be suspended with saved state Suspend ( S ), }","title":"1. Core LongRunningOutcome Enum"},{"location":"adrs/0022-longrunning-node-implementation/#2-longrunningnode-trait","text":"We will implement the LongRunningNode trait as outlined in ADR-0016: #[async_trait] pub trait LongRunningNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , Self :: State : Serialize + Deserialize <' static > + Send + Sync + ' static , Self :: Output : Send + ' static , { /// Type representing the node's processing state type State ; /// Type representing the final output type Output ; /// Process the next step, potentially suspending execution async fn process ( & self , state : Option < Self :: State > , ctx : & mut Context , ) -> Result < LongRunningOutcome < Self :: Output , Self :: State > , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; }","title":"2. LongRunningNode Trait"},{"location":"adrs/0022-longrunning-node-implementation/#3-longrunningactionext-extension-trait","text":"We will provide a LongRunningActionExt trait to extend ActionType with long-running specific actions: pub trait LongRunningActionExt : ActionType { /// Create a suspend action for long-running nodes fn suspend () -> Self ; /// Create a resume action for long-running nodes fn resume () -> Self ; /// Create a complete action for long-running nodes fn complete () -> Self ; /// Check if this is a suspend action fn is_suspend ( & self ) -> bool ; /// Check if this is a resume action fn is_resume ( & self ) -> bool ; /// Check if this is a complete action fn is_complete ( & self ) -> bool ; }","title":"3. LongRunningActionExt Extension Trait"},{"location":"adrs/0022-longrunning-node-implementation/#4-concrete-implementations","text":"We will provide these concrete implementations: SimpleLongRunningNode : A long-running node that uses a closure for processing LongRunningNodeAdapter : An adapter to use a long-running node as a standard node StateStore : A trait for storing and retrieving node states InMemoryStateStore : A simple in-memory implementation of StateStore for testing","title":"4. Concrete Implementations"},{"location":"adrs/0022-longrunning-node-implementation/#5-workflow-integration","text":"The LongRunningNodeAdapter will implement the Node trait, allowing long-running nodes to be used in standard workflows. This adapter will handle state management and action conversion.","title":"5. Workflow Integration"},{"location":"adrs/0022-longrunning-node-implementation/#consequences","text":"","title":"Consequences"},{"location":"adrs/0022-longrunning-node-implementation/#advantages","text":"State Persistence : Enables workflows to save and resume state across multiple executions. Incremental Processing : Allows breaking down large tasks into manageable chunks. Checkpoint Recovery : Provides a mechanism for resuming from the last successful checkpoint. Workflow Suspension : Supports pausing workflows for external events or human interaction. Resource Efficiency : Prevents long-running tasks from blocking workflow execution.","title":"Advantages"},{"location":"adrs/0022-longrunning-node-implementation/#disadvantages","text":"Complexity : Adds another node type, increasing the conceptual overhead. State Management : Requires proper state serialization and storage infrastructure. Debugging Challenges : Stateful workflows can be more difficult to debug and reason about. Implementation Overhead : Users need to implement proper state management.","title":"Disadvantages"},{"location":"adrs/0022-longrunning-node-implementation/#implementation-notes","text":"States must be serializable and deserializable to be properly stored between executions. The actual storage mechanism (database, file system, etc.) is left to the implementation. The LongRunningNodeAdapter allows using long-running nodes seamlessly in standard workflows. Integration tests demonstrate proper state management and resumption.","title":"Implementation Notes"},{"location":"adrs/0022-longrunning-node-implementation/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0022-longrunning-node-implementation/#1-using-event-driven-nodes-for-long-running-processes","text":"We considered implementing long-running process support as a special case of event-driven nodes. However, the state management requirements are sufficiently different to warrant a separate abstraction.","title":"1. Using Event-Driven Nodes for Long-Running Processes"},{"location":"adrs/0022-longrunning-node-implementation/#2-implicit-state-management-in-the-workflow-engine","text":"We considered building state management directly into the workflow engine rather than in the nodes. While this would simplify the node implementation, it would make the workflow engine more complex and limit flexibility.","title":"2. Implicit State Management in the Workflow Engine"},{"location":"adrs/0022-longrunning-node-implementation/#3-framework-provided-storage-backend","text":"We considered implementing a standard storage backend for states. However, we decided that providing a trait-based interface allows users to implement storage that best fits their needs.","title":"3. Framework-Provided Storage Backend"},{"location":"adrs/0022-longrunning-node-implementation/#related-adrs","text":"ADR-0016: TransformNode Renaming and Async Extension Patterns ADR-0021: Timer Node Implementation","title":"Related ADRs"},{"location":"adrs/0022-longrunning-node-implementation/#references","text":"Serde for Rust Tokio Documentation","title":"References"},{"location":"adrs/0023-examples-directory-restructuring/","text":"ADR-0023: Examples Directory Restructuring \u00b6 Status \u00b6 Implemented Date \u00b6 2025-02-27 Context \u00b6 Currently, the examples in the floxide framework are organized as a separate crate in the workspace defined in the examples/ directory with its own Cargo.toml . This approach has several drawbacks: It adds unnecessary complexity to the project structure It requires maintaining a separate crate with its own dependencies It doesn't align with the standard Rust convention for examples The examples are still being defined in the root Cargo.toml file using the [[example]] format, creating redundancy The standard Rust convention is to have a simple examples/ directory with standalone .rs files that are referenced in the root Cargo.toml file, without a separate crate structure. Decision \u00b6 We will restructure the examples directory to follow the standard Rust convention: Remove the separate crate structure (delete examples/Cargo.toml and examples/src/ ) Keep the .rs files in the examples/ directory Keep the examples configuration in the root Cargo.toml file Ensure all examples can still be run with cargo run --example example_name Consequences \u00b6 Advantages \u00b6 Simplifies Project Structure : Removes unnecessary complexity from the project Follows Rust Conventions : Better aligns with standard Rust practices for organizing examples Reduces Maintenance Overhead : No need to maintain a separate crate with its own dependencies Clearer Organization : Makes the purpose of the examples directory more obvious to new contributors Disadvantages \u00b6 Migration Effort : Requires removing the crate structure while ensuring examples still work Potential Build Changes : May require adjusting how examples are built and run Implementation Details \u00b6 The implementation will involve: Removing examples/Cargo.toml Removing examples/src/ directory Keeping all existing .rs example files in the examples/ directory Ensuring all examples use dependencies from the root Cargo.toml Testing that all examples can be run using cargo run --example example_name Repository Information \u00b6 This ADR applies to the Floxide project hosted at github.com/aitoroses/floxide . Related ADRs \u00b6 ADR-0002: Project Structure and Crate Organization ADR-0019: Examples Structure Standardization","title":"ADR-0023: Examples Directory Restructuring"},{"location":"adrs/0023-examples-directory-restructuring/#adr-0023-examples-directory-restructuring","text":"","title":"ADR-0023: Examples Directory Restructuring"},{"location":"adrs/0023-examples-directory-restructuring/#status","text":"Implemented","title":"Status"},{"location":"adrs/0023-examples-directory-restructuring/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0023-examples-directory-restructuring/#context","text":"Currently, the examples in the floxide framework are organized as a separate crate in the workspace defined in the examples/ directory with its own Cargo.toml . This approach has several drawbacks: It adds unnecessary complexity to the project structure It requires maintaining a separate crate with its own dependencies It doesn't align with the standard Rust convention for examples The examples are still being defined in the root Cargo.toml file using the [[example]] format, creating redundancy The standard Rust convention is to have a simple examples/ directory with standalone .rs files that are referenced in the root Cargo.toml file, without a separate crate structure.","title":"Context"},{"location":"adrs/0023-examples-directory-restructuring/#decision","text":"We will restructure the examples directory to follow the standard Rust convention: Remove the separate crate structure (delete examples/Cargo.toml and examples/src/ ) Keep the .rs files in the examples/ directory Keep the examples configuration in the root Cargo.toml file Ensure all examples can still be run with cargo run --example example_name","title":"Decision"},{"location":"adrs/0023-examples-directory-restructuring/#consequences","text":"","title":"Consequences"},{"location":"adrs/0023-examples-directory-restructuring/#advantages","text":"Simplifies Project Structure : Removes unnecessary complexity from the project Follows Rust Conventions : Better aligns with standard Rust practices for organizing examples Reduces Maintenance Overhead : No need to maintain a separate crate with its own dependencies Clearer Organization : Makes the purpose of the examples directory more obvious to new contributors","title":"Advantages"},{"location":"adrs/0023-examples-directory-restructuring/#disadvantages","text":"Migration Effort : Requires removing the crate structure while ensuring examples still work Potential Build Changes : May require adjusting how examples are built and run","title":"Disadvantages"},{"location":"adrs/0023-examples-directory-restructuring/#implementation-details","text":"The implementation will involve: Removing examples/Cargo.toml Removing examples/src/ directory Keeping all existing .rs example files in the examples/ directory Ensuring all examples use dependencies from the root Cargo.toml Testing that all examples can be run using cargo run --example example_name","title":"Implementation Details"},{"location":"adrs/0023-examples-directory-restructuring/#repository-information","text":"This ADR applies to the Floxide project hosted at github.com/aitoroses/floxide .","title":"Repository Information"},{"location":"adrs/0023-examples-directory-restructuring/#related-adrs","text":"ADR-0002: Project Structure and Crate Organization ADR-0019: Examples Structure Standardization","title":"Related ADRs"},{"location":"adrs/0024-mkdocs-documentation-setup/","text":"ADR-0024: MkDocs Documentation Setup with Terminal Theme \u00b6 Status \u00b6 Proposed Date \u00b6 2025-02-27 Context \u00b6 The Floxide project has extensive documentation in the form of ADRs, guides, and examples. However, this documentation is currently scattered across different directories and lacks a cohesive, easily navigable structure. To improve the developer experience and make the documentation more accessible, we need a documentation site that: Presents all documentation in a unified, searchable format Provides clear navigation between different documentation types Renders Markdown files with proper syntax highlighting Aligns with the project's technical nature through an appropriate theme Is easy to maintain and extend as the project grows MkDocs is a popular static site generator specifically designed for project documentation. It uses Markdown files as its source format, which aligns with our existing documentation approach. Additionally, there are terminal-themed options available that would match the technical nature of the Floxide project. Decision \u00b6 We will implement MkDocs with a terminal theme for the Floxide project documentation. Specifically: Use MkDocs as the documentation site generator Implement the \"terminal\" theme to align with the project's technical nature Organize documentation into a clear hierarchy: Getting Started Core Concepts Guides API Reference Examples Architecture (ADRs) Configure automatic deployment of the documentation site through GitHub Pages Include search functionality for easy navigation Ensure proper syntax highlighting for Rust code examples Consequences \u00b6 Advantages \u00b6 Improved developer experience with a unified, searchable documentation site Better organization of existing documentation Easier maintenance of documentation alongside code Improved discoverability of project features and patterns Professional appearance that aligns with the project's technical nature Simplified onboarding for new contributors Disadvantages \u00b6 Additional dependency on MkDocs and its requirements Need for ongoing maintenance of the documentation structure Potential for documentation to become outdated if not actively maintained Migration Path \u00b6 Install MkDocs and required plugins Create initial configuration in mkdocs.yml Organize existing documentation into the new structure Set up GitHub Actions for automatic deployment Update contribution guidelines to include documentation practices Alternatives Considered \u00b6 Rust-specific documentation tools (mdBook) \u00b6 mdBook is a documentation tool created by the Rust team and used for the Rust documentation. While it would align well with a Rust project, MkDocs offers more flexibility in themes and plugins, particularly for the terminal theme requirement. GitHub Wiki \u00b6 GitHub's built-in wiki would require minimal setup but lacks the customization options and local development capabilities that MkDocs provides. Custom documentation site \u00b6 Building a custom documentation site would offer maximum flexibility but would require significantly more development and maintenance effort. Implementation Notes \u00b6 The implementation will include: Installing MkDocs and the terminal theme Creating a mkdocs.yml configuration file Organizing existing documentation into the new structure Setting up GitHub Actions for automatic deployment Adding search functionality Configuring syntax highlighting for Rust code Related ADRs \u00b6 ADR-0001: ADR Process and Format ADR-0002: Project Structure and Crate Organization ADR-0019: Examples Structure Standardization References \u00b6 MkDocs Official Documentation MkDocs Terminal Theme GitHub Pages Deployment","title":"ADR-0024: MkDocs Documentation Setup with Terminal Theme"},{"location":"adrs/0024-mkdocs-documentation-setup/#adr-0024-mkdocs-documentation-setup-with-terminal-theme","text":"","title":"ADR-0024: MkDocs Documentation Setup with Terminal Theme"},{"location":"adrs/0024-mkdocs-documentation-setup/#status","text":"Proposed","title":"Status"},{"location":"adrs/0024-mkdocs-documentation-setup/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0024-mkdocs-documentation-setup/#context","text":"The Floxide project has extensive documentation in the form of ADRs, guides, and examples. However, this documentation is currently scattered across different directories and lacks a cohesive, easily navigable structure. To improve the developer experience and make the documentation more accessible, we need a documentation site that: Presents all documentation in a unified, searchable format Provides clear navigation between different documentation types Renders Markdown files with proper syntax highlighting Aligns with the project's technical nature through an appropriate theme Is easy to maintain and extend as the project grows MkDocs is a popular static site generator specifically designed for project documentation. It uses Markdown files as its source format, which aligns with our existing documentation approach. Additionally, there are terminal-themed options available that would match the technical nature of the Floxide project.","title":"Context"},{"location":"adrs/0024-mkdocs-documentation-setup/#decision","text":"We will implement MkDocs with a terminal theme for the Floxide project documentation. Specifically: Use MkDocs as the documentation site generator Implement the \"terminal\" theme to align with the project's technical nature Organize documentation into a clear hierarchy: Getting Started Core Concepts Guides API Reference Examples Architecture (ADRs) Configure automatic deployment of the documentation site through GitHub Pages Include search functionality for easy navigation Ensure proper syntax highlighting for Rust code examples","title":"Decision"},{"location":"adrs/0024-mkdocs-documentation-setup/#consequences","text":"","title":"Consequences"},{"location":"adrs/0024-mkdocs-documentation-setup/#advantages","text":"Improved developer experience with a unified, searchable documentation site Better organization of existing documentation Easier maintenance of documentation alongside code Improved discoverability of project features and patterns Professional appearance that aligns with the project's technical nature Simplified onboarding for new contributors","title":"Advantages"},{"location":"adrs/0024-mkdocs-documentation-setup/#disadvantages","text":"Additional dependency on MkDocs and its requirements Need for ongoing maintenance of the documentation structure Potential for documentation to become outdated if not actively maintained","title":"Disadvantages"},{"location":"adrs/0024-mkdocs-documentation-setup/#migration-path","text":"Install MkDocs and required plugins Create initial configuration in mkdocs.yml Organize existing documentation into the new structure Set up GitHub Actions for automatic deployment Update contribution guidelines to include documentation practices","title":"Migration Path"},{"location":"adrs/0024-mkdocs-documentation-setup/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0024-mkdocs-documentation-setup/#rust-specific-documentation-tools-mdbook","text":"mdBook is a documentation tool created by the Rust team and used for the Rust documentation. While it would align well with a Rust project, MkDocs offers more flexibility in themes and plugins, particularly for the terminal theme requirement.","title":"Rust-specific documentation tools (mdBook)"},{"location":"adrs/0024-mkdocs-documentation-setup/#github-wiki","text":"GitHub's built-in wiki would require minimal setup but lacks the customization options and local development capabilities that MkDocs provides.","title":"GitHub Wiki"},{"location":"adrs/0024-mkdocs-documentation-setup/#custom-documentation-site","text":"Building a custom documentation site would offer maximum flexibility but would require significantly more development and maintenance effort.","title":"Custom documentation site"},{"location":"adrs/0024-mkdocs-documentation-setup/#implementation-notes","text":"The implementation will include: Installing MkDocs and the terminal theme Creating a mkdocs.yml configuration file Organizing existing documentation into the new structure Setting up GitHub Actions for automatic deployment Adding search functionality Configuring syntax highlighting for Rust code","title":"Implementation Notes"},{"location":"adrs/0024-mkdocs-documentation-setup/#related-adrs","text":"ADR-0001: ADR Process and Format ADR-0002: Project Structure and Crate Organization ADR-0019: Examples Structure Standardization","title":"Related ADRs"},{"location":"adrs/0024-mkdocs-documentation-setup/#references","text":"MkDocs Official Documentation MkDocs Terminal Theme GitHub Pages Deployment","title":"References"},{"location":"adrs/0025-mermaid-diagrams-for-mkdocs/","text":"ADR-0025: Mermaid Diagram Support for MkDocs \u00b6 Status \u00b6 Proposed Date \u00b6 2025-02-27 Context \u00b6 Floxide is a directed graph workflow system, and visualizing these workflows is essential for documentation and understanding. Mermaid is a JavaScript-based diagramming tool that allows creating diagrams using Markdown-like syntax, making it ideal for our documentation needs. While our MkDocs configuration already includes some Mermaid support, we need to ensure it's properly configured and working correctly across all documentation pages. This includes: Proper rendering of Mermaid diagrams in the MkDocs site Consistent styling that works with our terminal theme Support for all diagram types needed for workflow visualization (flowcharts, sequence diagrams, class diagrams) Proper deployment of the MkDocs site with Mermaid support Decision \u00b6 We will enhance our MkDocs configuration to fully support Mermaid diagrams by: Ensuring the proper Mermaid JavaScript library is included Configuring the PyMdown Extensions for custom fences to properly render Mermaid diagrams Adding initialization code to ensure Mermaid works with our terminal theme Creating a GitHub workflow to build and deploy the MkDocs documentation site Providing comprehensive examples in our documentation Consequences \u00b6 Advantages \u00b6 Improved visualization of workflows and architectural concepts Better documentation through visual representation Easier understanding of complex workflow patterns Consistent diagram styling across the documentation Disadvantages \u00b6 Additional JavaScript dependency Potential rendering issues with certain diagram types Need for ongoing maintenance of diagrams as the codebase evolves Implementation Details \u00b6 Update mkdocs.yml to include: The latest Mermaid JavaScript library Proper configuration of PyMdown Extensions for Mermaid fences Mermaid initialization code to work with the terminal theme Create a GitHub workflow for building and deploying the MkDocs site Update the existing Mermaid diagrams guide with additional examples specific to Floxide Alternatives Considered \u00b6 Using static images instead of Mermaid \u00b6 Static images would be simpler but would make the documentation harder to maintain as diagrams would need to be regenerated whenever changes are made. Using a different diagramming tool \u00b6 Other diagramming tools like PlantUML were considered, but Mermaid has better integration with Markdown and MkDocs, and its syntax is more intuitive for our needs. Related ADRs \u00b6 ADR-0024: MkDocs Documentation Setup with Terminal Theme References \u00b6 Mermaid Official Documentation PyMdown Extensions Documentation MkDocs Documentation","title":"ADR-0025: Mermaid Diagram Support for MkDocs"},{"location":"adrs/0025-mermaid-diagrams-for-mkdocs/#adr-0025-mermaid-diagram-support-for-mkdocs","text":"","title":"ADR-0025: Mermaid Diagram Support for MkDocs"},{"location":"adrs/0025-mermaid-diagrams-for-mkdocs/#status","text":"Proposed","title":"Status"},{"location":"adrs/0025-mermaid-diagrams-for-mkdocs/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0025-mermaid-diagrams-for-mkdocs/#context","text":"Floxide is a directed graph workflow system, and visualizing these workflows is essential for documentation and understanding. Mermaid is a JavaScript-based diagramming tool that allows creating diagrams using Markdown-like syntax, making it ideal for our documentation needs. While our MkDocs configuration already includes some Mermaid support, we need to ensure it's properly configured and working correctly across all documentation pages. This includes: Proper rendering of Mermaid diagrams in the MkDocs site Consistent styling that works with our terminal theme Support for all diagram types needed for workflow visualization (flowcharts, sequence diagrams, class diagrams) Proper deployment of the MkDocs site with Mermaid support","title":"Context"},{"location":"adrs/0025-mermaid-diagrams-for-mkdocs/#decision","text":"We will enhance our MkDocs configuration to fully support Mermaid diagrams by: Ensuring the proper Mermaid JavaScript library is included Configuring the PyMdown Extensions for custom fences to properly render Mermaid diagrams Adding initialization code to ensure Mermaid works with our terminal theme Creating a GitHub workflow to build and deploy the MkDocs documentation site Providing comprehensive examples in our documentation","title":"Decision"},{"location":"adrs/0025-mermaid-diagrams-for-mkdocs/#consequences","text":"","title":"Consequences"},{"location":"adrs/0025-mermaid-diagrams-for-mkdocs/#advantages","text":"Improved visualization of workflows and architectural concepts Better documentation through visual representation Easier understanding of complex workflow patterns Consistent diagram styling across the documentation","title":"Advantages"},{"location":"adrs/0025-mermaid-diagrams-for-mkdocs/#disadvantages","text":"Additional JavaScript dependency Potential rendering issues with certain diagram types Need for ongoing maintenance of diagrams as the codebase evolves","title":"Disadvantages"},{"location":"adrs/0025-mermaid-diagrams-for-mkdocs/#implementation-details","text":"Update mkdocs.yml to include: The latest Mermaid JavaScript library Proper configuration of PyMdown Extensions for Mermaid fences Mermaid initialization code to work with the terminal theme Create a GitHub workflow for building and deploying the MkDocs site Update the existing Mermaid diagrams guide with additional examples specific to Floxide","title":"Implementation Details"},{"location":"adrs/0025-mermaid-diagrams-for-mkdocs/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0025-mermaid-diagrams-for-mkdocs/#using-static-images-instead-of-mermaid","text":"Static images would be simpler but would make the documentation harder to maintain as diagrams would need to be regenerated whenever changes are made.","title":"Using static images instead of Mermaid"},{"location":"adrs/0025-mermaid-diagrams-for-mkdocs/#using-a-different-diagramming-tool","text":"Other diagramming tools like PlantUML were considered, but Mermaid has better integration with Markdown and MkDocs, and its syntax is more intuitive for our needs.","title":"Using a different diagramming tool"},{"location":"adrs/0025-mermaid-diagrams-for-mkdocs/#related-adrs","text":"ADR-0024: MkDocs Documentation Setup with Terminal Theme","title":"Related ADRs"},{"location":"adrs/0025-mermaid-diagrams-for-mkdocs/#references","text":"Mermaid Official Documentation PyMdown Extensions Documentation MkDocs Documentation","title":"References"},{"location":"adrs/0026-documentation-deployment-strategy/","text":"ADR-0026: Documentation Deployment Strategy \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 As the Floxide framework matures, we need a reliable and automated way to deploy our documentation to GitHub Pages. This includes both API documentation generated from Rustdoc and conceptual documentation created with MkDocs. We need to determine the best approach for deploying this documentation, considering: Automation through GitHub Actions Permissions and security considerations Consistency and reliability of the deployment process Support for both Rustdoc and MkDocs documentation Decision \u00b6 We will implement a documentation deployment strategy using GitHub Actions with the following components: Documentation Types \u00b6 We will maintain two types of documentation: API Documentation : Generated from Rustdoc comments in the codebase (planned for future implementation) Conceptual Documentation : Written in Markdown and built with MkDocs (currently implemented) Current Implementation Status \u00b6 The current implementation focuses on MkDocs-based conceptual documentation. Rustdoc API documentation will be added in a future iteration. Deployment Approach \u00b6 We will use GitHub Actions to automate the deployment process: Direct GitHub Pages Deployment : We will use the peaceiris/actions-gh-pages@v3 action to directly deploy to the gh-pages branch, which is more reliable and has fewer permission issues than the newer GitHub Pages deployment approach. Permissions Configuration : We will set the following permissions for the GitHub Actions workflow: permissions : contents : write # Allows pushing to the gh-pages branch Force Orphan : We will use the force_orphan: true option to ensure a clean gh-pages branch with only the latest documentation. GitHub Pages Source : We will configure GitHub Pages to use the gh-pages branch as the source for deployment. Workflow Triggers \u00b6 The documentation deployment workflow will be triggered on: Pushes to the main branch Manual triggers through the workflow_dispatch event Concurrency Control \u00b6 We will implement concurrency control to ensure only one deployment runs at a time: concurrency : group : \"pages\" cancel-in-progress : true Consequences \u00b6 Positive \u00b6 Automated Deployment : Documentation is automatically updated when changes are pushed to the main branch. Consistent Process : The deployment process is consistent and reliable. Proper Permissions : The workflow uses the minimum necessary permissions for deployment. Modern Approach : We use the latest GitHub Actions features for Pages deployment. Manual Option : Documentation can be manually deployed when needed. Negative \u00b6 Setup Complexity : Initial setup requires configuring GitHub Pages in repository settings. Maintenance Overhead : The workflow may need updates as GitHub Actions evolves. Potential Conflicts : Multiple documentation types may require careful coordination. Incomplete Implementation : The current implementation only includes MkDocs documentation, with Rustdoc integration planned for the future. Future Work \u00b6 Rustdoc Integration : Add Rust API documentation generation to the workflow. Combined Documentation : Ensure both documentation types are accessible from a unified interface. Alternatives Considered \u00b6 GitHub Actions Pages Deployment API \u00b6 Pros : Modern approach with GitHub's latest features Potentially more secure Cleaner separation of concerns Cons : Requires specific permissions that may be restricted in some organizations More complex setup with environment configurations Less reliable in some repository configurations We initially tried this approach but encountered permission issues with the GitHub Pages API. The error \"Resource not accessible by integration\" indicated that our GitHub Actions workflow didn't have sufficient permissions to create or configure a GitHub Pages site, despite having the correct permission settings in the workflow file. External Documentation Hosting \u00b6 Pros : Independence from GitHub infrastructure Potentially more customization options Cons : Additional service to maintain Separate authentication and deployment process Disconnected from the repository We chose GitHub Pages for its tight integration with our GitHub repository and sufficient feature set for our documentation needs.","title":"ADR-0026: Documentation Deployment Strategy"},{"location":"adrs/0026-documentation-deployment-strategy/#adr-0026-documentation-deployment-strategy","text":"","title":"ADR-0026: Documentation Deployment Strategy"},{"location":"adrs/0026-documentation-deployment-strategy/#status","text":"Accepted","title":"Status"},{"location":"adrs/0026-documentation-deployment-strategy/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0026-documentation-deployment-strategy/#context","text":"As the Floxide framework matures, we need a reliable and automated way to deploy our documentation to GitHub Pages. This includes both API documentation generated from Rustdoc and conceptual documentation created with MkDocs. We need to determine the best approach for deploying this documentation, considering: Automation through GitHub Actions Permissions and security considerations Consistency and reliability of the deployment process Support for both Rustdoc and MkDocs documentation","title":"Context"},{"location":"adrs/0026-documentation-deployment-strategy/#decision","text":"We will implement a documentation deployment strategy using GitHub Actions with the following components:","title":"Decision"},{"location":"adrs/0026-documentation-deployment-strategy/#documentation-types","text":"We will maintain two types of documentation: API Documentation : Generated from Rustdoc comments in the codebase (planned for future implementation) Conceptual Documentation : Written in Markdown and built with MkDocs (currently implemented)","title":"Documentation Types"},{"location":"adrs/0026-documentation-deployment-strategy/#current-implementation-status","text":"The current implementation focuses on MkDocs-based conceptual documentation. Rustdoc API documentation will be added in a future iteration.","title":"Current Implementation Status"},{"location":"adrs/0026-documentation-deployment-strategy/#deployment-approach","text":"We will use GitHub Actions to automate the deployment process: Direct GitHub Pages Deployment : We will use the peaceiris/actions-gh-pages@v3 action to directly deploy to the gh-pages branch, which is more reliable and has fewer permission issues than the newer GitHub Pages deployment approach. Permissions Configuration : We will set the following permissions for the GitHub Actions workflow: permissions : contents : write # Allows pushing to the gh-pages branch Force Orphan : We will use the force_orphan: true option to ensure a clean gh-pages branch with only the latest documentation. GitHub Pages Source : We will configure GitHub Pages to use the gh-pages branch as the source for deployment.","title":"Deployment Approach"},{"location":"adrs/0026-documentation-deployment-strategy/#workflow-triggers","text":"The documentation deployment workflow will be triggered on: Pushes to the main branch Manual triggers through the workflow_dispatch event","title":"Workflow Triggers"},{"location":"adrs/0026-documentation-deployment-strategy/#concurrency-control","text":"We will implement concurrency control to ensure only one deployment runs at a time: concurrency : group : \"pages\" cancel-in-progress : true","title":"Concurrency Control"},{"location":"adrs/0026-documentation-deployment-strategy/#consequences","text":"","title":"Consequences"},{"location":"adrs/0026-documentation-deployment-strategy/#positive","text":"Automated Deployment : Documentation is automatically updated when changes are pushed to the main branch. Consistent Process : The deployment process is consistent and reliable. Proper Permissions : The workflow uses the minimum necessary permissions for deployment. Modern Approach : We use the latest GitHub Actions features for Pages deployment. Manual Option : Documentation can be manually deployed when needed.","title":"Positive"},{"location":"adrs/0026-documentation-deployment-strategy/#negative","text":"Setup Complexity : Initial setup requires configuring GitHub Pages in repository settings. Maintenance Overhead : The workflow may need updates as GitHub Actions evolves. Potential Conflicts : Multiple documentation types may require careful coordination. Incomplete Implementation : The current implementation only includes MkDocs documentation, with Rustdoc integration planned for the future.","title":"Negative"},{"location":"adrs/0026-documentation-deployment-strategy/#future-work","text":"Rustdoc Integration : Add Rust API documentation generation to the workflow. Combined Documentation : Ensure both documentation types are accessible from a unified interface.","title":"Future Work"},{"location":"adrs/0026-documentation-deployment-strategy/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0026-documentation-deployment-strategy/#github-actions-pages-deployment-api","text":"Pros : Modern approach with GitHub's latest features Potentially more secure Cleaner separation of concerns Cons : Requires specific permissions that may be restricted in some organizations More complex setup with environment configurations Less reliable in some repository configurations We initially tried this approach but encountered permission issues with the GitHub Pages API. The error \"Resource not accessible by integration\" indicated that our GitHub Actions workflow didn't have sufficient permissions to create or configure a GitHub Pages site, despite having the correct permission settings in the workflow file.","title":"GitHub Actions Pages Deployment API"},{"location":"adrs/0026-documentation-deployment-strategy/#external-documentation-hosting","text":"Pros : Independence from GitHub infrastructure Potentially more customization options Cons : Additional service to maintain Separate authentication and deployment process Disconnected from the repository We chose GitHub Pages for its tight integration with our GitHub repository and sufficient feature set for our documentation needs.","title":"External Documentation Hosting"},{"location":"adrs/0027-github-actions-workflow-permissions/","text":"ADR-0027: GitHub Actions Workflow Permissions \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 Our project uses GitHub Actions for various automation tasks, including CI/CD, documentation deployment, and version bumping. Each of these workflows requires specific permissions to perform their tasks. In particular, workflows that need to push changes to the repository (such as version bumping and documentation deployment) require write permissions to the repository contents. We encountered an issue with our version bump workflow where the GitHub Actions bot was denied permission to push changes to the repository: error: unable to push to remote, out = , err = remote: Permission to aitoroses/floxide.git denied to github-actions[bot]. fatal: unable to access 'https://github.com/aitoroses/floxide/': The requested URL returned error: 403 This indicates that the workflow does not have the necessary permissions to push changes to the repository. Decision \u00b6 We will explicitly set the required permissions for each GitHub Actions workflow to ensure they have the necessary access to perform their tasks. Specifically: For workflows that need to push changes to the repository (version-bump.yml, docs.yml), we will set: permissions : contents : write # Allows pushing to the repository For workflows that only need to read from the repository (ci.yml), we will set: permissions : contents : read For the release workflow that needs to publish to crates.io, we will continue to use the CRATES_IO_TOKEN secret. We will update all existing workflows to include these explicit permission declarations to prevent permission-related issues in the future. Consequences \u00b6 Positive \u00b6 Clear Permission Model : Explicitly declaring permissions makes it clear what access each workflow requires. Reduced Errors : Proper permission settings will prevent permission-denied errors during workflow execution. Security Best Practice : Following the principle of least privilege by only granting the permissions each workflow needs. Consistency : All workflows will follow the same pattern for permission declaration. Negative \u00b6 Maintenance Overhead : Additional configuration to maintain in each workflow file. Potential for Over-permissioning : If not carefully managed, workflows might be granted more permissions than necessary. Implementation Plan \u00b6 Update the version-bump.yml workflow to include the contents: write permission. Review and update all other workflows to include appropriate permission declarations. Test the workflows to ensure they function correctly with the new permission settings. Alternatives Considered \u00b6 Using Personal Access Tokens (PATs) \u00b6 Pros : More flexible permissions Can work across repositories Cons : Security risk if tokens are compromised Additional secret management More complex setup We chose to use the built-in GITHUB_TOKEN with explicit permissions as it provides sufficient access for our needs while being more secure and easier to manage. Repository-Level Permission Settings \u00b6 Pros : Centralized permission management Applies to all workflows Cons : Less granular control May grant more permissions than necessary to some workflows We chose workflow-specific permissions for more granular control and to follow the principle of least privilege.","title":"ADR-0027: GitHub Actions Workflow Permissions"},{"location":"adrs/0027-github-actions-workflow-permissions/#adr-0027-github-actions-workflow-permissions","text":"","title":"ADR-0027: GitHub Actions Workflow Permissions"},{"location":"adrs/0027-github-actions-workflow-permissions/#status","text":"Accepted","title":"Status"},{"location":"adrs/0027-github-actions-workflow-permissions/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0027-github-actions-workflow-permissions/#context","text":"Our project uses GitHub Actions for various automation tasks, including CI/CD, documentation deployment, and version bumping. Each of these workflows requires specific permissions to perform their tasks. In particular, workflows that need to push changes to the repository (such as version bumping and documentation deployment) require write permissions to the repository contents. We encountered an issue with our version bump workflow where the GitHub Actions bot was denied permission to push changes to the repository: error: unable to push to remote, out = , err = remote: Permission to aitoroses/floxide.git denied to github-actions[bot]. fatal: unable to access 'https://github.com/aitoroses/floxide/': The requested URL returned error: 403 This indicates that the workflow does not have the necessary permissions to push changes to the repository.","title":"Context"},{"location":"adrs/0027-github-actions-workflow-permissions/#decision","text":"We will explicitly set the required permissions for each GitHub Actions workflow to ensure they have the necessary access to perform their tasks. Specifically: For workflows that need to push changes to the repository (version-bump.yml, docs.yml), we will set: permissions : contents : write # Allows pushing to the repository For workflows that only need to read from the repository (ci.yml), we will set: permissions : contents : read For the release workflow that needs to publish to crates.io, we will continue to use the CRATES_IO_TOKEN secret. We will update all existing workflows to include these explicit permission declarations to prevent permission-related issues in the future.","title":"Decision"},{"location":"adrs/0027-github-actions-workflow-permissions/#consequences","text":"","title":"Consequences"},{"location":"adrs/0027-github-actions-workflow-permissions/#positive","text":"Clear Permission Model : Explicitly declaring permissions makes it clear what access each workflow requires. Reduced Errors : Proper permission settings will prevent permission-denied errors during workflow execution. Security Best Practice : Following the principle of least privilege by only granting the permissions each workflow needs. Consistency : All workflows will follow the same pattern for permission declaration.","title":"Positive"},{"location":"adrs/0027-github-actions-workflow-permissions/#negative","text":"Maintenance Overhead : Additional configuration to maintain in each workflow file. Potential for Over-permissioning : If not carefully managed, workflows might be granted more permissions than necessary.","title":"Negative"},{"location":"adrs/0027-github-actions-workflow-permissions/#implementation-plan","text":"Update the version-bump.yml workflow to include the contents: write permission. Review and update all other workflows to include appropriate permission declarations. Test the workflows to ensure they function correctly with the new permission settings.","title":"Implementation Plan"},{"location":"adrs/0027-github-actions-workflow-permissions/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0027-github-actions-workflow-permissions/#using-personal-access-tokens-pats","text":"Pros : More flexible permissions Can work across repositories Cons : Security risk if tokens are compromised Additional secret management More complex setup We chose to use the built-in GITHUB_TOKEN with explicit permissions as it provides sufficient access for our needs while being more secure and easier to manage.","title":"Using Personal Access Tokens (PATs)"},{"location":"adrs/0027-github-actions-workflow-permissions/#repository-level-permission-settings","text":"Pros : Centralized permission management Applies to all workflows Cons : Less granular control May grant more permissions than necessary to some workflows We chose workflow-specific permissions for more granular control and to follow the principle of least privilege.","title":"Repository-Level Permission Settings"},{"location":"adrs/0028-version-bump-workflow-improvements/","text":"ADR-0028: Version Bump Workflow Improvements \u00b6 Status \u00b6 Proposed Date \u00b6 2025-02-27 Context \u00b6 The Floxide framework uses GitHub Actions for CI/CD, including a workflow for version bumping that automates the process of updating version numbers across the workspace. This workflow uses cargo-workspaces to coordinate version changes and creates Git tags for releases. We've encountered several issues with the version bump workflow: Tag Conflict Error : The workflow fails with the error: fatal: tag 'v1.0.0' already exists Error: Process completed with exit code 128 This occurs because: - The workflow attempts to create a tag that already exists in the repository - The current implementation doesn't check for existing tags before attempting to create them - The workflow creates both individual package tags (e.g., floxide-core@1.0.0 ) and a global version tag (e.g., v1.0.0 ) Dependency Version Mismatch : When trying to update only some crates in the workspace, we encounter errors like: error: failed to select a version for the requirement `floxide-core = \"=1.0.0\"` candidate versions found which didn't match: 1.0.1 This happens because: - Some crates are updated while others remain at the previous version - Crates have exact version dependencies on each other (using = in version requirements) - This creates inconsistent dependency requirements that cannot be satisfied Explicit Version Requirements Not Updated : The main floxide crate at the root has explicit version requirements for its dependencies: floxide-core = { path = \"crates/floxide-core\" , version = \"1.0.0\" , optional = true } When the workspace version is updated, these explicit version requirements are not automatically updated by cargo-workspaces , causing dependency resolution errors. Additionally, when the version bump workflow fails, it doesn't trigger the release workflow that publishes the crates to crates.io, breaking the automated release process. Decision \u00b6 We will improve the version bump workflow to handle existing tags gracefully, ensure consistent versioning across all crates, update explicit version requirements, and ensure proper triggering of the release workflow by implementing the following changes: Check for Existing Tags : Before attempting to create tags, check if they already exist. Force Option for Tags : Add an option to force-update existing tags when necessary. Skip Tag Creation Option : Add a workflow input parameter to optionally skip tag creation entirely. Trigger Release Option : Add an option to ensure the release workflow is triggered after a successful version bump. Handle Individual Crate Tags : When force-updating tags, also handle individual crate tags. Update All Crates Together : Use the --all flag with cargo workspaces to ensure all crates in the workspace are updated to the same version, maintaining consistent dependencies. Update Explicit Version Requirements : Add a step to update the explicit version requirements in the main Cargo.toml file. Improved Error Handling : Enhance error handling to provide clearer messages when tag-related issues occur. The implementation will involve updating the .github/workflows/version-bump.yml file to include these improvements. Updated Workflow \u00b6 The updated workflow will include: # In the workflow_dispatch inputs section inputs : bump_type : description : \"Version bump type\" required : true type : choice options : - patch - minor - major dry_run : description : \"Dry run (no commits)\" required : true type : boolean default : true force_tag : description : \"Force update existing tags\" required : false type : boolean default : false skip_tagging : description : \"Skip tag creation entirely\" required : false type : boolean default : false trigger_release : description : \"Trigger release workflow after version bump\" required : false type : boolean default : false The version bump command will be updated to: # Use --no-git-tag to handle tagging manually and --all to update all packages cargo workspaces version ${{ github.event.inputs.bump_type }} --exact --yes --no-git-tag --all And we'll add a step to update explicit version requirements: # Also update the explicit version requirements in the main Cargo.toml # This is needed because cargo-workspaces doesn't update these sed -i \"s/floxide-core = { path = \\\"crates\\/floxide-core\\\", version = \\\"[0-9.]*\\\"/floxide-core = { path = \\\"crates\\/floxide-core\\\", version = \\\"$NEW_VERSION\\\"/g\" Cargo.toml sed -i \"s/floxide-transform = { path = \\\"crates\\/floxide-transform\\\", version = \\\"[0-9.]*\\\"/floxide-transform = { path = \\\"crates\\/floxide-transform\\\", version = \\\"$NEW_VERSION\\\"/g\" Cargo.toml # ... (similar lines for other crates) # Commit the changes to the explicit version requirements git add Cargo.toml git commit --amend --no-edit git push --force-with-lease origin HEAD The tag creation step will be updated to: # Create and push tag if not skipped if [ \"${{ github.event.inputs.skip_tagging }}\" != \"true\" ]; then TAG_NAME=\"v${NEW_VERSION}\" # Check if tag exists if git show-ref --tags \"$TAG_NAME\" --quiet; then if [ \"${{ github.event.inputs.force_tag }}\" = \"true\" ]; then echo \"Tag $TAG_NAME exists, force updating\" git tag -d \"$TAG_NAME\" git push origin \":refs/tags/$TAG_NAME\" || true # Also delete individual crate tags if they exist for CRATE_TAG in $(git tag | grep \"@${NEW_VERSION}$\"); do echo \"Deleting individual crate tag : $CRATE_TAG\" git tag -d \"$CRATE_TAG\" git push origin \":refs/tags/$CRATE_TAG\" || true done # Create new tag git tag -a \"$TAG_NAME\" -m \"Release $TAG_NAME\" git push origin \"$TAG_NAME\" echo \"Tags updated successfully\" else echo \"Tag $TAG_NAME already exists. Use force_tag option to update it.\" exit 0 fi else git tag -a \"$TAG_NAME\" -m \"Release $TAG_NAME\" git push origin \"$TAG_NAME\" fi # If trigger_release is true, wait a moment for tag to be processed # and then check if the tag was successfully pushed if [ \"${{ github.event.inputs.trigger_release }}\" = \"true\" ]; then echo \"Waiting for tag to be processed before triggering release workflow...\" sleep 10 # Check if the tag was successfully pushed if git ls-remote --tags origin | grep -q \"$TAG_NAME\"; then echo \"Tag $TAG_NAME successfully pushed, release workflow should be triggered automatically.\" else echo \"Warning : Tag $TAG_NAME not found on remote. Release workflow may not trigger.\" fi fi else echo \"Skipping tag creation as requested\" fi Consequences \u00b6 Positive \u00b6 Improved Reliability : The workflow will handle existing tags gracefully, reducing CI failures. More Flexibility : Users will have more control over tag creation and updates. Better Error Messages : Clearer feedback when tag-related issues occur. Safer Operations : Prevents accidental tag overwrites without explicit permission. Complete Release Process : Ensures the entire release pipeline from version bump to crates.io publishing works correctly. Comprehensive Tag Management : Handles both global and individual crate tags properly. Consistent Versioning : Ensures all crates in the workspace are updated together, preventing dependency version mismatches. Dependency Coherence : Updates explicit version requirements in the main Cargo.toml to match the new workspace version. Negative \u00b6 Increased Complexity : The workflow becomes slightly more complex with additional options. Potential for Confusion : More options may require better documentation for users. Less Granular Control : Using --all means individual crates cannot be versioned independently, which may be limiting in some scenarios. Brittle Version Updating : The sed commands for updating explicit version requirements are somewhat brittle and may need maintenance if the Cargo.toml structure changes. Alternatives Considered \u00b6 Manually Deleting Tags Before Running the Workflow \u00b6 Pros : Simple approach No workflow changes needed Cons : Manual intervention required Error-prone Not scalable We rejected this approach as it doesn't provide a sustainable, automated solution. Using Different Tag Naming Conventions \u00b6 Pros : Could avoid conflicts by using unique names Maintains history differently Cons : Breaks existing conventions May confuse users Requires migration We chose to maintain the existing tag naming convention for consistency and compatibility. Disabling Tagging in cargo-workspaces \u00b6 Pros : Simpler workflow More direct control Cons : Loses some automation benefits Requires manual tag management We chose a hybrid approach that leverages cargo-workspaces for version management but gives us more control over tag creation. Separate Workflows for Version Bump and Release \u00b6 Pros : Clearer separation of concerns Potentially simpler workflows Cons : More manual steps Potential for inconsistency between version and release We chose to enhance the existing workflow structure to maintain the automated pipeline while adding more control options. Independent Versioning of Crates \u00b6 Pros : More granular control over individual crate versions Allows for different release cycles per crate Cons : Requires more complex dependency management Prone to version mismatch errors More difficult to maintain We chose to use the --all flag to ensure consistent versioning across all crates, which simplifies dependency management and reduces the chance of errors. Using Cargo Workspace Inheritance Without Explicit Versions \u00b6 Pros : Simpler dependency declarations Automatic version synchronization Cons : Less explicit about version requirements May not work well with external consumers of the crates We chose to maintain explicit version requirements in the main Cargo.toml for clarity and compatibility with external consumers, while adding automation to keep these versions in sync.","title":"ADR-0028: Version Bump Workflow Improvements"},{"location":"adrs/0028-version-bump-workflow-improvements/#adr-0028-version-bump-workflow-improvements","text":"","title":"ADR-0028: Version Bump Workflow Improvements"},{"location":"adrs/0028-version-bump-workflow-improvements/#status","text":"Proposed","title":"Status"},{"location":"adrs/0028-version-bump-workflow-improvements/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0028-version-bump-workflow-improvements/#context","text":"The Floxide framework uses GitHub Actions for CI/CD, including a workflow for version bumping that automates the process of updating version numbers across the workspace. This workflow uses cargo-workspaces to coordinate version changes and creates Git tags for releases. We've encountered several issues with the version bump workflow: Tag Conflict Error : The workflow fails with the error: fatal: tag 'v1.0.0' already exists Error: Process completed with exit code 128 This occurs because: - The workflow attempts to create a tag that already exists in the repository - The current implementation doesn't check for existing tags before attempting to create them - The workflow creates both individual package tags (e.g., floxide-core@1.0.0 ) and a global version tag (e.g., v1.0.0 ) Dependency Version Mismatch : When trying to update only some crates in the workspace, we encounter errors like: error: failed to select a version for the requirement `floxide-core = \"=1.0.0\"` candidate versions found which didn't match: 1.0.1 This happens because: - Some crates are updated while others remain at the previous version - Crates have exact version dependencies on each other (using = in version requirements) - This creates inconsistent dependency requirements that cannot be satisfied Explicit Version Requirements Not Updated : The main floxide crate at the root has explicit version requirements for its dependencies: floxide-core = { path = \"crates/floxide-core\" , version = \"1.0.0\" , optional = true } When the workspace version is updated, these explicit version requirements are not automatically updated by cargo-workspaces , causing dependency resolution errors. Additionally, when the version bump workflow fails, it doesn't trigger the release workflow that publishes the crates to crates.io, breaking the automated release process.","title":"Context"},{"location":"adrs/0028-version-bump-workflow-improvements/#decision","text":"We will improve the version bump workflow to handle existing tags gracefully, ensure consistent versioning across all crates, update explicit version requirements, and ensure proper triggering of the release workflow by implementing the following changes: Check for Existing Tags : Before attempting to create tags, check if they already exist. Force Option for Tags : Add an option to force-update existing tags when necessary. Skip Tag Creation Option : Add a workflow input parameter to optionally skip tag creation entirely. Trigger Release Option : Add an option to ensure the release workflow is triggered after a successful version bump. Handle Individual Crate Tags : When force-updating tags, also handle individual crate tags. Update All Crates Together : Use the --all flag with cargo workspaces to ensure all crates in the workspace are updated to the same version, maintaining consistent dependencies. Update Explicit Version Requirements : Add a step to update the explicit version requirements in the main Cargo.toml file. Improved Error Handling : Enhance error handling to provide clearer messages when tag-related issues occur. The implementation will involve updating the .github/workflows/version-bump.yml file to include these improvements.","title":"Decision"},{"location":"adrs/0028-version-bump-workflow-improvements/#updated-workflow","text":"The updated workflow will include: # In the workflow_dispatch inputs section inputs : bump_type : description : \"Version bump type\" required : true type : choice options : - patch - minor - major dry_run : description : \"Dry run (no commits)\" required : true type : boolean default : true force_tag : description : \"Force update existing tags\" required : false type : boolean default : false skip_tagging : description : \"Skip tag creation entirely\" required : false type : boolean default : false trigger_release : description : \"Trigger release workflow after version bump\" required : false type : boolean default : false The version bump command will be updated to: # Use --no-git-tag to handle tagging manually and --all to update all packages cargo workspaces version ${{ github.event.inputs.bump_type }} --exact --yes --no-git-tag --all And we'll add a step to update explicit version requirements: # Also update the explicit version requirements in the main Cargo.toml # This is needed because cargo-workspaces doesn't update these sed -i \"s/floxide-core = { path = \\\"crates\\/floxide-core\\\", version = \\\"[0-9.]*\\\"/floxide-core = { path = \\\"crates\\/floxide-core\\\", version = \\\"$NEW_VERSION\\\"/g\" Cargo.toml sed -i \"s/floxide-transform = { path = \\\"crates\\/floxide-transform\\\", version = \\\"[0-9.]*\\\"/floxide-transform = { path = \\\"crates\\/floxide-transform\\\", version = \\\"$NEW_VERSION\\\"/g\" Cargo.toml # ... (similar lines for other crates) # Commit the changes to the explicit version requirements git add Cargo.toml git commit --amend --no-edit git push --force-with-lease origin HEAD The tag creation step will be updated to: # Create and push tag if not skipped if [ \"${{ github.event.inputs.skip_tagging }}\" != \"true\" ]; then TAG_NAME=\"v${NEW_VERSION}\" # Check if tag exists if git show-ref --tags \"$TAG_NAME\" --quiet; then if [ \"${{ github.event.inputs.force_tag }}\" = \"true\" ]; then echo \"Tag $TAG_NAME exists, force updating\" git tag -d \"$TAG_NAME\" git push origin \":refs/tags/$TAG_NAME\" || true # Also delete individual crate tags if they exist for CRATE_TAG in $(git tag | grep \"@${NEW_VERSION}$\"); do echo \"Deleting individual crate tag : $CRATE_TAG\" git tag -d \"$CRATE_TAG\" git push origin \":refs/tags/$CRATE_TAG\" || true done # Create new tag git tag -a \"$TAG_NAME\" -m \"Release $TAG_NAME\" git push origin \"$TAG_NAME\" echo \"Tags updated successfully\" else echo \"Tag $TAG_NAME already exists. Use force_tag option to update it.\" exit 0 fi else git tag -a \"$TAG_NAME\" -m \"Release $TAG_NAME\" git push origin \"$TAG_NAME\" fi # If trigger_release is true, wait a moment for tag to be processed # and then check if the tag was successfully pushed if [ \"${{ github.event.inputs.trigger_release }}\" = \"true\" ]; then echo \"Waiting for tag to be processed before triggering release workflow...\" sleep 10 # Check if the tag was successfully pushed if git ls-remote --tags origin | grep -q \"$TAG_NAME\"; then echo \"Tag $TAG_NAME successfully pushed, release workflow should be triggered automatically.\" else echo \"Warning : Tag $TAG_NAME not found on remote. Release workflow may not trigger.\" fi fi else echo \"Skipping tag creation as requested\" fi","title":"Updated Workflow"},{"location":"adrs/0028-version-bump-workflow-improvements/#consequences","text":"","title":"Consequences"},{"location":"adrs/0028-version-bump-workflow-improvements/#positive","text":"Improved Reliability : The workflow will handle existing tags gracefully, reducing CI failures. More Flexibility : Users will have more control over tag creation and updates. Better Error Messages : Clearer feedback when tag-related issues occur. Safer Operations : Prevents accidental tag overwrites without explicit permission. Complete Release Process : Ensures the entire release pipeline from version bump to crates.io publishing works correctly. Comprehensive Tag Management : Handles both global and individual crate tags properly. Consistent Versioning : Ensures all crates in the workspace are updated together, preventing dependency version mismatches. Dependency Coherence : Updates explicit version requirements in the main Cargo.toml to match the new workspace version.","title":"Positive"},{"location":"adrs/0028-version-bump-workflow-improvements/#negative","text":"Increased Complexity : The workflow becomes slightly more complex with additional options. Potential for Confusion : More options may require better documentation for users. Less Granular Control : Using --all means individual crates cannot be versioned independently, which may be limiting in some scenarios. Brittle Version Updating : The sed commands for updating explicit version requirements are somewhat brittle and may need maintenance if the Cargo.toml structure changes.","title":"Negative"},{"location":"adrs/0028-version-bump-workflow-improvements/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0028-version-bump-workflow-improvements/#manually-deleting-tags-before-running-the-workflow","text":"Pros : Simple approach No workflow changes needed Cons : Manual intervention required Error-prone Not scalable We rejected this approach as it doesn't provide a sustainable, automated solution.","title":"Manually Deleting Tags Before Running the Workflow"},{"location":"adrs/0028-version-bump-workflow-improvements/#using-different-tag-naming-conventions","text":"Pros : Could avoid conflicts by using unique names Maintains history differently Cons : Breaks existing conventions May confuse users Requires migration We chose to maintain the existing tag naming convention for consistency and compatibility.","title":"Using Different Tag Naming Conventions"},{"location":"adrs/0028-version-bump-workflow-improvements/#disabling-tagging-in-cargo-workspaces","text":"Pros : Simpler workflow More direct control Cons : Loses some automation benefits Requires manual tag management We chose a hybrid approach that leverages cargo-workspaces for version management but gives us more control over tag creation.","title":"Disabling Tagging in cargo-workspaces"},{"location":"adrs/0028-version-bump-workflow-improvements/#separate-workflows-for-version-bump-and-release","text":"Pros : Clearer separation of concerns Potentially simpler workflows Cons : More manual steps Potential for inconsistency between version and release We chose to enhance the existing workflow structure to maintain the automated pipeline while adding more control options.","title":"Separate Workflows for Version Bump and Release"},{"location":"adrs/0028-version-bump-workflow-improvements/#independent-versioning-of-crates","text":"Pros : More granular control over individual crate versions Allows for different release cycles per crate Cons : Requires more complex dependency management Prone to version mismatch errors More difficult to maintain We chose to use the --all flag to ensure consistent versioning across all crates, which simplifies dependency management and reduces the chance of errors.","title":"Independent Versioning of Crates"},{"location":"adrs/0028-version-bump-workflow-improvements/#using-cargo-workspace-inheritance-without-explicit-versions","text":"Pros : Simpler dependency declarations Automatic version synchronization Cons : Less explicit about version requirements May not work well with external consumers of the crates We chose to maintain explicit version requirements in the main Cargo.toml for clarity and compatibility with external consumers, while adding automation to keep these versions in sync.","title":"Using Cargo Workspace Inheritance Without Explicit Versions"},{"location":"adrs/0029-feature-based-crate-organization/","text":"ADR-0029: Feature-Based Crate Organization \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 Currently, the Floxide framework is organized as a workspace with multiple subcrates, each providing specific functionality: floxide-core : Core abstractions and functionality floxide-transform : Transform node implementations floxide-event : Event-driven workflow functionality floxide-timer : Time-based workflow functionality floxide-longrunning : Long-running process functionality floxide-reactive : Reactive workflow functionality While this modular approach provides flexibility, it also introduces complexity for users who need to manage multiple dependencies. We want to maintain the ability to selectively include functionality while simplifying the user experience. Decision \u00b6 We will reorganize the crate structure to use feature flags instead of separate crates for publishing. This approach will: Publish a single crate named floxide to crates.io Use feature flags to enable/disable specific functionality Maintain the workspace structure for development Conditionally re-export modules based on enabled features Feature Structure \u00b6 The following features will be defined: core (default): Core abstractions and functionality transform : Transform node implementations event : Event-driven workflow functionality timer : Time-based workflow functionality longrunning : Long-running process functionality reactive : Reactive workflow functionality full : Enables all features Implementation Approach \u00b6 Keep the root crate name as floxide in Cargo.toml Add feature definitions that conditionally include subcrates Modify src/lib.rs to conditionally re-export modules based on enabled features Update examples to use the appropriate features Update the GitHub Actions workflow to publish the single crate Consequences \u00b6 Positive \u00b6 Simplified User Experience : Users can include a single dependency with desired features Reduced Dependency Management : No need to manage version compatibility between subcrates Flexible Inclusion : Users can include only the functionality they need Smaller Binaries : Applications only include the code they use Consistent Branding : Maintains the \"floxide\" name across all components Negative \u00b6 Migration Effort : Existing users will need to update their dependencies More Complex Conditional Compilation : The codebase will use more #[cfg(feature = \"...\")] directives Potential for Unused Code : Users might include more functionality than needed if they don't carefully select features Alternatives Considered \u00b6 Keep Current Structure with Multiple Crates \u00b6 Pros : Minimal changes required Clear separation of concerns Cons : Users still need to manage multiple dependencies More complex dependency graph Single Monolithic Crate \u00b6 Pros : Simplest user experience No conditional compilation needed Cons : No way to exclude unused functionality Larger binary sizes Less modular development Implementation Plan \u00b6 Implement feature flags in the root crate Update documentation to reflect the new structure Create a migration guide for existing users Update CI/CD pipeline for the new publishing approach","title":"ADR-0029: Feature-Based Crate Organization"},{"location":"adrs/0029-feature-based-crate-organization/#adr-0029-feature-based-crate-organization","text":"","title":"ADR-0029: Feature-Based Crate Organization"},{"location":"adrs/0029-feature-based-crate-organization/#status","text":"Accepted","title":"Status"},{"location":"adrs/0029-feature-based-crate-organization/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0029-feature-based-crate-organization/#context","text":"Currently, the Floxide framework is organized as a workspace with multiple subcrates, each providing specific functionality: floxide-core : Core abstractions and functionality floxide-transform : Transform node implementations floxide-event : Event-driven workflow functionality floxide-timer : Time-based workflow functionality floxide-longrunning : Long-running process functionality floxide-reactive : Reactive workflow functionality While this modular approach provides flexibility, it also introduces complexity for users who need to manage multiple dependencies. We want to maintain the ability to selectively include functionality while simplifying the user experience.","title":"Context"},{"location":"adrs/0029-feature-based-crate-organization/#decision","text":"We will reorganize the crate structure to use feature flags instead of separate crates for publishing. This approach will: Publish a single crate named floxide to crates.io Use feature flags to enable/disable specific functionality Maintain the workspace structure for development Conditionally re-export modules based on enabled features","title":"Decision"},{"location":"adrs/0029-feature-based-crate-organization/#feature-structure","text":"The following features will be defined: core (default): Core abstractions and functionality transform : Transform node implementations event : Event-driven workflow functionality timer : Time-based workflow functionality longrunning : Long-running process functionality reactive : Reactive workflow functionality full : Enables all features","title":"Feature Structure"},{"location":"adrs/0029-feature-based-crate-organization/#implementation-approach","text":"Keep the root crate name as floxide in Cargo.toml Add feature definitions that conditionally include subcrates Modify src/lib.rs to conditionally re-export modules based on enabled features Update examples to use the appropriate features Update the GitHub Actions workflow to publish the single crate","title":"Implementation Approach"},{"location":"adrs/0029-feature-based-crate-organization/#consequences","text":"","title":"Consequences"},{"location":"adrs/0029-feature-based-crate-organization/#positive","text":"Simplified User Experience : Users can include a single dependency with desired features Reduced Dependency Management : No need to manage version compatibility between subcrates Flexible Inclusion : Users can include only the functionality they need Smaller Binaries : Applications only include the code they use Consistent Branding : Maintains the \"floxide\" name across all components","title":"Positive"},{"location":"adrs/0029-feature-based-crate-organization/#negative","text":"Migration Effort : Existing users will need to update their dependencies More Complex Conditional Compilation : The codebase will use more #[cfg(feature = \"...\")] directives Potential for Unused Code : Users might include more functionality than needed if they don't carefully select features","title":"Negative"},{"location":"adrs/0029-feature-based-crate-organization/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0029-feature-based-crate-organization/#keep-current-structure-with-multiple-crates","text":"Pros : Minimal changes required Clear separation of concerns Cons : Users still need to manage multiple dependencies More complex dependency graph","title":"Keep Current Structure with Multiple Crates"},{"location":"adrs/0029-feature-based-crate-organization/#single-monolithic-crate","text":"Pros : Simplest user experience No conditional compilation needed Cons : No way to exclude unused functionality Larger binary sizes Less modular development","title":"Single Monolithic Crate"},{"location":"adrs/0029-feature-based-crate-organization/#implementation-plan","text":"Implement feature flags in the root crate Update documentation to reflect the new structure Create a migration guide for existing users Update CI/CD pipeline for the new publishing approach","title":"Implementation Plan"},{"location":"adrs/0030-workspace-dependency-versioning/","text":"ADR 0030: Workspace Dependency Versioning for Publishing \u00b6 Status \u00b6 Accepted Context \u00b6 When publishing Rust crates to crates.io, all dependencies must have explicit version specifications, even if they are path dependencies within a workspace. This is a requirement of the crates.io publishing process. In our workspace structure, we have a root floxide crate that depends on several subcrates ( floxide-core , floxide-transform , etc.). These dependencies were initially specified only with path attributes, which works fine for local development but causes errors during the publishing process. The error encountered was: error: all dependencies must have a version specified when publishing. dependency `floxide-core` does not specify a version Note: The published dependency will use the version from crates.io, the `path` specification will be removed from the dependency declaration. Additionally, we need to ensure that when the workspace version changes, we don't have to manually update multiple version specifications throughout the codebase, which would be error-prone and create maintenance overhead. We also encountered an issue during version bumping where some subcrates had hardcoded versions instead of using workspace inheritance, causing errors like: error: failed to select a version for the requirement `floxide-event = \"^1.0.2\"` candidate versions found which didn't match: 1.0.0 location searched: /home/runner/work/floxide/floxide/crates/floxide-event required by package `floxide v1.0.3 (/home/runner/work/floxide/floxide)` Decision \u00b6 We will implement a two-part solution to address these versioning issues: For the root crate's dependencies : Maintain explicit version specifications for all workspace dependencies in the root Cargo.toml file, in addition to the path specifications. These versions will match the workspace version defined in [workspace.package] . For subcrates : Ensure all subcrates use workspace inheritance for their versions by using version.workspace = true instead of hardcoded versions. The dependency specifications in the root crate will follow this pattern: floxide-core = { path = \"./crates/floxide-core\" , version = \"1.0.2\" , optional = true } To reduce maintenance burden, we will create two scripts: - scripts/update_dependency_versions.sh : Automatically updates the version specifications in the root Cargo.toml - scripts/update_subcrate_versions.sh : Ensures all subcrates use workspace inheritance for their versions Additionally, we will maintain a specific publishing order in our release workflow, where subcrates are published first, followed by the main crate. This ensures that all dependencies are available on crates.io when the main crate is published. Consequences \u00b6 Positive \u00b6 Enables smooth publishing to crates.io Maintains correct version relationships between published crates Preserves local development workflow using path dependencies Ensures that users installing the crate from crates.io get the correct dependency versions The update scripts reduce maintenance burden The defined publishing order ensures all dependencies are available when needed Consistent versioning across all crates in the workspace Negative \u00b6 Requires running the update scripts when the workspace version changes Introduces potential for version mismatch if the scripts are not run Adds slight complexity to the Cargo.toml file Requires a specific publishing order that must be maintained in the CI/CD pipeline Neutral \u00b6 The published crate on crates.io will only use the version specification, as the path specification is removed during publishing Implementation \u00b6 The implementation involves: Updating the root Cargo.toml file to include version specifications for all internal dependencies: floxide-core = { path = \"./crates/floxide-core\" , version = \"1.0.2\" , optional = true } floxide-transform = { path = \"./crates/floxide-transform\" , version = \"1.0.2\" , optional = true } floxide-event = { path = \"./crates/floxide-event\" , version = \"1.0.2\" , optional = true } floxide-timer = { path = \"./crates/floxide-timer\" , version = \"1.0.2\" , optional = true } floxide-longrunning = { path = \"./crates/floxide-longrunning\" , version = \"1.0.2\" , optional = true } floxide-reactive = { path = \"./crates/floxide-reactive\" , version = \"1.0.2\" , optional = true } Ensuring all subcrates use workspace inheritance for their versions: [package] name = \"floxide-event\" version . workspace = true edition . workspace = true # ... Creating a script ( scripts/update_dependency_versions.sh ) to automatically update the root crate's dependency versions: #!/bin/bash set -e # Get the current workspace version from Cargo.toml WORKSPACE_VERSION = $( grep -m 1 'version = ' Cargo.toml | cut -d '\"' -f 2 ) echo \"Updating dependency versions to match workspace version: $WORKSPACE_VERSION \" # Update all internal dependency versions in the root Cargo.toml sed -i.bak -E \"s/(floxide-[a-z]+[[:space:]]*=[[:space:]]*\\{[[:space:]]*path[[:space:]]*=[[:space:]]*\\\"[^\\\"]+\\\",[[:space:]]*version[[:space:]]*=[[:space:]]*\\\")[0-9]+\\.[0-9]+\\.[0-9]+/\\1 $WORKSPACE_VERSION /g\" Cargo.toml # Remove backup file rm Cargo.toml.bak echo \"Dependency versions updated successfully!\" # Verify the changes echo \"Verifying changes...\" grep -n \"floxide-\" Cargo.toml | grep \"version\" echo \"Done!\" Creating a script ( scripts/update_subcrate_versions.sh ) to ensure all subcrates use workspace inheritance: #!/bin/bash set -e echo \"Updating subcrates to use workspace inheritance for versions...\" # List of subcrates to check and update SUBCRATES =( \"floxide-core\" \"floxide-transform\" \"floxide-event\" \"floxide-timer\" \"floxide-longrunning\" \"floxide-reactive\" ) for subcrate in \" ${ SUBCRATES [@] } \" ; do CARGO_FILE = \"crates/ $subcrate /Cargo.toml\" echo \"Checking $CARGO_FILE ...\" # Check if the subcrate is using workspace inheritance for version if grep -q \"version.workspace = true\" \" $CARGO_FILE \" ; then echo \" \u2705 $subcrate is already using workspace inheritance for version\" else echo \" \u26a0\ufe0f $subcrate is not using workspace inheritance for version. Updating...\" # Get the current version line VERSION_LINE = $( grep -m 1 \"^version = \" \" $CARGO_FILE \" || echo \"\" ) if [ -n \" $VERSION_LINE \" ] ; then # Replace the version line with workspace inheritance sed -i.bak \"s/^version = .*/version.workspace = true/\" \" $CARGO_FILE \" rm \" $CARGO_FILE .bak\" echo \" \u2705 Updated $subcrate to use workspace inheritance for version\" else echo \" \u274c Could not find version line in $subcrate \" fi fi done echo \"Done updating subcrates!\" Maintaining a specific publishing order in our release workflow: First publish floxide-core Then publish floxide-transform Then publish floxide-event Then publish floxide-timer Then publish floxide-longrunning Then publish floxide-reactive Finally publish the root floxide crate Integrating both update scripts into our release process to ensure versions are always in sync. Future Improvements \u00b6 For future consideration: Adding a CI check to ensure that all version specifications match the workspace version Investigating if Cargo workspace inheritance can be leveraged for this use case in future Rust/Cargo versions Exploring other approaches to simplify dependency management in workspaces Automating the publishing process further to reduce manual steps Adding pre-commit hooks to run the version update scripts automatically Related ADRs \u00b6 ADR 0014: Crate Publishing and CI/CD ADR 0029: Feature-Based Crate Organization Additional Considerations \u00b6 Consistent Version Formats \u00b6 When specifying versions for internal dependencies in the root crate, it's important to maintain a consistent format. We encountered issues with mixed version formats: # Inconsistent version formats floxide-core = { path = \"./crates/floxide-core\" , version = \"1.0.2\" , optional = true } # No equals sign floxide-event = { path = \"./crates/floxide-event\" , version = \"=1.0.3\" , optional = true } # With equals sign The update_dependency_versions.sh script has been enhanced to handle both formats: - Version specifications with no equals sign: version = \"1.0.3\" - Version specifications with equals sign: version = \"=1.0.3\" Additionally, the script now handles cases where the version attribute might appear before the path attribute in the dependency specification. Publishing Order Importance \u00b6 When publishing to crates.io, all dependencies must have explicit version specifications, even for local path dependencies. If a subcrate is published without all its dependencies having explicit versions, the publish will fail with an error like: error: all dependencies must have a version specified when publishing. dependency `floxide-core` does not specify a version This reinforces the importance of: 1. Running the update_dependency_versions.sh script before publishing 2. Following the correct publishing order (subcrates first, then the root crate) 3. Ensuring all version numbers are consistent across the workspace","title":"ADR 0030: Workspace Dependency Versioning for Publishing"},{"location":"adrs/0030-workspace-dependency-versioning/#adr-0030-workspace-dependency-versioning-for-publishing","text":"","title":"ADR 0030: Workspace Dependency Versioning for Publishing"},{"location":"adrs/0030-workspace-dependency-versioning/#status","text":"Accepted","title":"Status"},{"location":"adrs/0030-workspace-dependency-versioning/#context","text":"When publishing Rust crates to crates.io, all dependencies must have explicit version specifications, even if they are path dependencies within a workspace. This is a requirement of the crates.io publishing process. In our workspace structure, we have a root floxide crate that depends on several subcrates ( floxide-core , floxide-transform , etc.). These dependencies were initially specified only with path attributes, which works fine for local development but causes errors during the publishing process. The error encountered was: error: all dependencies must have a version specified when publishing. dependency `floxide-core` does not specify a version Note: The published dependency will use the version from crates.io, the `path` specification will be removed from the dependency declaration. Additionally, we need to ensure that when the workspace version changes, we don't have to manually update multiple version specifications throughout the codebase, which would be error-prone and create maintenance overhead. We also encountered an issue during version bumping where some subcrates had hardcoded versions instead of using workspace inheritance, causing errors like: error: failed to select a version for the requirement `floxide-event = \"^1.0.2\"` candidate versions found which didn't match: 1.0.0 location searched: /home/runner/work/floxide/floxide/crates/floxide-event required by package `floxide v1.0.3 (/home/runner/work/floxide/floxide)`","title":"Context"},{"location":"adrs/0030-workspace-dependency-versioning/#decision","text":"We will implement a two-part solution to address these versioning issues: For the root crate's dependencies : Maintain explicit version specifications for all workspace dependencies in the root Cargo.toml file, in addition to the path specifications. These versions will match the workspace version defined in [workspace.package] . For subcrates : Ensure all subcrates use workspace inheritance for their versions by using version.workspace = true instead of hardcoded versions. The dependency specifications in the root crate will follow this pattern: floxide-core = { path = \"./crates/floxide-core\" , version = \"1.0.2\" , optional = true } To reduce maintenance burden, we will create two scripts: - scripts/update_dependency_versions.sh : Automatically updates the version specifications in the root Cargo.toml - scripts/update_subcrate_versions.sh : Ensures all subcrates use workspace inheritance for their versions Additionally, we will maintain a specific publishing order in our release workflow, where subcrates are published first, followed by the main crate. This ensures that all dependencies are available on crates.io when the main crate is published.","title":"Decision"},{"location":"adrs/0030-workspace-dependency-versioning/#consequences","text":"","title":"Consequences"},{"location":"adrs/0030-workspace-dependency-versioning/#positive","text":"Enables smooth publishing to crates.io Maintains correct version relationships between published crates Preserves local development workflow using path dependencies Ensures that users installing the crate from crates.io get the correct dependency versions The update scripts reduce maintenance burden The defined publishing order ensures all dependencies are available when needed Consistent versioning across all crates in the workspace","title":"Positive"},{"location":"adrs/0030-workspace-dependency-versioning/#negative","text":"Requires running the update scripts when the workspace version changes Introduces potential for version mismatch if the scripts are not run Adds slight complexity to the Cargo.toml file Requires a specific publishing order that must be maintained in the CI/CD pipeline","title":"Negative"},{"location":"adrs/0030-workspace-dependency-versioning/#neutral","text":"The published crate on crates.io will only use the version specification, as the path specification is removed during publishing","title":"Neutral"},{"location":"adrs/0030-workspace-dependency-versioning/#implementation","text":"The implementation involves: Updating the root Cargo.toml file to include version specifications for all internal dependencies: floxide-core = { path = \"./crates/floxide-core\" , version = \"1.0.2\" , optional = true } floxide-transform = { path = \"./crates/floxide-transform\" , version = \"1.0.2\" , optional = true } floxide-event = { path = \"./crates/floxide-event\" , version = \"1.0.2\" , optional = true } floxide-timer = { path = \"./crates/floxide-timer\" , version = \"1.0.2\" , optional = true } floxide-longrunning = { path = \"./crates/floxide-longrunning\" , version = \"1.0.2\" , optional = true } floxide-reactive = { path = \"./crates/floxide-reactive\" , version = \"1.0.2\" , optional = true } Ensuring all subcrates use workspace inheritance for their versions: [package] name = \"floxide-event\" version . workspace = true edition . workspace = true # ... Creating a script ( scripts/update_dependency_versions.sh ) to automatically update the root crate's dependency versions: #!/bin/bash set -e # Get the current workspace version from Cargo.toml WORKSPACE_VERSION = $( grep -m 1 'version = ' Cargo.toml | cut -d '\"' -f 2 ) echo \"Updating dependency versions to match workspace version: $WORKSPACE_VERSION \" # Update all internal dependency versions in the root Cargo.toml sed -i.bak -E \"s/(floxide-[a-z]+[[:space:]]*=[[:space:]]*\\{[[:space:]]*path[[:space:]]*=[[:space:]]*\\\"[^\\\"]+\\\",[[:space:]]*version[[:space:]]*=[[:space:]]*\\\")[0-9]+\\.[0-9]+\\.[0-9]+/\\1 $WORKSPACE_VERSION /g\" Cargo.toml # Remove backup file rm Cargo.toml.bak echo \"Dependency versions updated successfully!\" # Verify the changes echo \"Verifying changes...\" grep -n \"floxide-\" Cargo.toml | grep \"version\" echo \"Done!\" Creating a script ( scripts/update_subcrate_versions.sh ) to ensure all subcrates use workspace inheritance: #!/bin/bash set -e echo \"Updating subcrates to use workspace inheritance for versions...\" # List of subcrates to check and update SUBCRATES =( \"floxide-core\" \"floxide-transform\" \"floxide-event\" \"floxide-timer\" \"floxide-longrunning\" \"floxide-reactive\" ) for subcrate in \" ${ SUBCRATES [@] } \" ; do CARGO_FILE = \"crates/ $subcrate /Cargo.toml\" echo \"Checking $CARGO_FILE ...\" # Check if the subcrate is using workspace inheritance for version if grep -q \"version.workspace = true\" \" $CARGO_FILE \" ; then echo \" \u2705 $subcrate is already using workspace inheritance for version\" else echo \" \u26a0\ufe0f $subcrate is not using workspace inheritance for version. Updating...\" # Get the current version line VERSION_LINE = $( grep -m 1 \"^version = \" \" $CARGO_FILE \" || echo \"\" ) if [ -n \" $VERSION_LINE \" ] ; then # Replace the version line with workspace inheritance sed -i.bak \"s/^version = .*/version.workspace = true/\" \" $CARGO_FILE \" rm \" $CARGO_FILE .bak\" echo \" \u2705 Updated $subcrate to use workspace inheritance for version\" else echo \" \u274c Could not find version line in $subcrate \" fi fi done echo \"Done updating subcrates!\" Maintaining a specific publishing order in our release workflow: First publish floxide-core Then publish floxide-transform Then publish floxide-event Then publish floxide-timer Then publish floxide-longrunning Then publish floxide-reactive Finally publish the root floxide crate Integrating both update scripts into our release process to ensure versions are always in sync.","title":"Implementation"},{"location":"adrs/0030-workspace-dependency-versioning/#future-improvements","text":"For future consideration: Adding a CI check to ensure that all version specifications match the workspace version Investigating if Cargo workspace inheritance can be leveraged for this use case in future Rust/Cargo versions Exploring other approaches to simplify dependency management in workspaces Automating the publishing process further to reduce manual steps Adding pre-commit hooks to run the version update scripts automatically","title":"Future Improvements"},{"location":"adrs/0030-workspace-dependency-versioning/#related-adrs","text":"ADR 0014: Crate Publishing and CI/CD ADR 0029: Feature-Based Crate Organization","title":"Related ADRs"},{"location":"adrs/0030-workspace-dependency-versioning/#additional-considerations","text":"","title":"Additional Considerations"},{"location":"adrs/0030-workspace-dependency-versioning/#consistent-version-formats","text":"When specifying versions for internal dependencies in the root crate, it's important to maintain a consistent format. We encountered issues with mixed version formats: # Inconsistent version formats floxide-core = { path = \"./crates/floxide-core\" , version = \"1.0.2\" , optional = true } # No equals sign floxide-event = { path = \"./crates/floxide-event\" , version = \"=1.0.3\" , optional = true } # With equals sign The update_dependency_versions.sh script has been enhanced to handle both formats: - Version specifications with no equals sign: version = \"1.0.3\" - Version specifications with equals sign: version = \"=1.0.3\" Additionally, the script now handles cases where the version attribute might appear before the path attribute in the dependency specification.","title":"Consistent Version Formats"},{"location":"adrs/0030-workspace-dependency-versioning/#publishing-order-importance","text":"When publishing to crates.io, all dependencies must have explicit version specifications, even for local path dependencies. If a subcrate is published without all its dependencies having explicit versions, the publish will fail with an error like: error: all dependencies must have a version specified when publishing. dependency `floxide-core` does not specify a version This reinforces the importance of: 1. Running the update_dependency_versions.sh script before publishing 2. Following the correct publishing order (subcrates first, then the root crate) 3. Ensuring all version numbers are consistent across the workspace","title":"Publishing Order Importance"},{"location":"adrs/0031-github-workflow-authentication/","text":"ADR 0031: GitHub Workflow Authentication for Release Process \u00b6 Date \u00b6 2023-11-15 2025-02-27 Status \u00b6 Accepted Context \u00b6 The Flow Framework release process requires authentication for two primary operations: 1. Publishing packages to crates.io 2. Creating GitHub releases and interacting with the repository These operations require different types of authentication tokens with specific permissions. Without proper authentication, the automated release process would fail, requiring manual intervention. Decision \u00b6 We have decided to implement a two-token authentication strategy for our GitHub Actions workflow: CRATES_IO_TOKEN : Used for authenticating with crates.io to publish packages This token is specific to crates.io and has permissions to publish packages under the account that generated it It is used with the --token flag in all cargo publish commands GITHUB_TOKEN : Used for GitHub repository operations that don't need to trigger other workflows This is automatically provided by GitHub Actions It has permissions defined in the workflow file It's used for creating GitHub releases WORKFLOW_PAT (only when needed): Used for GitHub repository operations that need to trigger other workflows This is a GitHub Personal Access Token (PAT) with appropriate repository permissions It's used for checkout operations when we need to push changes that should trigger other workflows The default GITHUB_TOKEN cannot trigger workflow runs when pushing commits/tags Both tokens are stored as GitHub repository secrets and referenced in the workflow file using the ${{ secrets.TOKEN_NAME }} syntax. The GITHUB_TOKEN is automatically available in all workflows. Implementation \u00b6 The tokens are used in the following ways in our .github/workflows/release.yml file: CRATES_IO_TOKEN : Used in all cargo publish commands for each subcrate and the root crate Example: cargo publish --token ${{ secrets.CRATES_IO_TOKEN }} WORKFLOW_PAT : Used in the checkout step when we need to push changes that should trigger workflows: - uses : actions/checkout@v3 with : ref : ${{ steps.determine_ref.outputs.TAG_REF }} fetch-depth : 0 token : ${{ secrets.WORKFLOW_PAT }} GITHUB_TOKEN : Used in the GitHub Release creation step: - name : Create GitHub Release uses : softprops/action-gh-release@v1 with : tag_name : ${{ steps.determine_ref.outputs.TAG_REF }} name : Release ${{ steps.determine_ref.outputs.TAG_REF }} draft : false prerelease : false generate_release_notes : true env : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }} Token Setup Instructions \u00b6 CRATES_IO_TOKEN : Log in to crates.io Go to Account Settings Generate a new API token Add it as a repository secret in GitHub WORKFLOW_PAT (only needed for operations that must trigger other workflows): Go to GitHub Settings > Developer settings > Personal access tokens Generate a new token with the following permissions: repo (Full control of private repositories) workflow (Update GitHub Action workflows) Add it as a repository secret in GitHub GITHUB_TOKEN : No setup required - automatically provided by GitHub Actions Permissions can be configured in the workflow file When to Use Each Token \u00b6 GITHUB_TOKEN : Use for most GitHub operations (creating releases, commenting on issues, etc.) Has limited permissions by default for security Cannot trigger new workflow runs when pushing commits/tags WORKFLOW_PAT : Use only when you need to push commits/tags that should trigger other workflows Has broader permissions, so use with caution Requires manual creation and rotation CRATES_IO_TOKEN : Use only for publishing to crates.io Specific to the crates.io registry Consequences \u00b6 Positive \u00b6 Automated release process with proper authentication Separation of concerns between crates.io publishing and GitHub operations Secure handling of tokens through GitHub secrets Using GITHUB_TOKEN where possible for better security Negative \u00b6 Tokens need to be rotated periodically for security Multiple tokens to manage PATs are tied to individual GitHub accounts, which may cause issues if the token creator leaves the project Need to understand the limitations of GITHUB_TOKEN vs. WORKFLOW_PAT Future Considerations \u00b6 Consider using GitHub's OIDC provider for more secure, short-lived tokens Implement token rotation reminders or automation Document token permissions and access in a central location Consider using GitHub Apps instead of PATs for better security and management Evaluate if we can restructure workflows to minimize the need for WORKFLOW_PAT","title":"ADR 0031: GitHub Workflow Authentication for Release Process"},{"location":"adrs/0031-github-workflow-authentication/#adr-0031-github-workflow-authentication-for-release-process","text":"","title":"ADR 0031: GitHub Workflow Authentication for Release Process"},{"location":"adrs/0031-github-workflow-authentication/#date","text":"2023-11-15 2025-02-27","title":"Date"},{"location":"adrs/0031-github-workflow-authentication/#status","text":"Accepted","title":"Status"},{"location":"adrs/0031-github-workflow-authentication/#context","text":"The Flow Framework release process requires authentication for two primary operations: 1. Publishing packages to crates.io 2. Creating GitHub releases and interacting with the repository These operations require different types of authentication tokens with specific permissions. Without proper authentication, the automated release process would fail, requiring manual intervention.","title":"Context"},{"location":"adrs/0031-github-workflow-authentication/#decision","text":"We have decided to implement a two-token authentication strategy for our GitHub Actions workflow: CRATES_IO_TOKEN : Used for authenticating with crates.io to publish packages This token is specific to crates.io and has permissions to publish packages under the account that generated it It is used with the --token flag in all cargo publish commands GITHUB_TOKEN : Used for GitHub repository operations that don't need to trigger other workflows This is automatically provided by GitHub Actions It has permissions defined in the workflow file It's used for creating GitHub releases WORKFLOW_PAT (only when needed): Used for GitHub repository operations that need to trigger other workflows This is a GitHub Personal Access Token (PAT) with appropriate repository permissions It's used for checkout operations when we need to push changes that should trigger other workflows The default GITHUB_TOKEN cannot trigger workflow runs when pushing commits/tags Both tokens are stored as GitHub repository secrets and referenced in the workflow file using the ${{ secrets.TOKEN_NAME }} syntax. The GITHUB_TOKEN is automatically available in all workflows.","title":"Decision"},{"location":"adrs/0031-github-workflow-authentication/#implementation","text":"The tokens are used in the following ways in our .github/workflows/release.yml file: CRATES_IO_TOKEN : Used in all cargo publish commands for each subcrate and the root crate Example: cargo publish --token ${{ secrets.CRATES_IO_TOKEN }} WORKFLOW_PAT : Used in the checkout step when we need to push changes that should trigger workflows: - uses : actions/checkout@v3 with : ref : ${{ steps.determine_ref.outputs.TAG_REF }} fetch-depth : 0 token : ${{ secrets.WORKFLOW_PAT }} GITHUB_TOKEN : Used in the GitHub Release creation step: - name : Create GitHub Release uses : softprops/action-gh-release@v1 with : tag_name : ${{ steps.determine_ref.outputs.TAG_REF }} name : Release ${{ steps.determine_ref.outputs.TAG_REF }} draft : false prerelease : false generate_release_notes : true env : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }}","title":"Implementation"},{"location":"adrs/0031-github-workflow-authentication/#token-setup-instructions","text":"CRATES_IO_TOKEN : Log in to crates.io Go to Account Settings Generate a new API token Add it as a repository secret in GitHub WORKFLOW_PAT (only needed for operations that must trigger other workflows): Go to GitHub Settings > Developer settings > Personal access tokens Generate a new token with the following permissions: repo (Full control of private repositories) workflow (Update GitHub Action workflows) Add it as a repository secret in GitHub GITHUB_TOKEN : No setup required - automatically provided by GitHub Actions Permissions can be configured in the workflow file","title":"Token Setup Instructions"},{"location":"adrs/0031-github-workflow-authentication/#when-to-use-each-token","text":"GITHUB_TOKEN : Use for most GitHub operations (creating releases, commenting on issues, etc.) Has limited permissions by default for security Cannot trigger new workflow runs when pushing commits/tags WORKFLOW_PAT : Use only when you need to push commits/tags that should trigger other workflows Has broader permissions, so use with caution Requires manual creation and rotation CRATES_IO_TOKEN : Use only for publishing to crates.io Specific to the crates.io registry","title":"When to Use Each Token"},{"location":"adrs/0031-github-workflow-authentication/#consequences","text":"","title":"Consequences"},{"location":"adrs/0031-github-workflow-authentication/#positive","text":"Automated release process with proper authentication Separation of concerns between crates.io publishing and GitHub operations Secure handling of tokens through GitHub secrets Using GITHUB_TOKEN where possible for better security","title":"Positive"},{"location":"adrs/0031-github-workflow-authentication/#negative","text":"Tokens need to be rotated periodically for security Multiple tokens to manage PATs are tied to individual GitHub accounts, which may cause issues if the token creator leaves the project Need to understand the limitations of GITHUB_TOKEN vs. WORKFLOW_PAT","title":"Negative"},{"location":"adrs/0031-github-workflow-authentication/#future-considerations","text":"Consider using GitHub's OIDC provider for more secure, short-lived tokens Implement token rotation reminders or automation Document token permissions and access in a central location Consider using GitHub Apps instead of PATs for better security and management Evaluate if we can restructure workflows to minimize the need for WORKFLOW_PAT","title":"Future Considerations"},{"location":"adrs/0032-project-rebranding-to-floxide/","text":"ADR 0032: Project Rebranding to Floxide \u00b6 Status \u00b6 Accepted Context \u00b6 The project was initially named \"floxide\" which served as a descriptive name combining \"flow\" (for workflow) and \"rs\" (for Rust). However, as the project evolved, we identified the need for a more distinctive and engaging name that would: Be more memorable and unique in the Rust ecosystem Better reflect the project's identity as a Rust-based workflow framework Be available as a crate name on crates.io Provide a stronger brand identity Decision \u00b6 We have decided to rebrand the project from \"floxide\" to \"floxide\". The name \"floxide\" is a portmanteau of: \"flow\" - representing the core workflow functionality \"oxide\" - a play on Rust (as iron oxide) and a common suffix for Rust projects This rebranding affects: The main crate name (from floxide to floxide ) All subcrate names (from floxide-* to floxide-* ) Error types (from FloxideError to FloxideError ) Result types (from FloxideResult to FloxideResult ) Repository URLs and documentation references All mentions in documentation and code comments Consequences \u00b6 Positive \u00b6 The new name is more distinctive and memorable \"floxide\" has a scientific feel that subtly references Rust's systems programming nature The name is available on crates.io The portmanteau is elegant and flows naturally when spoken The new name maintains the connection to workflow functionality while adding Rust-specific identity Negative \u00b6 Existing users will need to update their dependencies Documentation and references to the old name will need to be updated Some search engine results and external links may become outdated Neutral \u00b6 The core functionality and API design remain unchanged The rebranding is purely cosmetic and does not affect the framework's architecture Implementation \u00b6 The rebranding was implemented through a systematic approach: Renamed all crate directories from floxide-* to floxide-* Updated all Cargo.toml files to reference the new crate names Used search and replace to update all code references Updated documentation and examples Ensured backward compatibility notes are added to the README References \u00b6 Crates.io naming conventions Previous ADR on crate organization (ADR-0002) Previous ADR on crate publishing (ADR-0014)","title":"ADR 0032: Project Rebranding to Floxide"},{"location":"adrs/0032-project-rebranding-to-floxide/#adr-0032-project-rebranding-to-floxide","text":"","title":"ADR 0032: Project Rebranding to Floxide"},{"location":"adrs/0032-project-rebranding-to-floxide/#status","text":"Accepted","title":"Status"},{"location":"adrs/0032-project-rebranding-to-floxide/#context","text":"The project was initially named \"floxide\" which served as a descriptive name combining \"flow\" (for workflow) and \"rs\" (for Rust). However, as the project evolved, we identified the need for a more distinctive and engaging name that would: Be more memorable and unique in the Rust ecosystem Better reflect the project's identity as a Rust-based workflow framework Be available as a crate name on crates.io Provide a stronger brand identity","title":"Context"},{"location":"adrs/0032-project-rebranding-to-floxide/#decision","text":"We have decided to rebrand the project from \"floxide\" to \"floxide\". The name \"floxide\" is a portmanteau of: \"flow\" - representing the core workflow functionality \"oxide\" - a play on Rust (as iron oxide) and a common suffix for Rust projects This rebranding affects: The main crate name (from floxide to floxide ) All subcrate names (from floxide-* to floxide-* ) Error types (from FloxideError to FloxideError ) Result types (from FloxideResult to FloxideResult ) Repository URLs and documentation references All mentions in documentation and code comments","title":"Decision"},{"location":"adrs/0032-project-rebranding-to-floxide/#consequences","text":"","title":"Consequences"},{"location":"adrs/0032-project-rebranding-to-floxide/#positive","text":"The new name is more distinctive and memorable \"floxide\" has a scientific feel that subtly references Rust's systems programming nature The name is available on crates.io The portmanteau is elegant and flows naturally when spoken The new name maintains the connection to workflow functionality while adding Rust-specific identity","title":"Positive"},{"location":"adrs/0032-project-rebranding-to-floxide/#negative","text":"Existing users will need to update their dependencies Documentation and references to the old name will need to be updated Some search engine results and external links may become outdated","title":"Negative"},{"location":"adrs/0032-project-rebranding-to-floxide/#neutral","text":"The core functionality and API design remain unchanged The rebranding is purely cosmetic and does not affect the framework's architecture","title":"Neutral"},{"location":"adrs/0032-project-rebranding-to-floxide/#implementation","text":"The rebranding was implemented through a systematic approach: Renamed all crate directories from floxide-* to floxide-* Updated all Cargo.toml files to reference the new crate names Used search and replace to update all code references Updated documentation and examples Ensured backward compatibility notes are added to the README","title":"Implementation"},{"location":"adrs/0032-project-rebranding-to-floxide/#references","text":"Crates.io naming conventions Previous ADR on crate organization (ADR-0002) Previous ADR on crate publishing (ADR-0014)","title":"References"},{"location":"adrs/0033-implementing-single-package-with-features/","text":"ADR-0033: Simplified Publishing with Maintained Subcrate Structure \u00b6 Status \u00b6 Accepted (Updated) Date \u00b6 2025-02-27 Context \u00b6 In ADR-0029, we decided to transition to a feature-based crate organization to simplify the user experience. The current structure uses multiple subcrates: - floxide-core : Core abstractions and functionality - floxide-transform : Transform node implementations - floxide-event : Event-driven workflow functionality - floxide-timer : Time-based workflow functionality - floxide-longrunning : Long-running process functionality - floxide-reactive : Reactive workflow functionality This modular approach provides clear separation of concerns during development. However, it creates complexity in the publishing process, requiring subcrates to be published in a specific order before the root crate. We want to maintain the benefits of modular development with subcrates while simplifying the publishing process. Decision \u00b6 We will implement a standard Rust workspace approach that: Maintains the existing subcrate structure for development Publishes subcrates in the correct order before the root crate Uses feature flags in the root crate to conditionally include functionality Implementation Strategy \u00b6 Keep the existing workspace structure with subcrates for development Update the root Cargo.toml to include all subcrates as optional dependencies with feature flags Modify the release process to publish subcrates in the correct order before the root crate Update the GitHub Actions workflow to automate this publishing sequence Feature Structure \u00b6 We will maintain the existing feature structure in the root crate: [features] default = [ \"core\" ] core = [ \"floxide-core\" ] transform = [ \"core\" , \"floxide-transform\" ] event = [ \"core\" , \"floxide-event\" ] timer = [ \"core\" , \"floxide-timer\" ] longrunning = [ \"core\" , \"floxide-longrunning\" ] reactive = [ \"core\" , \"floxide-reactive\" ] full = [ \"transform\" , \"event\" , \"timer\" , \"longrunning\" , \"reactive\" ] Consequences \u00b6 Advantages \u00b6 Standard Rust Approach : Following established patterns in the Rust ecosystem Maintained Development Structure : Keep the benefits of modular development Improved User Experience : Users only need to include one dependency with appropriate features Simplified Dependency Management : Clear feature-based organization Consistent Versioning : Coordinated version numbers across all crates Disadvantages \u00b6 Publishing Order Dependency : Subcrates must be published before the root crate Release Process Complexity : Need to ensure correct publishing order Potential for Version Mismatches : If subcrates are published with different versions Implementation Plan \u00b6 Update the root Cargo.toml to include all subcrates as optional dependencies with feature flags Update the lib.rs file to properly re-export the subcrates Use cargo-workspaces for version management and publishing to ensure consistent versioning across all crates Create a release script that leverages cargo-workspaces for the publishing process Update the GitHub Actions workflow to use the new release process Alternatives Considered \u00b6 Manual Publishing of Individual Crates \u00b6 Pros : Fine-grained control over each crate's publication Ability to publish only specific crates Cons : Complex ordering requirements Error-prone manual process Difficult to maintain version consistency We rejected this approach in favor of using cargo-workspaces for a more automated and consistent process. Complex Publishing Scripts Approach \u00b6 Pros : Single publishing step No need to publish subcrates separately Cons : Complex and error-prone scripts Non-standard approach Difficult to maintain We rejected this approach in favor of a more standard Rust workspace approach. Complete Transition to Single Crate \u00b6 Pros : Simplest structure overall No distinction between development and published code Cons : Loss of modular development benefits Significant refactoring required Potential for a more complex codebase We rejected this approach to maintain the benefits of modular development. Related ADRs \u00b6 ADR-0029: Feature-Based Crate Organization ADR-0030: Workspace Dependency Versioning ADR-0014: Crate Publishing and CI/CD","title":"ADR-0033: Simplified Publishing with Maintained Subcrate Structure"},{"location":"adrs/0033-implementing-single-package-with-features/#adr-0033-simplified-publishing-with-maintained-subcrate-structure","text":"","title":"ADR-0033: Simplified Publishing with Maintained Subcrate Structure"},{"location":"adrs/0033-implementing-single-package-with-features/#status","text":"Accepted (Updated)","title":"Status"},{"location":"adrs/0033-implementing-single-package-with-features/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0033-implementing-single-package-with-features/#context","text":"In ADR-0029, we decided to transition to a feature-based crate organization to simplify the user experience. The current structure uses multiple subcrates: - floxide-core : Core abstractions and functionality - floxide-transform : Transform node implementations - floxide-event : Event-driven workflow functionality - floxide-timer : Time-based workflow functionality - floxide-longrunning : Long-running process functionality - floxide-reactive : Reactive workflow functionality This modular approach provides clear separation of concerns during development. However, it creates complexity in the publishing process, requiring subcrates to be published in a specific order before the root crate. We want to maintain the benefits of modular development with subcrates while simplifying the publishing process.","title":"Context"},{"location":"adrs/0033-implementing-single-package-with-features/#decision","text":"We will implement a standard Rust workspace approach that: Maintains the existing subcrate structure for development Publishes subcrates in the correct order before the root crate Uses feature flags in the root crate to conditionally include functionality","title":"Decision"},{"location":"adrs/0033-implementing-single-package-with-features/#implementation-strategy","text":"Keep the existing workspace structure with subcrates for development Update the root Cargo.toml to include all subcrates as optional dependencies with feature flags Modify the release process to publish subcrates in the correct order before the root crate Update the GitHub Actions workflow to automate this publishing sequence","title":"Implementation Strategy"},{"location":"adrs/0033-implementing-single-package-with-features/#feature-structure","text":"We will maintain the existing feature structure in the root crate: [features] default = [ \"core\" ] core = [ \"floxide-core\" ] transform = [ \"core\" , \"floxide-transform\" ] event = [ \"core\" , \"floxide-event\" ] timer = [ \"core\" , \"floxide-timer\" ] longrunning = [ \"core\" , \"floxide-longrunning\" ] reactive = [ \"core\" , \"floxide-reactive\" ] full = [ \"transform\" , \"event\" , \"timer\" , \"longrunning\" , \"reactive\" ]","title":"Feature Structure"},{"location":"adrs/0033-implementing-single-package-with-features/#consequences","text":"","title":"Consequences"},{"location":"adrs/0033-implementing-single-package-with-features/#advantages","text":"Standard Rust Approach : Following established patterns in the Rust ecosystem Maintained Development Structure : Keep the benefits of modular development Improved User Experience : Users only need to include one dependency with appropriate features Simplified Dependency Management : Clear feature-based organization Consistent Versioning : Coordinated version numbers across all crates","title":"Advantages"},{"location":"adrs/0033-implementing-single-package-with-features/#disadvantages","text":"Publishing Order Dependency : Subcrates must be published before the root crate Release Process Complexity : Need to ensure correct publishing order Potential for Version Mismatches : If subcrates are published with different versions","title":"Disadvantages"},{"location":"adrs/0033-implementing-single-package-with-features/#implementation-plan","text":"Update the root Cargo.toml to include all subcrates as optional dependencies with feature flags Update the lib.rs file to properly re-export the subcrates Use cargo-workspaces for version management and publishing to ensure consistent versioning across all crates Create a release script that leverages cargo-workspaces for the publishing process Update the GitHub Actions workflow to use the new release process","title":"Implementation Plan"},{"location":"adrs/0033-implementing-single-package-with-features/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0033-implementing-single-package-with-features/#manual-publishing-of-individual-crates","text":"Pros : Fine-grained control over each crate's publication Ability to publish only specific crates Cons : Complex ordering requirements Error-prone manual process Difficult to maintain version consistency We rejected this approach in favor of using cargo-workspaces for a more automated and consistent process.","title":"Manual Publishing of Individual Crates"},{"location":"adrs/0033-implementing-single-package-with-features/#complex-publishing-scripts-approach","text":"Pros : Single publishing step No need to publish subcrates separately Cons : Complex and error-prone scripts Non-standard approach Difficult to maintain We rejected this approach in favor of a more standard Rust workspace approach.","title":"Complex Publishing Scripts Approach"},{"location":"adrs/0033-implementing-single-package-with-features/#complete-transition-to-single-crate","text":"Pros : Simplest structure overall No distinction between development and published code Cons : Loss of modular development benefits Significant refactoring required Potential for a more complex codebase We rejected this approach to maintain the benefits of modular development.","title":"Complete Transition to Single Crate"},{"location":"adrs/0033-implementing-single-package-with-features/#related-adrs","text":"ADR-0029: Feature-Based Crate Organization ADR-0030: Workspace Dependency Versioning ADR-0014: Crate Publishing and CI/CD","title":"Related ADRs"},{"location":"adrs/0034-script-consolidation-for-release-process/","text":"ADR-0034: Script Consolidation for Release Process \u00b6 Status \u00b6 Proposed Date \u00b6 2025-02-27 Context \u00b6 As part of our transition to using cargo-workspaces for managing versions and publishing (as documented in ADR-0033), we need to evaluate our existing scripts and determine which ones are still necessary and which can be consolidated or removed. Currently, we have several scripts related to the release process: publish.sh - Original script for publishing crates in order publish_local.sh - Script for testing publishing locally release_with_workspaces.sh - New script using cargo-workspaces for releases test_version_bump.sh - Script for testing version bumps update_dependency_versions.sh - Script for updating dependency versions in the root crate update_subcrate_versions.sh - Script for updating subcrate versions to use workspace inheritance run_ci_locally.sh - Script for running CI checks locally serve-docs.sh - Script for serving documentation locally These scripts serve different purposes in our development and release workflows, but there may be redundancy or opportunities for consolidation. Decision \u00b6 After evaluating the scripts and their usage in our workflows, we will: Keep and maintain the following scripts : release_with_workspaces.sh - Our primary script for version management and publishing run_ci_locally.sh - Useful for developers to run CI checks locally serve-docs.sh - Useful for previewing documentation locally Consolidate the following scripts : update_dependency_versions.sh and update_subcrate_versions.sh - These can be combined into a single update_versions.sh script that handles both tasks test_version_bump.sh - This functionality can be incorporated into release_with_workspaces.sh with the --dry-run flag Deprecate the following scripts : publish.sh - Replaced by release_with_workspaces.sh publish_local.sh - Replaced by release_with_workspaces.sh with the --dry-run flag Implementation Plan \u00b6 Create a new update_versions.sh script that combines the functionality of update_dependency_versions.sh and update_subcrate_versions.sh Enhance release_with_workspaces.sh to include the functionality of test_version_bump.sh Update the GitHub Actions workflows to use the consolidated scripts Add deprecation notices to the scripts that will be removed Document the changes in the README.md file Consequences \u00b6 Advantages \u00b6 Simplified Maintenance : Fewer scripts to maintain and update Clearer Developer Experience : Developers have fewer scripts to learn and remember Consistent Versioning : Using cargo-workspaces ensures consistent versioning across all crates Streamlined Release Process : The release process is more straightforward and less error-prone Disadvantages \u00b6 Migration Period : There will be a period where developers need to adapt to the new scripts Potential for Confusion : Until the deprecated scripts are removed, there may be confusion about which scripts to use Alternatives Considered \u00b6 Keep All Scripts Separate \u00b6 Pros : Each script has a clear, focused purpose Cons : More scripts to maintain, potential for divergence in functionality Automate Everything in CI/CD \u00b6 Pros : Less reliance on scripts, more automation Cons : Less flexibility for developers, harder to debug issues locally Related ADRs \u00b6 ADR-0033: Simplified Publishing with Maintained Subcrate Structure","title":"ADR-0034: Script Consolidation for Release Process"},{"location":"adrs/0034-script-consolidation-for-release-process/#adr-0034-script-consolidation-for-release-process","text":"","title":"ADR-0034: Script Consolidation for Release Process"},{"location":"adrs/0034-script-consolidation-for-release-process/#status","text":"Proposed","title":"Status"},{"location":"adrs/0034-script-consolidation-for-release-process/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0034-script-consolidation-for-release-process/#context","text":"As part of our transition to using cargo-workspaces for managing versions and publishing (as documented in ADR-0033), we need to evaluate our existing scripts and determine which ones are still necessary and which can be consolidated or removed. Currently, we have several scripts related to the release process: publish.sh - Original script for publishing crates in order publish_local.sh - Script for testing publishing locally release_with_workspaces.sh - New script using cargo-workspaces for releases test_version_bump.sh - Script for testing version bumps update_dependency_versions.sh - Script for updating dependency versions in the root crate update_subcrate_versions.sh - Script for updating subcrate versions to use workspace inheritance run_ci_locally.sh - Script for running CI checks locally serve-docs.sh - Script for serving documentation locally These scripts serve different purposes in our development and release workflows, but there may be redundancy or opportunities for consolidation.","title":"Context"},{"location":"adrs/0034-script-consolidation-for-release-process/#decision","text":"After evaluating the scripts and their usage in our workflows, we will: Keep and maintain the following scripts : release_with_workspaces.sh - Our primary script for version management and publishing run_ci_locally.sh - Useful for developers to run CI checks locally serve-docs.sh - Useful for previewing documentation locally Consolidate the following scripts : update_dependency_versions.sh and update_subcrate_versions.sh - These can be combined into a single update_versions.sh script that handles both tasks test_version_bump.sh - This functionality can be incorporated into release_with_workspaces.sh with the --dry-run flag Deprecate the following scripts : publish.sh - Replaced by release_with_workspaces.sh publish_local.sh - Replaced by release_with_workspaces.sh with the --dry-run flag","title":"Decision"},{"location":"adrs/0034-script-consolidation-for-release-process/#implementation-plan","text":"Create a new update_versions.sh script that combines the functionality of update_dependency_versions.sh and update_subcrate_versions.sh Enhance release_with_workspaces.sh to include the functionality of test_version_bump.sh Update the GitHub Actions workflows to use the consolidated scripts Add deprecation notices to the scripts that will be removed Document the changes in the README.md file","title":"Implementation Plan"},{"location":"adrs/0034-script-consolidation-for-release-process/#consequences","text":"","title":"Consequences"},{"location":"adrs/0034-script-consolidation-for-release-process/#advantages","text":"Simplified Maintenance : Fewer scripts to maintain and update Clearer Developer Experience : Developers have fewer scripts to learn and remember Consistent Versioning : Using cargo-workspaces ensures consistent versioning across all crates Streamlined Release Process : The release process is more straightforward and less error-prone","title":"Advantages"},{"location":"adrs/0034-script-consolidation-for-release-process/#disadvantages","text":"Migration Period : There will be a period where developers need to adapt to the new scripts Potential for Confusion : Until the deprecated scripts are removed, there may be confusion about which scripts to use","title":"Disadvantages"},{"location":"adrs/0034-script-consolidation-for-release-process/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0034-script-consolidation-for-release-process/#keep-all-scripts-separate","text":"Pros : Each script has a clear, focused purpose Cons : More scripts to maintain, potential for divergence in functionality","title":"Keep All Scripts Separate"},{"location":"adrs/0034-script-consolidation-for-release-process/#automate-everything-in-cicd","text":"Pros : Less reliance on scripts, more automation Cons : Less flexibility for developers, harder to debug issues locally","title":"Automate Everything in CI/CD"},{"location":"adrs/0034-script-consolidation-for-release-process/#related-adrs","text":"ADR-0033: Simplified Publishing with Maintained Subcrate Structure","title":"Related ADRs"},{"location":"adrs/0035-combined-version-bump-and-release-workflow/","text":"ADR-0035: Combined Version Bump and Release Workflow \u00b6 Status \u00b6 Proposed Date \u00b6 2025-02-27 Context \u00b6 Currently, our release process is split across two GitHub Actions workflows: version-bump.yml : Handles version bumping, committing changes, and creating tags release.yml : Handles publishing to crates.io and creating GitHub releases This separation requires coordination between the workflows, where version-bump can optionally trigger release. This adds complexity and potential points of failure in the release process. Decision \u00b6 We will combine the version-bump and release workflows into a single unified workflow that handles the entire release process from version bumping to publishing. This new workflow will: Accept the same inputs as the current version-bump workflow Perform version bumping, committing, and tagging Optionally publish to crates.io and create a GitHub release based on user input Provide clear feedback throughout the process Implementation Plan \u00b6 Create a new release.yml workflow that combines the functionality of both existing workflows Add a new input parameter to control whether to publish after bumping the version Ensure the workflow can be run in dry-run mode for testing Deprecate the old workflows with notices pointing to the new workflow Update documentation to reflect the new release process Consequences \u00b6 Advantages \u00b6 Simplified Process : One workflow to handle the entire release process Reduced Complexity : No need for workflow triggering between separate workflows Better Atomicity : The entire release process becomes a single atomic operation Improved Visibility : Easier to track the entire release process in a single workflow run Reduced Maintenance : Only one workflow file to maintain Disadvantages \u00b6 Migration Period : Users will need to adapt to the new workflow Potentially Longer Workflow Runs : The combined workflow will take longer to run than each individual workflow Less Granular Control : Users lose the ability to run just the version bump or just the release process separately (though this can be mitigated with appropriate input parameters) Alternatives Considered \u00b6 Keep Workflows Separate \u00b6 Pros : More granular control, shorter individual workflow runs Cons : More complex process, potential for coordination issues Use External Release Management Tools \u00b6 Pros : Could provide more advanced features Cons : Adds external dependencies, increases complexity for contributors Related ADRs \u00b6 ADR-0033: Simplified Publishing with Maintained Subcrate Structure ADR-0034: Script Consolidation for Release Process","title":"ADR-0035: Combined Version Bump and Release Workflow"},{"location":"adrs/0035-combined-version-bump-and-release-workflow/#adr-0035-combined-version-bump-and-release-workflow","text":"","title":"ADR-0035: Combined Version Bump and Release Workflow"},{"location":"adrs/0035-combined-version-bump-and-release-workflow/#status","text":"Proposed","title":"Status"},{"location":"adrs/0035-combined-version-bump-and-release-workflow/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0035-combined-version-bump-and-release-workflow/#context","text":"Currently, our release process is split across two GitHub Actions workflows: version-bump.yml : Handles version bumping, committing changes, and creating tags release.yml : Handles publishing to crates.io and creating GitHub releases This separation requires coordination between the workflows, where version-bump can optionally trigger release. This adds complexity and potential points of failure in the release process.","title":"Context"},{"location":"adrs/0035-combined-version-bump-and-release-workflow/#decision","text":"We will combine the version-bump and release workflows into a single unified workflow that handles the entire release process from version bumping to publishing. This new workflow will: Accept the same inputs as the current version-bump workflow Perform version bumping, committing, and tagging Optionally publish to crates.io and create a GitHub release based on user input Provide clear feedback throughout the process","title":"Decision"},{"location":"adrs/0035-combined-version-bump-and-release-workflow/#implementation-plan","text":"Create a new release.yml workflow that combines the functionality of both existing workflows Add a new input parameter to control whether to publish after bumping the version Ensure the workflow can be run in dry-run mode for testing Deprecate the old workflows with notices pointing to the new workflow Update documentation to reflect the new release process","title":"Implementation Plan"},{"location":"adrs/0035-combined-version-bump-and-release-workflow/#consequences","text":"","title":"Consequences"},{"location":"adrs/0035-combined-version-bump-and-release-workflow/#advantages","text":"Simplified Process : One workflow to handle the entire release process Reduced Complexity : No need for workflow triggering between separate workflows Better Atomicity : The entire release process becomes a single atomic operation Improved Visibility : Easier to track the entire release process in a single workflow run Reduced Maintenance : Only one workflow file to maintain","title":"Advantages"},{"location":"adrs/0035-combined-version-bump-and-release-workflow/#disadvantages","text":"Migration Period : Users will need to adapt to the new workflow Potentially Longer Workflow Runs : The combined workflow will take longer to run than each individual workflow Less Granular Control : Users lose the ability to run just the version bump or just the release process separately (though this can be mitigated with appropriate input parameters)","title":"Disadvantages"},{"location":"adrs/0035-combined-version-bump-and-release-workflow/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0035-combined-version-bump-and-release-workflow/#keep-workflows-separate","text":"Pros : More granular control, shorter individual workflow runs Cons : More complex process, potential for coordination issues","title":"Keep Workflows Separate"},{"location":"adrs/0035-combined-version-bump-and-release-workflow/#use-external-release-management-tools","text":"Pros : Could provide more advanced features Cons : Adds external dependencies, increases complexity for contributors","title":"Use External Release Management Tools"},{"location":"adrs/0035-combined-version-bump-and-release-workflow/#related-adrs","text":"ADR-0033: Simplified Publishing with Maintained Subcrate Structure ADR-0034: Script Consolidation for Release Process","title":"Related ADRs"},{"location":"adrs/0036-cleanup-deprecated-scripts-and-workflows/","text":"ADR-0036: Cleanup of Deprecated Scripts and Workflows \u00b6 Status \u00b6 Proposed Date \u00b6 2025-02-27 Context \u00b6 As part of our ongoing efforts to streamline the release process, we have: Created a consolidated script ( update_versions.sh ) that combines the functionality of update_dependency_versions.sh and update_subcrate_versions.sh (ADR-0034) Enhanced release_with_workspaces.sh to handle both version bumping and publishing Created a combined workflow ( combined-release.yml ) that handles the entire release process (ADR-0035) These improvements have left several scripts and workflows deprecated: Scripts: - update_dependency_versions.sh - update_subcrate_versions.sh - test_version_bump.sh - publish.sh - publish_local.sh Workflows: - version-bump.yml - release.yml While we've added deprecation notices to these files, they still exist in the codebase and could cause confusion for contributors. Decision \u00b6 We will remove all deprecated scripts and workflows from the codebase to reduce clutter and prevent confusion. This will ensure that contributors only see and use the current, recommended tools for the release process. Implementation Plan \u00b6 Remove the following deprecated scripts: update_dependency_versions.sh update_subcrate_versions.sh test_version_bump.sh publish.sh publish_local.sh Remove the following deprecated workflows: version-bump.yml release.yml Update the README.md to reflect these changes and provide clear guidance on the current release process. Update any references to these scripts or workflows in other documentation or code. Fix compatibility issues with cargo-workspaces: Remove the conflicting --allow-branch parameter when used with --no-git-commit in the publish command. Consequences \u00b6 Advantages \u00b6 Cleaner Codebase : Removing deprecated files reduces clutter and makes the codebase easier to navigate. Clearer Guidance : Contributors will only see the current, recommended tools for the release process. Reduced Maintenance Burden : Fewer files to maintain and update. Prevents Accidental Use : Eliminates the possibility of accidentally using deprecated scripts or workflows. Disadvantages \u00b6 Breaking Change : Contributors who were using the deprecated scripts or workflows will need to adapt to the new ones. Historical Context : Some historical context about how the release process evolved may be lost. Alternatives Considered \u00b6 Keep Deprecated Files with Notices \u00b6 Pros : Preserves historical context and provides a transition period for contributors. Cons : Continues to clutter the codebase and may cause confusion. Move Deprecated Files to an Archive Directory \u00b6 Pros : Preserves historical context while reducing clutter in the main directories. Cons : Still maintains files that are no longer used, and the archive directory itself could become cluttered over time. Related ADRs \u00b6 ADR-0033: Simplified Publishing with Maintained Subcrate Structure ADR-0034: Script Consolidation for Release Process ADR-0035: Combined Version Bump and Release Workflow","title":"ADR-0036: Cleanup of Deprecated Scripts and Workflows"},{"location":"adrs/0036-cleanup-deprecated-scripts-and-workflows/#adr-0036-cleanup-of-deprecated-scripts-and-workflows","text":"","title":"ADR-0036: Cleanup of Deprecated Scripts and Workflows"},{"location":"adrs/0036-cleanup-deprecated-scripts-and-workflows/#status","text":"Proposed","title":"Status"},{"location":"adrs/0036-cleanup-deprecated-scripts-and-workflows/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0036-cleanup-deprecated-scripts-and-workflows/#context","text":"As part of our ongoing efforts to streamline the release process, we have: Created a consolidated script ( update_versions.sh ) that combines the functionality of update_dependency_versions.sh and update_subcrate_versions.sh (ADR-0034) Enhanced release_with_workspaces.sh to handle both version bumping and publishing Created a combined workflow ( combined-release.yml ) that handles the entire release process (ADR-0035) These improvements have left several scripts and workflows deprecated: Scripts: - update_dependency_versions.sh - update_subcrate_versions.sh - test_version_bump.sh - publish.sh - publish_local.sh Workflows: - version-bump.yml - release.yml While we've added deprecation notices to these files, they still exist in the codebase and could cause confusion for contributors.","title":"Context"},{"location":"adrs/0036-cleanup-deprecated-scripts-and-workflows/#decision","text":"We will remove all deprecated scripts and workflows from the codebase to reduce clutter and prevent confusion. This will ensure that contributors only see and use the current, recommended tools for the release process.","title":"Decision"},{"location":"adrs/0036-cleanup-deprecated-scripts-and-workflows/#implementation-plan","text":"Remove the following deprecated scripts: update_dependency_versions.sh update_subcrate_versions.sh test_version_bump.sh publish.sh publish_local.sh Remove the following deprecated workflows: version-bump.yml release.yml Update the README.md to reflect these changes and provide clear guidance on the current release process. Update any references to these scripts or workflows in other documentation or code. Fix compatibility issues with cargo-workspaces: Remove the conflicting --allow-branch parameter when used with --no-git-commit in the publish command.","title":"Implementation Plan"},{"location":"adrs/0036-cleanup-deprecated-scripts-and-workflows/#consequences","text":"","title":"Consequences"},{"location":"adrs/0036-cleanup-deprecated-scripts-and-workflows/#advantages","text":"Cleaner Codebase : Removing deprecated files reduces clutter and makes the codebase easier to navigate. Clearer Guidance : Contributors will only see the current, recommended tools for the release process. Reduced Maintenance Burden : Fewer files to maintain and update. Prevents Accidental Use : Eliminates the possibility of accidentally using deprecated scripts or workflows.","title":"Advantages"},{"location":"adrs/0036-cleanup-deprecated-scripts-and-workflows/#disadvantages","text":"Breaking Change : Contributors who were using the deprecated scripts or workflows will need to adapt to the new ones. Historical Context : Some historical context about how the release process evolved may be lost.","title":"Disadvantages"},{"location":"adrs/0036-cleanup-deprecated-scripts-and-workflows/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/0036-cleanup-deprecated-scripts-and-workflows/#keep-deprecated-files-with-notices","text":"Pros : Preserves historical context and provides a transition period for contributors. Cons : Continues to clutter the codebase and may cause confusion.","title":"Keep Deprecated Files with Notices"},{"location":"adrs/0036-cleanup-deprecated-scripts-and-workflows/#move-deprecated-files-to-an-archive-directory","text":"Pros : Preserves historical context while reducing clutter in the main directories. Cons : Still maintains files that are no longer used, and the archive directory itself could become cluttered over time.","title":"Move Deprecated Files to an Archive Directory"},{"location":"adrs/0036-cleanup-deprecated-scripts-and-workflows/#related-adrs","text":"ADR-0033: Simplified Publishing with Maintained Subcrate Structure ADR-0034: Script Consolidation for Release Process ADR-0035: Combined Version Bump and Release Workflow","title":"Related ADRs"},{"location":"adrs/0037-fix-cargo-workspaces-version-command-options/","text":"ADR-0037: Fix for Cargo Workspaces Version Command Options \u00b6 Status \u00b6 Proposed Date \u00b6 2025-02-27 Context \u00b6 As part of our transition to using cargo-workspaces for managing versions and publishing (as documented in ADR-0033, ADR-0034, and ADR-0035), we encountered several issues with the release_with_workspaces.sh script and its integration with GitHub Actions workflows: The script was using incompatible command-line options when calling cargo workspaces version . Specifically, it was using both --no-git-commit and --allow-branch=\"*\" options together, which caused the following error: error: The argument '--no-git-commit' cannot be used with '--allow-branch <pattern>' USAGE: cargo workspaces version --no-git-commit <BUMP> The script was not using the --yes flag with cargo workspaces commands, causing the CI process to hang waiting for user input during the version bump process. The script was always creating a git commit, which might not be desired in CI environments where git operations are handled separately. Decision \u00b6 We will make the following changes to the release_with_workspaces.sh script: Remove the --allow-branch=\"*\" option from the cargo workspaces version command, keeping only the --no-git-commit option. Add the --yes flag to all cargo workspaces commands to skip confirmation prompts, ensuring the script can run non-interactively in CI environments. Add a new --no-git-commit option to the script to allow skipping the git commit step, making it more flexible for different environments. Update the GitHub Actions workflow to use the new --no-git-commit option when calling the script. These changes ensure that: 1. The script can run successfully without errors 2. The script can run non-interactively in CI environments 3. We have more flexibility in how git operations are handled 4. We follow the intended usage pattern for cargo-workspaces Implementation Plan \u00b6 Update the release_with_workspaces.sh script to: Remove the --allow-branch=\"*\" option from the cargo workspaces version command Add the --yes flag to all cargo workspaces commands Add a new --no-git-commit option to the script Make the git commit step conditional based on the new option Update the GitHub Actions workflow to use the new --no-git-commit option when calling the script Test the script to ensure it works correctly with various options (patch, minor, major, dry-run, skip-publish, no-git-commit) Consequences \u00b6 Positive \u00b6 The release process will work correctly without errors The script will run non-interactively in CI environments We have more flexibility in how git operations are handled The script will handle git operations in a consistent manner We maintain a clean separation between cargo-workspaces version bumping and our custom git operations Negative \u00b6 None identified Related ADRs \u00b6 ADR-0033: Implementing Single Package with Features ADR-0034: Script Consolidation for Release Process ADR-0035: Combined Version Bump and Release Workflow ADR-0036: Cleanup of Deprecated Scripts and Workflows","title":"ADR-0037: Fix for Cargo Workspaces Version Command Options"},{"location":"adrs/0037-fix-cargo-workspaces-version-command-options/#adr-0037-fix-for-cargo-workspaces-version-command-options","text":"","title":"ADR-0037: Fix for Cargo Workspaces Version Command Options"},{"location":"adrs/0037-fix-cargo-workspaces-version-command-options/#status","text":"Proposed","title":"Status"},{"location":"adrs/0037-fix-cargo-workspaces-version-command-options/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0037-fix-cargo-workspaces-version-command-options/#context","text":"As part of our transition to using cargo-workspaces for managing versions and publishing (as documented in ADR-0033, ADR-0034, and ADR-0035), we encountered several issues with the release_with_workspaces.sh script and its integration with GitHub Actions workflows: The script was using incompatible command-line options when calling cargo workspaces version . Specifically, it was using both --no-git-commit and --allow-branch=\"*\" options together, which caused the following error: error: The argument '--no-git-commit' cannot be used with '--allow-branch <pattern>' USAGE: cargo workspaces version --no-git-commit <BUMP> The script was not using the --yes flag with cargo workspaces commands, causing the CI process to hang waiting for user input during the version bump process. The script was always creating a git commit, which might not be desired in CI environments where git operations are handled separately.","title":"Context"},{"location":"adrs/0037-fix-cargo-workspaces-version-command-options/#decision","text":"We will make the following changes to the release_with_workspaces.sh script: Remove the --allow-branch=\"*\" option from the cargo workspaces version command, keeping only the --no-git-commit option. Add the --yes flag to all cargo workspaces commands to skip confirmation prompts, ensuring the script can run non-interactively in CI environments. Add a new --no-git-commit option to the script to allow skipping the git commit step, making it more flexible for different environments. Update the GitHub Actions workflow to use the new --no-git-commit option when calling the script. These changes ensure that: 1. The script can run successfully without errors 2. The script can run non-interactively in CI environments 3. We have more flexibility in how git operations are handled 4. We follow the intended usage pattern for cargo-workspaces","title":"Decision"},{"location":"adrs/0037-fix-cargo-workspaces-version-command-options/#implementation-plan","text":"Update the release_with_workspaces.sh script to: Remove the --allow-branch=\"*\" option from the cargo workspaces version command Add the --yes flag to all cargo workspaces commands Add a new --no-git-commit option to the script Make the git commit step conditional based on the new option Update the GitHub Actions workflow to use the new --no-git-commit option when calling the script Test the script to ensure it works correctly with various options (patch, minor, major, dry-run, skip-publish, no-git-commit)","title":"Implementation Plan"},{"location":"adrs/0037-fix-cargo-workspaces-version-command-options/#consequences","text":"","title":"Consequences"},{"location":"adrs/0037-fix-cargo-workspaces-version-command-options/#positive","text":"The release process will work correctly without errors The script will run non-interactively in CI environments We have more flexibility in how git operations are handled The script will handle git operations in a consistent manner We maintain a clean separation between cargo-workspaces version bumping and our custom git operations","title":"Positive"},{"location":"adrs/0037-fix-cargo-workspaces-version-command-options/#negative","text":"None identified","title":"Negative"},{"location":"adrs/0037-fix-cargo-workspaces-version-command-options/#related-adrs","text":"ADR-0033: Implementing Single Package with Features ADR-0034: Script Consolidation for Release Process ADR-0035: Combined Version Bump and Release Workflow ADR-0036: Cleanup of Deprecated Scripts and Workflows","title":"Related ADRs"},{"location":"adrs/0038-core-as-non-optional-dependency/","text":"ADR 0038: Core as Non-Optional Dependency \u00b6 Status \u00b6 Accepted Context \u00b6 The Floxide framework was originally designed with a feature-based architecture where even the core functionality could be optionally included via the core feature flag. This approach caused issues with the Cargo workspace and package management, specifically when using tools like cargo-workspaces for version bumping. The error manifested as: feature `core` includes `floxide-core`, but `floxide-core` is not an optional dependency A non-optional dependency of the same name is defined; consider adding `optional = true` to its definition. This occurred because we were trying to define a feature called \"core\" that included \"floxide-core\", but \"floxide-core\" was not marked as an optional dependency. This created a conflict in the Cargo.toml configuration. Decision \u00b6 We have decided to make floxide-core a non-optional dependency that is always included in the framework. This means: The core feature flag has been removed from the feature definitions The floxide-core dependency is no longer marked as optional All other feature modules (transform, event, timer, etc.) remain optional and can be enabled as needed Examples that previously required the core feature now have no required features since core is always included This change simplifies the dependency structure and aligns with the reality that the core functionality is fundamental to the framework and should always be included. Consequences \u00b6 Positive \u00b6 Resolves the issue with cargo-workspaces and version bumping Simplifies the mental model of the framework - core is always included, other modules are optional Reduces complexity in the Cargo.toml configuration Makes it clearer to users that core functionality is not optional Negative \u00b6 Slightly increases the minimum size of the framework since core can no longer be excluded Requires updates to documentation that referenced the core feature flag Neutral \u00b6 Examples that only use core functionality no longer need to specify required features The re-export of floxide_core in lib.rs is no longer conditionally compiled Implementation \u00b6 The implementation involved: Removing the core feature from the features section in Cargo.toml Removing the dependency on core from other features Moving floxide-core to be a standard (non-optional) dependency Updating example configurations to remove the core feature requirement Removing the #[cfg(feature = \"core\")] attribute from the core module re-export in lib.rs Updating documentation to reflect that core is always included Related ADRs \u00b6 ADR-0033: Implementing Single Package with Features ADR-0029: Feature-Based Crate Organization","title":"ADR 0038: Core as Non-Optional Dependency"},{"location":"adrs/0038-core-as-non-optional-dependency/#adr-0038-core-as-non-optional-dependency","text":"","title":"ADR 0038: Core as Non-Optional Dependency"},{"location":"adrs/0038-core-as-non-optional-dependency/#status","text":"Accepted","title":"Status"},{"location":"adrs/0038-core-as-non-optional-dependency/#context","text":"The Floxide framework was originally designed with a feature-based architecture where even the core functionality could be optionally included via the core feature flag. This approach caused issues with the Cargo workspace and package management, specifically when using tools like cargo-workspaces for version bumping. The error manifested as: feature `core` includes `floxide-core`, but `floxide-core` is not an optional dependency A non-optional dependency of the same name is defined; consider adding `optional = true` to its definition. This occurred because we were trying to define a feature called \"core\" that included \"floxide-core\", but \"floxide-core\" was not marked as an optional dependency. This created a conflict in the Cargo.toml configuration.","title":"Context"},{"location":"adrs/0038-core-as-non-optional-dependency/#decision","text":"We have decided to make floxide-core a non-optional dependency that is always included in the framework. This means: The core feature flag has been removed from the feature definitions The floxide-core dependency is no longer marked as optional All other feature modules (transform, event, timer, etc.) remain optional and can be enabled as needed Examples that previously required the core feature now have no required features since core is always included This change simplifies the dependency structure and aligns with the reality that the core functionality is fundamental to the framework and should always be included.","title":"Decision"},{"location":"adrs/0038-core-as-non-optional-dependency/#consequences","text":"","title":"Consequences"},{"location":"adrs/0038-core-as-non-optional-dependency/#positive","text":"Resolves the issue with cargo-workspaces and version bumping Simplifies the mental model of the framework - core is always included, other modules are optional Reduces complexity in the Cargo.toml configuration Makes it clearer to users that core functionality is not optional","title":"Positive"},{"location":"adrs/0038-core-as-non-optional-dependency/#negative","text":"Slightly increases the minimum size of the framework since core can no longer be excluded Requires updates to documentation that referenced the core feature flag","title":"Negative"},{"location":"adrs/0038-core-as-non-optional-dependency/#neutral","text":"Examples that only use core functionality no longer need to specify required features The re-export of floxide_core in lib.rs is no longer conditionally compiled","title":"Neutral"},{"location":"adrs/0038-core-as-non-optional-dependency/#implementation","text":"The implementation involved: Removing the core feature from the features section in Cargo.toml Removing the dependency on core from other features Moving floxide-core to be a standard (non-optional) dependency Updating example configurations to remove the core feature requirement Removing the #[cfg(feature = \"core\")] attribute from the core module re-export in lib.rs Updating documentation to reflect that core is always included","title":"Implementation"},{"location":"adrs/0038-core-as-non-optional-dependency/#related-adrs","text":"ADR-0033: Implementing Single Package with Features ADR-0029: Feature-Based Crate Organization","title":"Related ADRs"},{"location":"adrs/0039-crates-io-publishing-strategy/","text":"ADR 0039: Crates.io Publishing Strategy \u00b6 Status \u00b6 Accepted Date \u00b6 2025-02-27 Context \u00b6 After successfully publishing the Floxide framework to crates.io, we need to document our publishing strategy and ensure it is properly integrated into our CI/CD pipeline. This ADR builds upon and consolidates the learnings from our initial publishing experience and supersedes parts of previous ADRs related to publishing. The key challenges we faced during the initial publishing process were: Dependency Order : Ensuring crates are published in the correct dependency order Version Consistency : Maintaining consistent version numbers across all crates Internal Dependency References : Ensuring internal dependencies reference the correct versions Automation : Streamlining the process to avoid manual steps We discovered that using cargo-workspaces with the --exact flag provides an effective solution for managing these challenges. Decision \u00b6 We will adopt a standardized publishing strategy using cargo-workspaces as our primary tool for version management and publishing. This approach will be integrated into our CI/CD pipeline. 1. Tool Selection \u00b6 We will use: - cargo-workspaces : For version bumping and publishing in the correct dependency order - GitHub Actions : For automating the release process 2. Configuration Files \u00b6 We will maintain two key configuration files: cargo-workspaces.toml \u00b6 [workspace] # Whether to inherit the version from the workspace inherit_version = true # Whether to link all dependencies to their workspace versions link_workspace_deps = true # Whether to update the versions of workspace dependencies when versioning update_workspace_deps = true # Custom publish order publish_order = [ \"floxide-core\" , \"floxide-transform\" , \"floxide-event\" , \"floxide-timer\" , \"floxide-longrunning\" , \"floxide-reactive\" , \"floxide\" ] release.toml (for cargo-release, as a backup option) \u00b6 # Don't push changes automatically push = false # Don't publish to crates.io automatically publish = false # Don't create a tag automatically tag = false # Don't create a GitHub release automatically release = false # Update dependencies with exact version dependent-version = \"fix\" # Use conventional commits for changelog generation pre-release-commit-message = \"chore(release): {{version}}\" 3. Version Management Process \u00b6 The standard process for bumping versions will be: # Bump versions with exact dependency references cargo workspaces version [ patch | minor | major ] --exact This command will: 1. Bump all crate versions according to semver 2. Update all internal dependency references with exact version pins (e.g., =1.0.11 ) 3. Create a commit with the version changes 4. Create git tags for the release 4. Publishing Process \u00b6 The standard process for publishing will be: # Publish all crates in the correct dependency order cargo workspaces publish --from-git This command will: 1. Publish crates in the order specified in cargo-workspaces.toml 2. Use the versions from the git tags 5. CI/CD Integration \u00b6 We will update our GitHub Actions workflow to use this approach: # Version bump step - name : Bump version run : | cargo workspaces version ${{ inputs.release_type }} --exact --yes # Publishing step - name : Publish to crates.io run : | cargo workspaces publish --from-git --yes Consequences \u00b6 Positive \u00b6 Automated Dependency Management : The --exact flag ensures all internal dependencies use exact version pins Correct Publishing Order : The publish_order in cargo-workspaces.toml ensures crates are published in the right order Simplified Process : The entire release process is reduced to two commands Consistency : All crates maintain the same version number Negative \u00b6 Tool Dependency : We now rely on cargo-workspaces for our release process Exact Version Pins : Using exact version pins ( =1.0.11 ) may be more restrictive than semver ranges Neutral \u00b6 Version Synchronization : All crates share the same version number, which may not reflect the actual changes in each crate Superseded ADRs \u00b6 This ADR supersedes or modifies parts of: ADR-0014 : Updates the publishing strategy section ADR-0030 : Replaces the manual scripts with cargo-workspaces ADR-0035 : Updates the release workflow to use cargo-workspaces References \u00b6 cargo-workspaces documentation Initial publishing experience (February 2025)","title":"ADR 0039: Crates.io Publishing Strategy"},{"location":"adrs/0039-crates-io-publishing-strategy/#adr-0039-cratesio-publishing-strategy","text":"","title":"ADR 0039: Crates.io Publishing Strategy"},{"location":"adrs/0039-crates-io-publishing-strategy/#status","text":"Accepted","title":"Status"},{"location":"adrs/0039-crates-io-publishing-strategy/#date","text":"2025-02-27","title":"Date"},{"location":"adrs/0039-crates-io-publishing-strategy/#context","text":"After successfully publishing the Floxide framework to crates.io, we need to document our publishing strategy and ensure it is properly integrated into our CI/CD pipeline. This ADR builds upon and consolidates the learnings from our initial publishing experience and supersedes parts of previous ADRs related to publishing. The key challenges we faced during the initial publishing process were: Dependency Order : Ensuring crates are published in the correct dependency order Version Consistency : Maintaining consistent version numbers across all crates Internal Dependency References : Ensuring internal dependencies reference the correct versions Automation : Streamlining the process to avoid manual steps We discovered that using cargo-workspaces with the --exact flag provides an effective solution for managing these challenges.","title":"Context"},{"location":"adrs/0039-crates-io-publishing-strategy/#decision","text":"We will adopt a standardized publishing strategy using cargo-workspaces as our primary tool for version management and publishing. This approach will be integrated into our CI/CD pipeline.","title":"Decision"},{"location":"adrs/0039-crates-io-publishing-strategy/#1-tool-selection","text":"We will use: - cargo-workspaces : For version bumping and publishing in the correct dependency order - GitHub Actions : For automating the release process","title":"1. Tool Selection"},{"location":"adrs/0039-crates-io-publishing-strategy/#2-configuration-files","text":"We will maintain two key configuration files:","title":"2. Configuration Files"},{"location":"adrs/0039-crates-io-publishing-strategy/#cargo-workspacestoml","text":"[workspace] # Whether to inherit the version from the workspace inherit_version = true # Whether to link all dependencies to their workspace versions link_workspace_deps = true # Whether to update the versions of workspace dependencies when versioning update_workspace_deps = true # Custom publish order publish_order = [ \"floxide-core\" , \"floxide-transform\" , \"floxide-event\" , \"floxide-timer\" , \"floxide-longrunning\" , \"floxide-reactive\" , \"floxide\" ]","title":"cargo-workspaces.toml"},{"location":"adrs/0039-crates-io-publishing-strategy/#releasetoml-for-cargo-release-as-a-backup-option","text":"# Don't push changes automatically push = false # Don't publish to crates.io automatically publish = false # Don't create a tag automatically tag = false # Don't create a GitHub release automatically release = false # Update dependencies with exact version dependent-version = \"fix\" # Use conventional commits for changelog generation pre-release-commit-message = \"chore(release): {{version}}\"","title":"release.toml (for cargo-release, as a backup option)"},{"location":"adrs/0039-crates-io-publishing-strategy/#3-version-management-process","text":"The standard process for bumping versions will be: # Bump versions with exact dependency references cargo workspaces version [ patch | minor | major ] --exact This command will: 1. Bump all crate versions according to semver 2. Update all internal dependency references with exact version pins (e.g., =1.0.11 ) 3. Create a commit with the version changes 4. Create git tags for the release","title":"3. Version Management Process"},{"location":"adrs/0039-crates-io-publishing-strategy/#4-publishing-process","text":"The standard process for publishing will be: # Publish all crates in the correct dependency order cargo workspaces publish --from-git This command will: 1. Publish crates in the order specified in cargo-workspaces.toml 2. Use the versions from the git tags","title":"4. Publishing Process"},{"location":"adrs/0039-crates-io-publishing-strategy/#5-cicd-integration","text":"We will update our GitHub Actions workflow to use this approach: # Version bump step - name : Bump version run : | cargo workspaces version ${{ inputs.release_type }} --exact --yes # Publishing step - name : Publish to crates.io run : | cargo workspaces publish --from-git --yes","title":"5. CI/CD Integration"},{"location":"adrs/0039-crates-io-publishing-strategy/#consequences","text":"","title":"Consequences"},{"location":"adrs/0039-crates-io-publishing-strategy/#positive","text":"Automated Dependency Management : The --exact flag ensures all internal dependencies use exact version pins Correct Publishing Order : The publish_order in cargo-workspaces.toml ensures crates are published in the right order Simplified Process : The entire release process is reduced to two commands Consistency : All crates maintain the same version number","title":"Positive"},{"location":"adrs/0039-crates-io-publishing-strategy/#negative","text":"Tool Dependency : We now rely on cargo-workspaces for our release process Exact Version Pins : Using exact version pins ( =1.0.11 ) may be more restrictive than semver ranges","title":"Negative"},{"location":"adrs/0039-crates-io-publishing-strategy/#neutral","text":"Version Synchronization : All crates share the same version number, which may not reflect the actual changes in each crate","title":"Neutral"},{"location":"adrs/0039-crates-io-publishing-strategy/#superseded-adrs","text":"This ADR supersedes or modifies parts of: ADR-0014 : Updates the publishing strategy section ADR-0030 : Replaces the manual scripts with cargo-workspaces ADR-0035 : Updates the release workflow to use cargo-workspaces","title":"Superseded ADRs"},{"location":"adrs/0039-crates-io-publishing-strategy/#references","text":"cargo-workspaces documentation Initial publishing experience (February 2025)","title":"References"},{"location":"adrs/adr-template/","text":"ADR-NUMBER: Title \u00b6 Status \u00b6 Proposed Date \u00b6 2025-02-27 YYYY-MM-DD Context \u00b6 Decision \u00b6 Consequences \u00b6 Advantages \u00b6 Disadvantages \u00b6 Migration Path \u00b6 Alternatives Considered \u00b6 Implementation Notes \u00b6 Related ADRs \u00b6 References \u00b6","title":"ADR-NUMBER: Title"},{"location":"adrs/adr-template/#adr-number-title","text":"","title":"ADR-NUMBER: Title"},{"location":"adrs/adr-template/#status","text":"Proposed","title":"Status"},{"location":"adrs/adr-template/#date","text":"2025-02-27 YYYY-MM-DD","title":"Date"},{"location":"adrs/adr-template/#context","text":"","title":"Context"},{"location":"adrs/adr-template/#decision","text":"","title":"Decision"},{"location":"adrs/adr-template/#consequences","text":"","title":"Consequences"},{"location":"adrs/adr-template/#advantages","text":"","title":"Advantages"},{"location":"adrs/adr-template/#disadvantages","text":"","title":"Disadvantages"},{"location":"adrs/adr-template/#migration-path","text":"","title":"Migration Path"},{"location":"adrs/adr-template/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adrs/adr-template/#implementation-notes","text":"","title":"Implementation Notes"},{"location":"adrs/adr-template/#related-adrs","text":"","title":"Related ADRs"},{"location":"adrs/adr-template/#references","text":"","title":"References"},{"location":"adrs/version-bump-diagram/","text":"Floxide Version Bump and Release Process \u00b6 flowchart TD A[Developer initiates version bump] --> B[Update workspace.package.version in Cargo.toml] B --> C1[Run update_subcrate_versions.sh script] C1 --> C2[Script ensures all subcrates use workspace inheritance] C2 --> D[Run update_dependency_versions.sh script] D --> E[Script updates all internal dependency versions] E --> F[Commit changes and create git tag] F --> G[Push tag to GitHub] G --> H[GitHub Actions workflow triggered by tag] H --> I[Checkout code at tag ref using WORKFLOW_PAT] I --> J[Verify tag version matches Cargo.toml version] J --> K[Run tests with all features] K --> L1[Run update_subcrate_versions.sh again] L1 --> L2[Run update_dependency_versions.sh again] L2 --> M[Publish floxide-core subcrate using CRATES_IO_TOKEN] M --> N[Wait for crates.io indexing] N --> O[Publish floxide-transform subcrate using CRATES_IO_TOKEN] O --> P[Wait for crates.io indexing] P --> Q[Publish floxide-event subcrate using CRATES_IO_TOKEN] Q --> R[Wait for crates.io indexing] R --> S[Publish floxide-timer subcrate using CRATES_IO_TOKEN] S --> T[Wait for crates.io indexing] T --> U[Publish floxide-longrunning subcrate using CRATES_IO_TOKEN] U --> V[Wait for crates.io indexing] V --> W[Publish floxide-reactive subcrate using CRATES_IO_TOKEN] W --> X[Wait for crates.io indexing] X --> Y[Publish root floxide crate using CRATES_IO_TOKEN] Y --> Z[Create GitHub Release using GITHUB_TOKEN] Explanation \u00b6 1. Version Bump Phase (Local Development) \u00b6 Developer initiates version bump : When it's time for a new release, a developer updates the version in the workspace. Update workspace.package.version : The version is updated in the [workspace.package] section of the root Cargo.toml. Run update_subcrate_versions.sh : This script ensures all subcrates use workspace inheritance for their versions. Run update_dependency_versions.sh : This script updates all internal dependency versions in the root Cargo.toml to match the workspace version. Commit and tag : Changes are committed and a git tag (e.g., v1.0.3 ) is created. Push tag : The tag is pushed to GitHub, which triggers the release workflow. 2. Release Workflow (CI/CD) \u00b6 Workflow triggered : The GitHub Actions workflow is triggered by the new tag. Checkout code : The workflow checks out the code at the tag reference using the WORKFLOW_PAT token for authentication. Verify versions : It verifies that the tag version matches the version in Cargo.toml. Run tests : All tests are run with all features enabled to ensure everything works. Run update scripts again : Both scripts are run again to ensure all versions are correct. 3. Publication Phase (CI/CD) \u00b6 Publish subcrates in order : The workflow publishes each subcrate in a specific order using the CRATES_IO_TOKEN for authentication: floxide-core floxide-transform floxide-event floxide-timer floxide-longrunning floxide-reactive Wait for indexing : After each subcrate is published, the workflow waits for crates.io to index it. Publish root crate : Finally, the root floxide crate is published, which depends on all the subcrates. Create GitHub Release : A GitHub Release is created with release notes using the GITHUB_TOKEN token. Key Points \u00b6 Version Synchronization : The scripts ensure all versions are synchronized: Subcrates use workspace inheritance ( version.workspace = true ) Root crate dependencies have explicit versions that match the workspace version Publication Order : Subcrates must be published before the root crate to ensure dependencies are available on crates.io. Waiting Periods : The workflow includes waiting periods to allow crates.io to index each published crate. Verification : The workflow verifies that the tag version matches the Cargo.toml version to prevent mismatches. Authentication Tokens : WORKFLOW_PAT : GitHub Personal Access Token used for repository operations that need to trigger other workflows (checkout) GITHUB_TOKEN : Automatically provided token used for GitHub operations (creating releases) CRATES_IO_TOKEN : Crates.io API token used for publishing packages to crates.io This process ensures that all crates in the workspace are published with consistent versions and that dependencies are available when needed during the publication process. The authentication tokens provide secure access to both GitHub and crates.io, enabling the automated release process to run without manual intervention.","title":"Floxide Version Bump and Release Process"},{"location":"adrs/version-bump-diagram/#floxide-version-bump-and-release-process","text":"flowchart TD A[Developer initiates version bump] --> B[Update workspace.package.version in Cargo.toml] B --> C1[Run update_subcrate_versions.sh script] C1 --> C2[Script ensures all subcrates use workspace inheritance] C2 --> D[Run update_dependency_versions.sh script] D --> E[Script updates all internal dependency versions] E --> F[Commit changes and create git tag] F --> G[Push tag to GitHub] G --> H[GitHub Actions workflow triggered by tag] H --> I[Checkout code at tag ref using WORKFLOW_PAT] I --> J[Verify tag version matches Cargo.toml version] J --> K[Run tests with all features] K --> L1[Run update_subcrate_versions.sh again] L1 --> L2[Run update_dependency_versions.sh again] L2 --> M[Publish floxide-core subcrate using CRATES_IO_TOKEN] M --> N[Wait for crates.io indexing] N --> O[Publish floxide-transform subcrate using CRATES_IO_TOKEN] O --> P[Wait for crates.io indexing] P --> Q[Publish floxide-event subcrate using CRATES_IO_TOKEN] Q --> R[Wait for crates.io indexing] R --> S[Publish floxide-timer subcrate using CRATES_IO_TOKEN] S --> T[Wait for crates.io indexing] T --> U[Publish floxide-longrunning subcrate using CRATES_IO_TOKEN] U --> V[Wait for crates.io indexing] V --> W[Publish floxide-reactive subcrate using CRATES_IO_TOKEN] W --> X[Wait for crates.io indexing] X --> Y[Publish root floxide crate using CRATES_IO_TOKEN] Y --> Z[Create GitHub Release using GITHUB_TOKEN]","title":"Floxide Version Bump and Release Process"},{"location":"adrs/version-bump-diagram/#explanation","text":"","title":"Explanation"},{"location":"adrs/version-bump-diagram/#1-version-bump-phase-local-development","text":"Developer initiates version bump : When it's time for a new release, a developer updates the version in the workspace. Update workspace.package.version : The version is updated in the [workspace.package] section of the root Cargo.toml. Run update_subcrate_versions.sh : This script ensures all subcrates use workspace inheritance for their versions. Run update_dependency_versions.sh : This script updates all internal dependency versions in the root Cargo.toml to match the workspace version. Commit and tag : Changes are committed and a git tag (e.g., v1.0.3 ) is created. Push tag : The tag is pushed to GitHub, which triggers the release workflow.","title":"1. Version Bump Phase (Local Development)"},{"location":"adrs/version-bump-diagram/#2-release-workflow-cicd","text":"Workflow triggered : The GitHub Actions workflow is triggered by the new tag. Checkout code : The workflow checks out the code at the tag reference using the WORKFLOW_PAT token for authentication. Verify versions : It verifies that the tag version matches the version in Cargo.toml. Run tests : All tests are run with all features enabled to ensure everything works. Run update scripts again : Both scripts are run again to ensure all versions are correct.","title":"2. Release Workflow (CI/CD)"},{"location":"adrs/version-bump-diagram/#3-publication-phase-cicd","text":"Publish subcrates in order : The workflow publishes each subcrate in a specific order using the CRATES_IO_TOKEN for authentication: floxide-core floxide-transform floxide-event floxide-timer floxide-longrunning floxide-reactive Wait for indexing : After each subcrate is published, the workflow waits for crates.io to index it. Publish root crate : Finally, the root floxide crate is published, which depends on all the subcrates. Create GitHub Release : A GitHub Release is created with release notes using the GITHUB_TOKEN token.","title":"3. Publication Phase (CI/CD)"},{"location":"adrs/version-bump-diagram/#key-points","text":"Version Synchronization : The scripts ensure all versions are synchronized: Subcrates use workspace inheritance ( version.workspace = true ) Root crate dependencies have explicit versions that match the workspace version Publication Order : Subcrates must be published before the root crate to ensure dependencies are available on crates.io. Waiting Periods : The workflow includes waiting periods to allow crates.io to index each published crate. Verification : The workflow verifies that the tag version matches the Cargo.toml version to prevent mismatches. Authentication Tokens : WORKFLOW_PAT : GitHub Personal Access Token used for repository operations that need to trigger other workflows (checkout) GITHUB_TOKEN : Automatically provided token used for GitHub operations (creating releases) CRATES_IO_TOKEN : Crates.io API token used for publishing packages to crates.io This process ensures that all crates in the workspace are published with consistent versions and that dependencies are available when needed during the publication process. The authentication tokens provide secure access to both GitHub and crates.io, enabling the automated release process to run without manual intervention.","title":"Key Points"},{"location":"api/floxide-batch/","text":"floxide-batch API Reference \u00b6 The floxide-batch crate provides batch processing capabilities for the Floxide framework. Overview \u00b6 This crate implements batch processing patterns for handling collections of items in parallel. It provides: Batch nodes for processing collections Configurable concurrency limits Progress tracking Error handling for batch operations Key Types \u00b6 BatchNode \u00b6 pub trait BatchNode < I , O > : Send + Sync { async fn process_batch ( & self , items : Vec < I > ) -> Result < Vec < O > , FloxideError > ; fn concurrency_limit ( & self ) -> usize ; } The BatchNode trait defines the core interface for batch processing nodes. BatchContext \u00b6 pub struct BatchContext < T > { items : Vec < T > , results : Vec < T > , errors : Vec < FloxideError > , progress : Progress , } BatchContext holds the state of a batch processing operation. BatchOptions \u00b6 pub struct BatchOptions { pub concurrency_limit : usize , pub chunk_size : usize , pub retry_count : usize , pub backoff_duration : Duration , } BatchOptions configures the behavior of batch processing. Usage Example \u00b6 use floxide_batch ::{ batch_node , BatchNode , BatchContext }; // Create a batch processing node fn create_batch_processor () -> impl BatchNode < String , String > { batch_node ( 10 , // Concurrency limit | item : String | async move { Ok ( item . to_uppercase ()) } ) } // Use the node in a workflow let node = create_batch_processor (); let mut context = BatchContext :: new ( vec! [ \"hello\" . to_string (), \"world\" . to_string (), ]); let result = node . process_batch ( context . items ). await ? ; println! ( \"Processed items: {:?}\" , result ); Advanced Features \u00b6 Chunked Processing \u00b6 let node = batch_node ( 5 , process_item ) . with_chunk_size ( 100 ) // Process in chunks of 100 items . with_progress_callback ( | progress | { println! ( \"Progress: {}%\" , progress . percent ); }); Error Handling \u00b6 let node = batch_node ( 5 , process_item ) . with_retry ( 3 ) // Retry failed items up to 3 times . with_backoff ( Duration :: from_secs ( 1 )) // Wait between retries . with_error_handler ( | e | { eprintln! ( \"Processing error: {}\" , e ); None // Skip failed items }); Custom Batch Processing \u00b6 struct CustomBatchProcessor ; impl BatchNode < String , String > for CustomBatchProcessor { async fn process_batch ( & self , items : Vec < String > ) -> Result < Vec < String > , FloxideError > { let mut results = Vec :: with_capacity ( items . len ()); for item in items { results . push ( process_item ( item ). await ? ); } Ok ( results ) } fn concurrency_limit ( & self ) -> usize { 10 } } Error Handling \u00b6 The crate uses the standard FloxideError type for error handling. All operations that can fail return a Result<T, FloxideError> . Best Practices \u00b6 Choose appropriate concurrency limits based on: System resources External service limits Data characteristics Implement proper error handling: Retry transient failures Log permanent failures Clean up resources Monitor batch processing: Track progress Log performance metrics Handle resource constraints Consider chunking for large datasets: Balance memory usage Maintain responsiveness Handle partial failures See Also \u00b6 Batch Processing Implementation ADR Batch Processing Example Node Lifecycle Methods","title":"floxide-batch"},{"location":"api/floxide-batch/#floxide-batch-api-reference","text":"The floxide-batch crate provides batch processing capabilities for the Floxide framework.","title":"floxide-batch API Reference"},{"location":"api/floxide-batch/#overview","text":"This crate implements batch processing patterns for handling collections of items in parallel. It provides: Batch nodes for processing collections Configurable concurrency limits Progress tracking Error handling for batch operations","title":"Overview"},{"location":"api/floxide-batch/#key-types","text":"","title":"Key Types"},{"location":"api/floxide-batch/#batchnode","text":"pub trait BatchNode < I , O > : Send + Sync { async fn process_batch ( & self , items : Vec < I > ) -> Result < Vec < O > , FloxideError > ; fn concurrency_limit ( & self ) -> usize ; } The BatchNode trait defines the core interface for batch processing nodes.","title":"BatchNode"},{"location":"api/floxide-batch/#batchcontext","text":"pub struct BatchContext < T > { items : Vec < T > , results : Vec < T > , errors : Vec < FloxideError > , progress : Progress , } BatchContext holds the state of a batch processing operation.","title":"BatchContext"},{"location":"api/floxide-batch/#batchoptions","text":"pub struct BatchOptions { pub concurrency_limit : usize , pub chunk_size : usize , pub retry_count : usize , pub backoff_duration : Duration , } BatchOptions configures the behavior of batch processing.","title":"BatchOptions"},{"location":"api/floxide-batch/#usage-example","text":"use floxide_batch ::{ batch_node , BatchNode , BatchContext }; // Create a batch processing node fn create_batch_processor () -> impl BatchNode < String , String > { batch_node ( 10 , // Concurrency limit | item : String | async move { Ok ( item . to_uppercase ()) } ) } // Use the node in a workflow let node = create_batch_processor (); let mut context = BatchContext :: new ( vec! [ \"hello\" . to_string (), \"world\" . to_string (), ]); let result = node . process_batch ( context . items ). await ? ; println! ( \"Processed items: {:?}\" , result );","title":"Usage Example"},{"location":"api/floxide-batch/#advanced-features","text":"","title":"Advanced Features"},{"location":"api/floxide-batch/#chunked-processing","text":"let node = batch_node ( 5 , process_item ) . with_chunk_size ( 100 ) // Process in chunks of 100 items . with_progress_callback ( | progress | { println! ( \"Progress: {}%\" , progress . percent ); });","title":"Chunked Processing"},{"location":"api/floxide-batch/#error-handling","text":"let node = batch_node ( 5 , process_item ) . with_retry ( 3 ) // Retry failed items up to 3 times . with_backoff ( Duration :: from_secs ( 1 )) // Wait between retries . with_error_handler ( | e | { eprintln! ( \"Processing error: {}\" , e ); None // Skip failed items });","title":"Error Handling"},{"location":"api/floxide-batch/#custom-batch-processing","text":"struct CustomBatchProcessor ; impl BatchNode < String , String > for CustomBatchProcessor { async fn process_batch ( & self , items : Vec < String > ) -> Result < Vec < String > , FloxideError > { let mut results = Vec :: with_capacity ( items . len ()); for item in items { results . push ( process_item ( item ). await ? ); } Ok ( results ) } fn concurrency_limit ( & self ) -> usize { 10 } }","title":"Custom Batch Processing"},{"location":"api/floxide-batch/#error-handling_1","text":"The crate uses the standard FloxideError type for error handling. All operations that can fail return a Result<T, FloxideError> .","title":"Error Handling"},{"location":"api/floxide-batch/#best-practices","text":"Choose appropriate concurrency limits based on: System resources External service limits Data characteristics Implement proper error handling: Retry transient failures Log permanent failures Clean up resources Monitor batch processing: Track progress Log performance metrics Handle resource constraints Consider chunking for large datasets: Balance memory usage Maintain responsiveness Handle partial failures","title":"Best Practices"},{"location":"api/floxide-batch/#see-also","text":"Batch Processing Implementation ADR Batch Processing Example Node Lifecycle Methods","title":"See Also"},{"location":"api/floxide-core/","text":"floxide-core API Reference \u00b6 The floxide-core crate provides the foundational types and traits for building workflows in the Floxide framework. For a detailed overview of the core abstractions, see Core Framework Abstractions . Core Types \u00b6 Node \u00b6 #[async_trait] pub trait Node < Context , Action > : Send + Sync + ' static where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { /// Get the node's unique identifier fn id ( & self ) -> NodeId ; /// Execute the node's logic async fn execute ( & self , ctx : & mut Context ) -> Result < Action , FloxideError > ; } The Node trait is the fundamental building block of workflows. For detailed information about the node lifecycle, see Node Lifecycle Methods . Workflow \u00b6 pub struct Workflow < Context , Action > where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { nodes : HashMap < NodeId , Box < dyn Node < Context , Action >>> , routes : HashMap < ( NodeId , Action ), NodeId > , } The Workflow struct orchestrates the execution of nodes. For more information about workflow patterns, see Event-Driven Workflow Pattern . ActionType \u00b6 pub trait ActionType : Clone + Debug + Send + Sync + ' static { /// Convert the action to a string representation fn as_str ( & self ) -> & str ; } The ActionType trait defines how nodes signal their completion and routing decisions. FloxideError \u00b6 #[derive(Debug, Error)] pub enum FloxideError { #[error( \"Node error: {0}\" )] NodeError ( String ), #[error( \"Workflow error: {0}\" )] WorkflowError ( String ), #[error( \"Routing error: {0}\" )] RoutingError ( String ), #[error( \"Context error: {0}\" )] ContextError ( String ), #[error( \"Other error: {0}\" )] Other ( String ), } The FloxideError type provides structured error handling throughout the framework. Usage Examples \u00b6 Basic Node \u00b6 use floxide_core ::{ Node , NodeId , FloxideError , DefaultAction }; struct MyNode { id : NodeId , } #[async_trait] impl Node < MyContext , DefaultAction > for MyNode { fn id ( & self ) -> NodeId { self . id . clone () } async fn execute ( & self , ctx : & mut MyContext ) -> Result < DefaultAction , FloxideError > { // Node implementation Ok ( DefaultAction :: Next ) } } Workflow Construction \u00b6 use floxide_core ::{ Workflow , DefaultAction }; let mut workflow = Workflow :: new ( start_node ) . then ( process_node ) . then ( end_node ); workflow . add_conditional_route ( process_node . id (), CustomAction :: Retry , start_node . id (), ); Best Practices \u00b6 Node Design Keep nodes focused on a single responsibility Use appropriate lifecycle methods Handle errors gracefully Clean up resources properly Workflow Design Model workflows as directed graphs Use clear routing logic Handle all possible actions Consider error recovery paths Error Handling Use specific error types Provide clear error messages Implement proper recovery Log relevant details Testing Test individual nodes Test workflow routing Test error scenarios Use mock contexts See Also \u00b6 Core Framework Abstractions Node Lifecycle Methods Event-Driven Workflow Pattern Batch Processing Implementation Async Runtime Selection","title":"floxide-core"},{"location":"api/floxide-core/#floxide-core-api-reference","text":"The floxide-core crate provides the foundational types and traits for building workflows in the Floxide framework. For a detailed overview of the core abstractions, see Core Framework Abstractions .","title":"floxide-core API Reference"},{"location":"api/floxide-core/#core-types","text":"","title":"Core Types"},{"location":"api/floxide-core/#node","text":"#[async_trait] pub trait Node < Context , Action > : Send + Sync + ' static where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { /// Get the node's unique identifier fn id ( & self ) -> NodeId ; /// Execute the node's logic async fn execute ( & self , ctx : & mut Context ) -> Result < Action , FloxideError > ; } The Node trait is the fundamental building block of workflows. For detailed information about the node lifecycle, see Node Lifecycle Methods .","title":"Node"},{"location":"api/floxide-core/#workflow","text":"pub struct Workflow < Context , Action > where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { nodes : HashMap < NodeId , Box < dyn Node < Context , Action >>> , routes : HashMap < ( NodeId , Action ), NodeId > , } The Workflow struct orchestrates the execution of nodes. For more information about workflow patterns, see Event-Driven Workflow Pattern .","title":"Workflow"},{"location":"api/floxide-core/#actiontype","text":"pub trait ActionType : Clone + Debug + Send + Sync + ' static { /// Convert the action to a string representation fn as_str ( & self ) -> & str ; } The ActionType trait defines how nodes signal their completion and routing decisions.","title":"ActionType"},{"location":"api/floxide-core/#floxideerror","text":"#[derive(Debug, Error)] pub enum FloxideError { #[error( \"Node error: {0}\" )] NodeError ( String ), #[error( \"Workflow error: {0}\" )] WorkflowError ( String ), #[error( \"Routing error: {0}\" )] RoutingError ( String ), #[error( \"Context error: {0}\" )] ContextError ( String ), #[error( \"Other error: {0}\" )] Other ( String ), } The FloxideError type provides structured error handling throughout the framework.","title":"FloxideError"},{"location":"api/floxide-core/#usage-examples","text":"","title":"Usage Examples"},{"location":"api/floxide-core/#basic-node","text":"use floxide_core ::{ Node , NodeId , FloxideError , DefaultAction }; struct MyNode { id : NodeId , } #[async_trait] impl Node < MyContext , DefaultAction > for MyNode { fn id ( & self ) -> NodeId { self . id . clone () } async fn execute ( & self , ctx : & mut MyContext ) -> Result < DefaultAction , FloxideError > { // Node implementation Ok ( DefaultAction :: Next ) } }","title":"Basic Node"},{"location":"api/floxide-core/#workflow-construction","text":"use floxide_core ::{ Workflow , DefaultAction }; let mut workflow = Workflow :: new ( start_node ) . then ( process_node ) . then ( end_node ); workflow . add_conditional_route ( process_node . id (), CustomAction :: Retry , start_node . id (), );","title":"Workflow Construction"},{"location":"api/floxide-core/#best-practices","text":"Node Design Keep nodes focused on a single responsibility Use appropriate lifecycle methods Handle errors gracefully Clean up resources properly Workflow Design Model workflows as directed graphs Use clear routing logic Handle all possible actions Consider error recovery paths Error Handling Use specific error types Provide clear error messages Implement proper recovery Log relevant details Testing Test individual nodes Test workflow routing Test error scenarios Use mock contexts","title":"Best Practices"},{"location":"api/floxide-core/#see-also","text":"Core Framework Abstractions Node Lifecycle Methods Event-Driven Workflow Pattern Batch Processing Implementation Async Runtime Selection","title":"See Also"},{"location":"api/floxide-event/","text":"floxide-event API Reference \u00b6 This document provides a reference for the floxide-event crate, which extends the Floxide framework with event-driven capabilities. Overview \u00b6 The floxide-event crate provides extensions to the core Floxide framework for building event-driven workflows, including: Event-driven node abstractions Event source implementations Event-driven workflow orchestration Integration with standard workflows Core Modules \u00b6 Event Module \u00b6 The Event module provides the core event abstractions: use floxide_event :: event ::{ Event , EventAction }; // Define a custom event type #[derive(Clone)] struct SensorEvent { id : String , value : f64 , } // Implement the Event trait impl Event for SensorEvent {} EventDrivenNode Module \u00b6 The EventDrivenNode module provides the core node abstractions for event-driven workflows: use floxide_event :: node ::{ EventDrivenNode , NodeId }; use async_trait :: async_trait ; struct MyEventNode { id : NodeId , } #[async_trait] impl EventDrivenNode < SensorEvent > for MyEventNode { fn id ( & self ) -> NodeId { self . id } async fn wait_for_event ( & self ) -> Result < SensorEvent , FloxideError > { // Wait for an event // ... } async fn process_event ( & self , event : SensorEvent ) -> Result < EventAction < SensorEvent > , FloxideError > { // Process the event // ... Ok ( EventAction :: Route ( \"next_node\" . to_string (), event )) } } EventSource Module \u00b6 The EventSource module provides abstractions and implementations for event sources: use floxide_event :: source ::{ EventSource , ChannelEventSource }; use tokio :: sync :: mpsc ; // Create a channel-based event source let ( tx , rx ) = mpsc :: channel :: < SensorEvent > ( 100 ); let source = ChannelEventSource :: new ( rx ); // Send an event tx . send ( SensorEvent { id : \"sensor1\" . to_string (), value : 42.0 }). await ? ; Workflow Module \u00b6 The Workflow module provides the event-driven workflow orchestration: use floxide_event :: workflow :: EventDrivenWorkflow ; // Create an event-driven workflow let mut workflow = EventDrivenWorkflow :: < SensorEvent > :: new (); // Add nodes and configure routes let node_id = workflow . add_node ( MyEventNode { id : NodeId :: new () }); workflow . set_initial_node ( node_id ); workflow . set_route ( \"next_node\" , node_id ); // Loop back to the same node // Run the workflow workflow . run (). await ? ; Key Types \u00b6 Event Trait \u00b6 The core trait for event types: pub trait Event : Clone + Send + ' static {} EventDrivenNode Trait \u00b6 The core trait for event-driven nodes: #[async_trait] pub trait EventDrivenNode < E > : Send + Sync + ' static where E : Event + Send + ' static , { /// Get the node's unique identifier fn id ( & self ) -> NodeId ; /// Wait for an event to arrive async fn wait_for_event ( & self ) -> Result < E , FloxideError > ; /// Process an event and return an action async fn process_event ( & self , event : E ) -> Result < EventAction < E > , FloxideError > ; } EventSource Trait \u00b6 The core trait for event sources: #[async_trait] pub trait EventSource < E > : Send + Sync + ' static where E : Event + Send + ' static , { /// Get the next event from the source async fn next_event ( & self ) -> Result < E , FloxideError > ; /// Check if the source has more events async fn has_more_events ( & self ) -> Result < bool , FloxideError > ; } EventAction Enum \u00b6 The action returned by event-driven nodes: pub enum EventAction < E > where E : Event + Send + ' static , { /// Route the event to another node Route ( String , E ), /// Terminate the workflow Terminate , } EventDrivenWorkflow \u00b6 The workflow orchestrator for event-driven nodes: pub struct EventDrivenWorkflow < E > where E : Event + Send + Clone + ' static , { nodes : HashMap < NodeId , Box < dyn EventDrivenNode < E >>> , routes : HashMap < String , NodeId > , initial_node : NodeId , timeout : Option < Duration > , } Usage Examples \u00b6 Basic Event-Driven Workflow \u00b6 use floxide_core :: prelude :: * ; use floxide_event :: prelude :: * ; use async_trait :: async_trait ; use std :: sync :: Arc ; use tokio :: sync :: mpsc ; // Define an event type #[derive(Clone)] struct SensorEvent { id : String , value : f64 , } impl Event for SensorEvent {} // Create an event source let ( tx , rx ) = mpsc :: channel :: < SensorEvent > ( 100 ); let source = Arc :: new ( ChannelEventSource :: new ( rx )); // Create an event-driven node struct SensorNode { id : NodeId , source : Arc < dyn EventSource < SensorEvent >> , } #[async_trait] impl EventDrivenNode < SensorEvent > for SensorNode { fn id ( & self ) -> NodeId { self . id } async fn wait_for_event ( & self ) -> Result < SensorEvent , FloxideError > { self . source . next_event (). await } async fn process_event ( & self , event : SensorEvent ) -> Result < EventAction < SensorEvent > , FloxideError > { println! ( \"Sensor {}: {}\" , event . id , event . value ); if event . value > 100.0 { Ok ( EventAction :: Terminate ) } else { Ok ( EventAction :: Route ( \"self\" . to_string (), event )) } } } // Create and run the workflow #[tokio::main] async fn main () -> Result < (), FloxideError > { let mut workflow = EventDrivenWorkflow :: < SensorEvent > :: new (); let node = SensorNode { id : NodeId :: new (), source : Arc :: clone ( & source ), }; let node_id = workflow . add_node ( node ); workflow . set_initial_node ( node_id ); workflow . set_route ( \"self\" , node_id ); // Run the workflow in a separate task let workflow_handle = tokio :: spawn ( async move { workflow . run (). await }); // Send events tx . send ( SensorEvent { id : \"sensor1\" . to_string (), value : 42.0 }). await ? ; tx . send ( SensorEvent { id : \"sensor1\" . to_string (), value : 75.0 }). await ? ; tx . send ( SensorEvent { id : \"sensor1\" . to_string (), value : 120.0 }). await ? ; // Wait for the workflow to complete workflow_handle . await ?? ; Ok (()) } Integration with Standard Workflows \u00b6 use floxide_core :: prelude :: * ; use floxide_event :: prelude :: * ; use floxide_event :: adapter :: EventDrivenNodeAdapter ; // Create an event-driven node adapter let event_node = SensorNode { id : NodeId :: new (), source : Arc :: clone ( & source ), }; let adapter = EventDrivenNodeAdapter :: new ( event_node ); // Use the adapter in a standard workflow let mut workflow = Workflow :: new (); workflow . add_node ( \"event_node\" , adapter ); workflow . set_initial_node ( \"event_node\" ); // Run the standard workflow workflow . run ( & mut context ). await ? ; Best Practices \u00b6 When using the floxide-event crate, consider these best practices: Event Design : Design event types to be small, focused, and immutable. Error Handling : Properly handle errors in event processing, especially for external event sources. Timeouts : Use timeouts to prevent workflows from hanging indefinitely. Resource Management : Ensure event sources are properly closed when no longer needed. Backpressure : Implement backpressure mechanisms for high-volume event sources. Related Documentation \u00b6 Event-Driven Workflow Pattern Event-Driven Workflow Example","title":"floxide-event"},{"location":"api/floxide-event/#floxide-event-api-reference","text":"This document provides a reference for the floxide-event crate, which extends the Floxide framework with event-driven capabilities.","title":"floxide-event API Reference"},{"location":"api/floxide-event/#overview","text":"The floxide-event crate provides extensions to the core Floxide framework for building event-driven workflows, including: Event-driven node abstractions Event source implementations Event-driven workflow orchestration Integration with standard workflows","title":"Overview"},{"location":"api/floxide-event/#core-modules","text":"","title":"Core Modules"},{"location":"api/floxide-event/#event-module","text":"The Event module provides the core event abstractions: use floxide_event :: event ::{ Event , EventAction }; // Define a custom event type #[derive(Clone)] struct SensorEvent { id : String , value : f64 , } // Implement the Event trait impl Event for SensorEvent {}","title":"Event Module"},{"location":"api/floxide-event/#eventdrivennode-module","text":"The EventDrivenNode module provides the core node abstractions for event-driven workflows: use floxide_event :: node ::{ EventDrivenNode , NodeId }; use async_trait :: async_trait ; struct MyEventNode { id : NodeId , } #[async_trait] impl EventDrivenNode < SensorEvent > for MyEventNode { fn id ( & self ) -> NodeId { self . id } async fn wait_for_event ( & self ) -> Result < SensorEvent , FloxideError > { // Wait for an event // ... } async fn process_event ( & self , event : SensorEvent ) -> Result < EventAction < SensorEvent > , FloxideError > { // Process the event // ... Ok ( EventAction :: Route ( \"next_node\" . to_string (), event )) } }","title":"EventDrivenNode Module"},{"location":"api/floxide-event/#eventsource-module","text":"The EventSource module provides abstractions and implementations for event sources: use floxide_event :: source ::{ EventSource , ChannelEventSource }; use tokio :: sync :: mpsc ; // Create a channel-based event source let ( tx , rx ) = mpsc :: channel :: < SensorEvent > ( 100 ); let source = ChannelEventSource :: new ( rx ); // Send an event tx . send ( SensorEvent { id : \"sensor1\" . to_string (), value : 42.0 }). await ? ;","title":"EventSource Module"},{"location":"api/floxide-event/#workflow-module","text":"The Workflow module provides the event-driven workflow orchestration: use floxide_event :: workflow :: EventDrivenWorkflow ; // Create an event-driven workflow let mut workflow = EventDrivenWorkflow :: < SensorEvent > :: new (); // Add nodes and configure routes let node_id = workflow . add_node ( MyEventNode { id : NodeId :: new () }); workflow . set_initial_node ( node_id ); workflow . set_route ( \"next_node\" , node_id ); // Loop back to the same node // Run the workflow workflow . run (). await ? ;","title":"Workflow Module"},{"location":"api/floxide-event/#key-types","text":"","title":"Key Types"},{"location":"api/floxide-event/#event-trait","text":"The core trait for event types: pub trait Event : Clone + Send + ' static {}","title":"Event Trait"},{"location":"api/floxide-event/#eventdrivennode-trait","text":"The core trait for event-driven nodes: #[async_trait] pub trait EventDrivenNode < E > : Send + Sync + ' static where E : Event + Send + ' static , { /// Get the node's unique identifier fn id ( & self ) -> NodeId ; /// Wait for an event to arrive async fn wait_for_event ( & self ) -> Result < E , FloxideError > ; /// Process an event and return an action async fn process_event ( & self , event : E ) -> Result < EventAction < E > , FloxideError > ; }","title":"EventDrivenNode Trait"},{"location":"api/floxide-event/#eventsource-trait","text":"The core trait for event sources: #[async_trait] pub trait EventSource < E > : Send + Sync + ' static where E : Event + Send + ' static , { /// Get the next event from the source async fn next_event ( & self ) -> Result < E , FloxideError > ; /// Check if the source has more events async fn has_more_events ( & self ) -> Result < bool , FloxideError > ; }","title":"EventSource Trait"},{"location":"api/floxide-event/#eventaction-enum","text":"The action returned by event-driven nodes: pub enum EventAction < E > where E : Event + Send + ' static , { /// Route the event to another node Route ( String , E ), /// Terminate the workflow Terminate , }","title":"EventAction Enum"},{"location":"api/floxide-event/#eventdrivenworkflow","text":"The workflow orchestrator for event-driven nodes: pub struct EventDrivenWorkflow < E > where E : Event + Send + Clone + ' static , { nodes : HashMap < NodeId , Box < dyn EventDrivenNode < E >>> , routes : HashMap < String , NodeId > , initial_node : NodeId , timeout : Option < Duration > , }","title":"EventDrivenWorkflow"},{"location":"api/floxide-event/#usage-examples","text":"","title":"Usage Examples"},{"location":"api/floxide-event/#basic-event-driven-workflow","text":"use floxide_core :: prelude :: * ; use floxide_event :: prelude :: * ; use async_trait :: async_trait ; use std :: sync :: Arc ; use tokio :: sync :: mpsc ; // Define an event type #[derive(Clone)] struct SensorEvent { id : String , value : f64 , } impl Event for SensorEvent {} // Create an event source let ( tx , rx ) = mpsc :: channel :: < SensorEvent > ( 100 ); let source = Arc :: new ( ChannelEventSource :: new ( rx )); // Create an event-driven node struct SensorNode { id : NodeId , source : Arc < dyn EventSource < SensorEvent >> , } #[async_trait] impl EventDrivenNode < SensorEvent > for SensorNode { fn id ( & self ) -> NodeId { self . id } async fn wait_for_event ( & self ) -> Result < SensorEvent , FloxideError > { self . source . next_event (). await } async fn process_event ( & self , event : SensorEvent ) -> Result < EventAction < SensorEvent > , FloxideError > { println! ( \"Sensor {}: {}\" , event . id , event . value ); if event . value > 100.0 { Ok ( EventAction :: Terminate ) } else { Ok ( EventAction :: Route ( \"self\" . to_string (), event )) } } } // Create and run the workflow #[tokio::main] async fn main () -> Result < (), FloxideError > { let mut workflow = EventDrivenWorkflow :: < SensorEvent > :: new (); let node = SensorNode { id : NodeId :: new (), source : Arc :: clone ( & source ), }; let node_id = workflow . add_node ( node ); workflow . set_initial_node ( node_id ); workflow . set_route ( \"self\" , node_id ); // Run the workflow in a separate task let workflow_handle = tokio :: spawn ( async move { workflow . run (). await }); // Send events tx . send ( SensorEvent { id : \"sensor1\" . to_string (), value : 42.0 }). await ? ; tx . send ( SensorEvent { id : \"sensor1\" . to_string (), value : 75.0 }). await ? ; tx . send ( SensorEvent { id : \"sensor1\" . to_string (), value : 120.0 }). await ? ; // Wait for the workflow to complete workflow_handle . await ?? ; Ok (()) }","title":"Basic Event-Driven Workflow"},{"location":"api/floxide-event/#integration-with-standard-workflows","text":"use floxide_core :: prelude :: * ; use floxide_event :: prelude :: * ; use floxide_event :: adapter :: EventDrivenNodeAdapter ; // Create an event-driven node adapter let event_node = SensorNode { id : NodeId :: new (), source : Arc :: clone ( & source ), }; let adapter = EventDrivenNodeAdapter :: new ( event_node ); // Use the adapter in a standard workflow let mut workflow = Workflow :: new (); workflow . add_node ( \"event_node\" , adapter ); workflow . set_initial_node ( \"event_node\" ); // Run the standard workflow workflow . run ( & mut context ). await ? ;","title":"Integration with Standard Workflows"},{"location":"api/floxide-event/#best-practices","text":"When using the floxide-event crate, consider these best practices: Event Design : Design event types to be small, focused, and immutable. Error Handling : Properly handle errors in event processing, especially for external event sources. Timeouts : Use timeouts to prevent workflows from hanging indefinitely. Resource Management : Ensure event sources are properly closed when no longer needed. Backpressure : Implement backpressure mechanisms for high-volume event sources.","title":"Best Practices"},{"location":"api/floxide-event/#related-documentation","text":"Event-Driven Workflow Pattern Event-Driven Workflow Example","title":"Related Documentation"},{"location":"api/floxide-longrunning/","text":"floxide-longrunning API Reference \u00b6 The floxide-longrunning crate provides support for long-running operations in the Floxide framework. Overview \u00b6 This crate implements patterns and utilities for handling long-running operations within workflows. It provides: Long-running node types Progress tracking Cancellation support Resource cleanup Key Types \u00b6 LongRunningNode \u00b6 pub trait LongRunningNode < C , A > : Send + Sync { async fn start ( & self , context : & mut C ) -> Result < (), FloxideError > ; async fn check_status ( & self , context : & mut C ) -> Result < LongRunningStatus , FloxideError > ; async fn cleanup ( & self , context : & mut C ) -> Result < A , FloxideError > ; } The LongRunningNode trait defines the interface for nodes that perform long-running operations. LongRunningStatus \u00b6 pub enum LongRunningStatus { Running ( Progress ), Complete , Failed ( FloxideError ), } LongRunningStatus represents the current state of a long-running operation. Progress \u00b6 pub struct Progress { pub percent : f64 , pub message : String , } Progress provides information about the progress of a long-running operation. Usage Example \u00b6 use floxide_longrunning ::{ LongRunningNode , LongRunningStatus , Progress }; struct DataProcessingNode ; impl LongRunningNode < ProcessingContext , ProcessingAction > for DataProcessingNode { async fn start ( & self , context : & mut ProcessingContext ) -> Result < (), FloxideError > { // Initialize the long-running operation context . start_processing (); Ok (()) } async fn check_status ( & self , context : & mut ProcessingContext ) -> Result < LongRunningStatus , FloxideError > { let progress = context . get_progress (); if progress . percent < 100.0 { Ok ( LongRunningStatus :: Running ( progress )) } else { Ok ( LongRunningStatus :: Complete ) } } async fn cleanup ( & self , context : & mut ProcessingContext ) -> Result < ProcessingAction , FloxideError > { // Clean up resources and return final action context . cleanup (); Ok ( ProcessingAction :: Complete ) } } Error Handling \u00b6 The crate uses the standard FloxideError type for error handling. All operations that can fail return a Result<T, FloxideError> . Best Practices \u00b6 Implement proper resource cleanup Provide meaningful progress updates Handle cancellation gracefully Consider timeout mechanisms Implement proper error recovery See Also \u00b6 Long-Running Node Implementation ADR Node Lifecycle Methods","title":"floxide-longrunning"},{"location":"api/floxide-longrunning/#floxide-longrunning-api-reference","text":"The floxide-longrunning crate provides support for long-running operations in the Floxide framework.","title":"floxide-longrunning API Reference"},{"location":"api/floxide-longrunning/#overview","text":"This crate implements patterns and utilities for handling long-running operations within workflows. It provides: Long-running node types Progress tracking Cancellation support Resource cleanup","title":"Overview"},{"location":"api/floxide-longrunning/#key-types","text":"","title":"Key Types"},{"location":"api/floxide-longrunning/#longrunningnode","text":"pub trait LongRunningNode < C , A > : Send + Sync { async fn start ( & self , context : & mut C ) -> Result < (), FloxideError > ; async fn check_status ( & self , context : & mut C ) -> Result < LongRunningStatus , FloxideError > ; async fn cleanup ( & self , context : & mut C ) -> Result < A , FloxideError > ; } The LongRunningNode trait defines the interface for nodes that perform long-running operations.","title":"LongRunningNode"},{"location":"api/floxide-longrunning/#longrunningstatus","text":"pub enum LongRunningStatus { Running ( Progress ), Complete , Failed ( FloxideError ), } LongRunningStatus represents the current state of a long-running operation.","title":"LongRunningStatus"},{"location":"api/floxide-longrunning/#progress","text":"pub struct Progress { pub percent : f64 , pub message : String , } Progress provides information about the progress of a long-running operation.","title":"Progress"},{"location":"api/floxide-longrunning/#usage-example","text":"use floxide_longrunning ::{ LongRunningNode , LongRunningStatus , Progress }; struct DataProcessingNode ; impl LongRunningNode < ProcessingContext , ProcessingAction > for DataProcessingNode { async fn start ( & self , context : & mut ProcessingContext ) -> Result < (), FloxideError > { // Initialize the long-running operation context . start_processing (); Ok (()) } async fn check_status ( & self , context : & mut ProcessingContext ) -> Result < LongRunningStatus , FloxideError > { let progress = context . get_progress (); if progress . percent < 100.0 { Ok ( LongRunningStatus :: Running ( progress )) } else { Ok ( LongRunningStatus :: Complete ) } } async fn cleanup ( & self , context : & mut ProcessingContext ) -> Result < ProcessingAction , FloxideError > { // Clean up resources and return final action context . cleanup (); Ok ( ProcessingAction :: Complete ) } }","title":"Usage Example"},{"location":"api/floxide-longrunning/#error-handling","text":"The crate uses the standard FloxideError type for error handling. All operations that can fail return a Result<T, FloxideError> .","title":"Error Handling"},{"location":"api/floxide-longrunning/#best-practices","text":"Implement proper resource cleanup Provide meaningful progress updates Handle cancellation gracefully Consider timeout mechanisms Implement proper error recovery","title":"Best Practices"},{"location":"api/floxide-longrunning/#see-also","text":"Long-Running Node Implementation ADR Node Lifecycle Methods","title":"See Also"},{"location":"api/floxide-reactive/","text":"floxide-reactive API Reference \u00b6 The floxide-reactive crate provides support for reactive patterns in the Floxide framework, enabling nodes to respond to changes in external data sources using a stream-based approach. Core Types \u00b6 ReactiveNode \u00b6 #[async_trait] pub trait ReactiveNode < Change , Context , Action > : Send + Sync where Change : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static + Debug , { /// Set up a stream of changes to watch async fn watch ( & self ) -> Result < impl Stream < Item = Change > + Send , FloxideError > ; /// React to a detected change async fn react_to_change ( & self , change : Change , context : & mut Context , ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; } The ReactiveNode trait is the core abstraction for reactive patterns, providing: - Stream-based change detection - Context-aware change handling - Integration with the workflow engine ReactiveError \u00b6 pub enum ReactiveError { WatchError ( String ), StreamClosed , ConnectionError ( String ), ResourceNotFound ( String ), } Specialized error types for reactive operations, handling: - Resource watching failures - Stream lifecycle issues - Connection problems - Resource availability Built-in Implementations \u00b6 FileWatcherNode \u00b6 pub struct FileWatcherNode < Context , Action > where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { // ... } A specialized node for file system monitoring: - File modification detection - Configurable polling intervals - Custom change handlers - Metadata tracking CustomReactiveNode \u00b6 pub struct CustomReactiveNode < Change , Context , Action , WatchFn , ReactFn > where Change : Send + Sync + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { // ... } A flexible implementation for custom reactive patterns: - User-defined watch functions - Custom change reaction logic - Configurable node identity Stream Management \u00b6 ReactiveNodeAdapter \u00b6 pub struct ReactiveNodeAdapter < R , Change , Context , Action > where R : ReactiveNode < Change , Context , Action > , { // ... } Adapts reactive nodes to the standard Node interface: - Buffering with backpressure handling - Stream lifecycle management - Background processing Buffering and Backpressure \u00b6 The implementation includes configurable buffering to handle rapid changes: // Configure buffer size let node = reactive_node . with_buffer_size ( 100 ) . with_backoff ( Duration :: from_millis ( 100 )); Usage Examples \u00b6 File Watching \u00b6 use floxide_reactive ::{ FileWatcherNode , FileChange }; let watcher = FileWatcherNode :: new ( \"config.toml\" ) . with_poll_interval ( Duration :: from_secs ( 5 )) . with_change_handler ( | change : FileChange , ctx : & mut Context | async move { match change { FileChange :: Modified ( path ) => { println! ( \"File modified: {}\" , path . display ()); Ok ( DefaultAction :: Next ) } FileChange :: NotFound => { println! ( \"File not found\" ); Ok ( DefaultAction :: Stop ) } } }); let mut workflow = Workflow :: new ( watcher ); workflow . run ( context ). await ? ; Custom Reactive Pattern \u00b6 use floxide_reactive :: CustomReactiveNode ; let node = CustomReactiveNode :: new ( // Watch function returns a stream of changes || async { let ( tx , rx ) = mpsc :: channel ( 100 ); // Set up change detection... Ok ( ReceiverStream :: new ( rx ). boxed ()) }, // React function handles changes | change , ctx | async move { println! ( \"Processing change: {:?}\" , change ); Ok ( DefaultAction :: Next ) } ); Best Practices \u00b6 Stream Management Configure appropriate buffer sizes based on change frequency Implement proper backpressure handling Clean up resources when streams are dropped Handle stream closure gracefully Error Handling Handle transient failures with retries Provide clear error context Clean up resources on error Log relevant error details Resource Usage Monitor memory usage with large streams Use appropriate polling intervals Implement proper cleanup Consider system resource limits Testing Test stream handling Verify error scenarios Check resource cleanup Test backpressure handling See Also \u00b6 ADR-0017: ReactiveNode Implementation Reactive Node Example Event-Driven Architecture","title":"floxide-reactive"},{"location":"api/floxide-reactive/#floxide-reactive-api-reference","text":"The floxide-reactive crate provides support for reactive patterns in the Floxide framework, enabling nodes to respond to changes in external data sources using a stream-based approach.","title":"floxide-reactive API Reference"},{"location":"api/floxide-reactive/#core-types","text":"","title":"Core Types"},{"location":"api/floxide-reactive/#reactivenode","text":"#[async_trait] pub trait ReactiveNode < Change , Context , Action > : Send + Sync where Change : Send + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static + Debug , { /// Set up a stream of changes to watch async fn watch ( & self ) -> Result < impl Stream < Item = Change > + Send , FloxideError > ; /// React to a detected change async fn react_to_change ( & self , change : Change , context : & mut Context , ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; } The ReactiveNode trait is the core abstraction for reactive patterns, providing: - Stream-based change detection - Context-aware change handling - Integration with the workflow engine","title":"ReactiveNode"},{"location":"api/floxide-reactive/#reactiveerror","text":"pub enum ReactiveError { WatchError ( String ), StreamClosed , ConnectionError ( String ), ResourceNotFound ( String ), } Specialized error types for reactive operations, handling: - Resource watching failures - Stream lifecycle issues - Connection problems - Resource availability","title":"ReactiveError"},{"location":"api/floxide-reactive/#built-in-implementations","text":"","title":"Built-in Implementations"},{"location":"api/floxide-reactive/#filewatchernode","text":"pub struct FileWatcherNode < Context , Action > where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { // ... } A specialized node for file system monitoring: - File modification detection - Configurable polling intervals - Custom change handlers - Metadata tracking","title":"FileWatcherNode"},{"location":"api/floxide-reactive/#customreactivenode","text":"pub struct CustomReactiveNode < Change , Context , Action , WatchFn , ReactFn > where Change : Send + Sync + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , { // ... } A flexible implementation for custom reactive patterns: - User-defined watch functions - Custom change reaction logic - Configurable node identity","title":"CustomReactiveNode"},{"location":"api/floxide-reactive/#stream-management","text":"","title":"Stream Management"},{"location":"api/floxide-reactive/#reactivenodeadapter","text":"pub struct ReactiveNodeAdapter < R , Change , Context , Action > where R : ReactiveNode < Change , Context , Action > , { // ... } Adapts reactive nodes to the standard Node interface: - Buffering with backpressure handling - Stream lifecycle management - Background processing","title":"ReactiveNodeAdapter"},{"location":"api/floxide-reactive/#buffering-and-backpressure","text":"The implementation includes configurable buffering to handle rapid changes: // Configure buffer size let node = reactive_node . with_buffer_size ( 100 ) . with_backoff ( Duration :: from_millis ( 100 ));","title":"Buffering and Backpressure"},{"location":"api/floxide-reactive/#usage-examples","text":"","title":"Usage Examples"},{"location":"api/floxide-reactive/#file-watching","text":"use floxide_reactive ::{ FileWatcherNode , FileChange }; let watcher = FileWatcherNode :: new ( \"config.toml\" ) . with_poll_interval ( Duration :: from_secs ( 5 )) . with_change_handler ( | change : FileChange , ctx : & mut Context | async move { match change { FileChange :: Modified ( path ) => { println! ( \"File modified: {}\" , path . display ()); Ok ( DefaultAction :: Next ) } FileChange :: NotFound => { println! ( \"File not found\" ); Ok ( DefaultAction :: Stop ) } } }); let mut workflow = Workflow :: new ( watcher ); workflow . run ( context ). await ? ;","title":"File Watching"},{"location":"api/floxide-reactive/#custom-reactive-pattern","text":"use floxide_reactive :: CustomReactiveNode ; let node = CustomReactiveNode :: new ( // Watch function returns a stream of changes || async { let ( tx , rx ) = mpsc :: channel ( 100 ); // Set up change detection... Ok ( ReceiverStream :: new ( rx ). boxed ()) }, // React function handles changes | change , ctx | async move { println! ( \"Processing change: {:?}\" , change ); Ok ( DefaultAction :: Next ) } );","title":"Custom Reactive Pattern"},{"location":"api/floxide-reactive/#best-practices","text":"Stream Management Configure appropriate buffer sizes based on change frequency Implement proper backpressure handling Clean up resources when streams are dropped Handle stream closure gracefully Error Handling Handle transient failures with retries Provide clear error context Clean up resources on error Log relevant error details Resource Usage Monitor memory usage with large streams Use appropriate polling intervals Implement proper cleanup Consider system resource limits Testing Test stream handling Verify error scenarios Check resource cleanup Test backpressure handling","title":"Best Practices"},{"location":"api/floxide-reactive/#see-also","text":"ADR-0017: ReactiveNode Implementation Reactive Node Example Event-Driven Architecture","title":"See Also"},{"location":"api/floxide-timer/","text":"floxide-timer API Reference \u00b6 The floxide-timer crate provides time-based workflow capabilities through the TimerNode trait and various schedule implementations, as defined in ADR-0021 . Core Types \u00b6 Schedule \u00b6 pub enum Schedule { /// Execute once at a specific time Once ( DateTime < Utc > ), /// Execute repeatedly at fixed intervals Interval ( Duration ), /// Execute daily at a specified hour and minute Daily ( u32 , u32 ), /// Execute weekly on a specified day and time Weekly ( Weekday , u32 , u32 ), /// Execute monthly on a specified day and time Monthly ( u32 , u32 , u32 ), /// Execute according to a cron expression (future) Cron ( String ), } The Schedule enum defines when a timer node should execute, supporting: - One-time execution at specific times - Fixed interval execution - Daily/Weekly/Monthly scheduling - Future support for cron expressions TimerNode \u00b6 #[async_trait] pub trait TimerNode < Context , Action > : Send + Sync { /// Get the node's schedule fn schedule ( & self ) -> Schedule ; /// Execute when the schedule triggers async fn execute_on_schedule ( & self , ctx : & mut Context ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; } The TimerNode trait is the core abstraction for timer-based execution. Built-in Implementations \u00b6 SimpleTimer \u00b6 pub struct SimpleTimer < F > { schedule : Schedule , action : F , id : NodeId , } impl < F > SimpleTimer < F > { pub fn new ( schedule : Schedule , action : F ) -> Self ; pub fn with_id ( id : impl Into < String > , schedule : Schedule , action : F ) -> Self ; } A basic timer implementation that executes a function based on a schedule: - Configurable execution schedule - Custom action function - Optional custom node identity TimerWorkflow \u00b6 pub struct TimerWorkflow < Context , Action > { nodes : HashMap < NodeId , Arc < dyn TimerNode < Context , Action >>> , routes : HashMap < ( NodeId , Action ), NodeId > , termination_action : Action , } Orchestrates execution of multiple timer nodes: - Node routing based on actions - Workflow-level termination conditions - Execution state management TimerNodeAdapter \u00b6 pub struct TimerNodeAdapter < Context , Action > { node : Arc < dyn TimerNode < Context , Action >> , execute_immediately : bool , id : NodeId , } Adapts a TimerNode to be used as a standard Node : - Optional immediate execution - Schedule-based processing - Standard node interface compatibility NestedTimerWorkflow \u00b6 pub struct NestedTimerWorkflow < Context , Action > { workflow : Arc < TimerWorkflow < Context , Action >> , complete_action : Action , id : NodeId , } Allows using a timer workflow as a standard node: - Workflow nesting - Completion action configuration - Workflow isolation Usage Examples \u00b6 Simple Timer \u00b6 use floxide_timer ::{ SimpleTimer , Schedule }; use chrono ::{ Utc , Duration }; // Create a timer that runs every minute let timer = SimpleTimer :: new ( Schedule :: Interval ( Duration :: minutes ( 1 )), | ctx : & mut Context | async move { println! ( \"Timer executed at: {}\" , Utc :: now ()); Ok ( DefaultAction :: Next ) } ); // Use in a workflow let mut workflow = Workflow :: new ( timer ); workflow . run ( context ). await ? ; Timer Workflow \u00b6 use floxide_timer :: TimerWorkflow ; // Create nodes let daily_report = Arc :: new ( SimpleTimer :: new ( Schedule :: Daily ( 9 , 0 ), // 9:00 AM generate_daily_report )); let weekly_cleanup = Arc :: new ( SimpleTimer :: new ( Schedule :: Weekly ( Weekday :: Sun , 0 , 0 ), // Sunday midnight perform_weekly_cleanup )); // Create workflow let mut workflow = TimerWorkflow :: new ( daily_report . clone (), DefaultAction :: Stop ); // Add nodes and routes workflow . add_node ( weekly_cleanup . clone ()); workflow . set_route ( & daily_report . id (), DefaultAction :: Next , & weekly_cleanup . id () ); // Execute workflow . execute ( & mut context ) ? ; Best Practices \u00b6 Schedule Configuration Choose appropriate intervals for your use case Consider timezone implications Handle daylight savings time Use UTC for consistency Resource Management Clean up timers when done Handle cancellation properly Monitor system load Consider resource limits Error Handling Handle missed executions Implement retry logic Log timing issues Handle edge cases Testing Test schedule calculations Verify timer behavior Check workflow routing Test error scenarios See Also \u00b6 ADR-0021: Timer Node Implementation Timer Node Example Event-Driven Architecture","title":"floxide-timer"},{"location":"api/floxide-timer/#floxide-timer-api-reference","text":"The floxide-timer crate provides time-based workflow capabilities through the TimerNode trait and various schedule implementations, as defined in ADR-0021 .","title":"floxide-timer API Reference"},{"location":"api/floxide-timer/#core-types","text":"","title":"Core Types"},{"location":"api/floxide-timer/#schedule","text":"pub enum Schedule { /// Execute once at a specific time Once ( DateTime < Utc > ), /// Execute repeatedly at fixed intervals Interval ( Duration ), /// Execute daily at a specified hour and minute Daily ( u32 , u32 ), /// Execute weekly on a specified day and time Weekly ( Weekday , u32 , u32 ), /// Execute monthly on a specified day and time Monthly ( u32 , u32 , u32 ), /// Execute according to a cron expression (future) Cron ( String ), } The Schedule enum defines when a timer node should execute, supporting: - One-time execution at specific times - Fixed interval execution - Daily/Weekly/Monthly scheduling - Future support for cron expressions","title":"Schedule"},{"location":"api/floxide-timer/#timernode","text":"#[async_trait] pub trait TimerNode < Context , Action > : Send + Sync { /// Get the node's schedule fn schedule ( & self ) -> Schedule ; /// Execute when the schedule triggers async fn execute_on_schedule ( & self , ctx : & mut Context ) -> Result < Action , FloxideError > ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; } The TimerNode trait is the core abstraction for timer-based execution.","title":"TimerNode"},{"location":"api/floxide-timer/#built-in-implementations","text":"","title":"Built-in Implementations"},{"location":"api/floxide-timer/#simpletimer","text":"pub struct SimpleTimer < F > { schedule : Schedule , action : F , id : NodeId , } impl < F > SimpleTimer < F > { pub fn new ( schedule : Schedule , action : F ) -> Self ; pub fn with_id ( id : impl Into < String > , schedule : Schedule , action : F ) -> Self ; } A basic timer implementation that executes a function based on a schedule: - Configurable execution schedule - Custom action function - Optional custom node identity","title":"SimpleTimer"},{"location":"api/floxide-timer/#timerworkflow","text":"pub struct TimerWorkflow < Context , Action > { nodes : HashMap < NodeId , Arc < dyn TimerNode < Context , Action >>> , routes : HashMap < ( NodeId , Action ), NodeId > , termination_action : Action , } Orchestrates execution of multiple timer nodes: - Node routing based on actions - Workflow-level termination conditions - Execution state management","title":"TimerWorkflow"},{"location":"api/floxide-timer/#timernodeadapter","text":"pub struct TimerNodeAdapter < Context , Action > { node : Arc < dyn TimerNode < Context , Action >> , execute_immediately : bool , id : NodeId , } Adapts a TimerNode to be used as a standard Node : - Optional immediate execution - Schedule-based processing - Standard node interface compatibility","title":"TimerNodeAdapter"},{"location":"api/floxide-timer/#nestedtimerworkflow","text":"pub struct NestedTimerWorkflow < Context , Action > { workflow : Arc < TimerWorkflow < Context , Action >> , complete_action : Action , id : NodeId , } Allows using a timer workflow as a standard node: - Workflow nesting - Completion action configuration - Workflow isolation","title":"NestedTimerWorkflow"},{"location":"api/floxide-timer/#usage-examples","text":"","title":"Usage Examples"},{"location":"api/floxide-timer/#simple-timer","text":"use floxide_timer ::{ SimpleTimer , Schedule }; use chrono ::{ Utc , Duration }; // Create a timer that runs every minute let timer = SimpleTimer :: new ( Schedule :: Interval ( Duration :: minutes ( 1 )), | ctx : & mut Context | async move { println! ( \"Timer executed at: {}\" , Utc :: now ()); Ok ( DefaultAction :: Next ) } ); // Use in a workflow let mut workflow = Workflow :: new ( timer ); workflow . run ( context ). await ? ;","title":"Simple Timer"},{"location":"api/floxide-timer/#timer-workflow","text":"use floxide_timer :: TimerWorkflow ; // Create nodes let daily_report = Arc :: new ( SimpleTimer :: new ( Schedule :: Daily ( 9 , 0 ), // 9:00 AM generate_daily_report )); let weekly_cleanup = Arc :: new ( SimpleTimer :: new ( Schedule :: Weekly ( Weekday :: Sun , 0 , 0 ), // Sunday midnight perform_weekly_cleanup )); // Create workflow let mut workflow = TimerWorkflow :: new ( daily_report . clone (), DefaultAction :: Stop ); // Add nodes and routes workflow . add_node ( weekly_cleanup . clone ()); workflow . set_route ( & daily_report . id (), DefaultAction :: Next , & weekly_cleanup . id () ); // Execute workflow . execute ( & mut context ) ? ;","title":"Timer Workflow"},{"location":"api/floxide-timer/#best-practices","text":"Schedule Configuration Choose appropriate intervals for your use case Consider timezone implications Handle daylight savings time Use UTC for consistency Resource Management Clean up timers when done Handle cancellation properly Monitor system load Consider resource limits Error Handling Handle missed executions Implement retry logic Log timing issues Handle edge cases Testing Test schedule calculations Verify timer behavior Check workflow routing Test error scenarios","title":"Best Practices"},{"location":"api/floxide-timer/#see-also","text":"ADR-0021: Timer Node Implementation Timer Node Example Event-Driven Architecture","title":"See Also"},{"location":"api/floxide-transform/","text":"floxide-transform API Reference \u00b6 This document provides a reference for the floxide-transform crate, which extends the core Floxide framework with transformation and asynchronous capabilities. Overview \u00b6 The floxide-transform crate provides extensions to the core Floxide framework for building more complex workflows, including: Asynchronous node execution Transformation operations Batch processing capabilities Advanced workflow patterns Core Modules \u00b6 Transform Module \u00b6 The Transform module provides utilities for transforming data within workflows: use floxide_transform :: transform ::{ TransformNode , Transformer }; // Create a transformer function let uppercase_transformer = | input : String | -> Result < String , FloxideError > { Ok ( input . to_uppercase ()) }; // Create a transform node let transform_node = TransformNode :: new ( uppercase_transformer ); Async Module \u00b6 The Async module provides asynchronous execution capabilities: use floxide_transform :: async_ext ::{ AsyncNode , AsyncFlow }; use async_trait :: async_trait ; struct MyAsyncNode ; #[async_trait] impl AsyncNode for MyAsyncNode { type Context = MyContext ; async fn exec_async ( & self , ctx : & mut Self :: Context ) -> Result < (), FloxideError > { // Perform async operations tokio :: time :: sleep ( tokio :: time :: Duration :: from_millis ( 100 )). await ; Ok (()) } } // Create an async flow let async_flow = AsyncFlow :: new ( MyAsyncNode ); Batch Module \u00b6 The Batch module provides utilities for batch processing: use floxide_transform :: batch ::{ BatchNode , BatchFlow , BatchContext }; struct MyBatchNode ; impl Node for MyBatchNode { type Context = MyBatchContext ; fn exec ( & self , ctx : & mut Self :: Context ) -> Result < (), FloxideError > { // Process a single item Ok (()) } } impl BatchNode < MyItem > for MyBatchNode {} // Create a batch flow with concurrency limit of 4 let batch_flow = BatchFlow :: new ( MyBatchNode , 4 ); Key Types \u00b6 TransformNode \u00b6 A node that applies a transformation function to input data: pub struct TransformNode < F , I , O > where F : Fn ( I ) -> Result < O , FloxideError > + Send + Sync + ' static , I : Send + ' static , O : Send + ' static , { transformer : F , _phantom : PhantomData < ( I , O ) > , } impl < F , I , O > TransformNode < F , I , O > where F : Fn ( I ) -> Result < O , FloxideError > + Send + Sync + ' static , I : Send + ' static , O : Send + ' static , { pub fn new ( transformer : F ) -> Self { Self { transformer , _phantom : PhantomData , } } } AsyncFlow \u00b6 A wrapper for executing nodes asynchronously: pub struct AsyncFlow < N > where N : AsyncNode , { node : N , } impl < N > AsyncFlow < N > where N : AsyncNode , { pub fn new ( node : N ) -> Self { Self { node } } pub async fn execute ( & self , mut ctx : N :: Context ) -> Result < N :: Context , FloxideError > { self . node . exec_async ( & mut ctx ). await ? ; Ok ( ctx ) } } BatchFlow \u00b6 A wrapper for executing batch processing nodes: pub struct BatchFlow < T , N > where N : BatchNode < T > , { node : N , concurrency : usize , _phantom : PhantomData < T > , } impl < T , N > BatchFlow < T , N > where N : BatchNode < T > , { pub fn new ( node : N , concurrency : usize ) -> Self { Self { node , concurrency , _phantom : PhantomData , } } pub fn execute < C > ( & self , mut context : C ) -> Result < C , FloxideError > where C : BatchContext < T > , { self . node . process_batch ( & mut context , self . concurrency ) ? ; Ok ( context ) } } Usage Examples \u00b6 Basic Transformation \u00b6 use floxide_core :: prelude :: * ; use floxide_transform :: prelude :: * ; // Define a state type struct MyState { value : String , } // Create a context let state = MyState { value : \"hello\" . to_string () }; let mut context = Context :: new ( state ); // Create a transform node let transform_node = TransformNode :: new ( | state : & mut MyState | -> Result < (), FloxideError > { state . value = state . value . to_uppercase (); Ok (()) }); // Execute the node transform_node . exec ( & mut context ) ? ; // Check the result assert_eq! ( context . state (). value , \"HELLO\" ); Async Execution \u00b6 use floxide_core :: prelude :: * ; use floxide_transform :: prelude :: * ; use async_trait :: async_trait ; // Define an async node struct DelayedTransformNode ; #[async_trait] impl AsyncNode for DelayedTransformNode { type Context = Context < MyState > ; async fn exec_async ( & self , ctx : & mut Self :: Context ) -> Result < (), FloxideError > { // Simulate a delay tokio :: time :: sleep ( tokio :: time :: Duration :: from_millis ( 100 )). await ; // Transform the state ctx . state_mut (). value = ctx . state (). value . to_uppercase (); Ok (()) } } // Create and execute the async flow #[tokio::main] async fn main () -> Result < (), FloxideError > { let state = MyState { value : \"hello\" . to_string () }; let context = Context :: new ( state ); let async_flow = AsyncFlow :: new ( DelayedTransformNode ); let result_context = async_flow . execute ( context ). await ? ; assert_eq! ( result_context . state (). value , \"HELLO\" ); Ok (()) } Best Practices \u00b6 When using the floxide-transform crate, consider these best practices: Use Appropriate Concurrency Limits : Set batch processing concurrency limits based on your system's capabilities. Handle Async Errors : Properly handle errors in async code using the ? operator or explicit error handling. Compose Transformations : Chain multiple transform nodes for complex data transformations. Leverage Type Safety : Use Rust's type system to ensure type safety in transformations. Consider Resource Usage : Be mindful of resource usage in batch processing operations. Related Documentation \u00b6 Async Runtime Selection Batch Processing Implementation Node Lifecycle Methods Batch Processing Example","title":"floxide-transform"},{"location":"api/floxide-transform/#floxide-transform-api-reference","text":"This document provides a reference for the floxide-transform crate, which extends the core Floxide framework with transformation and asynchronous capabilities.","title":"floxide-transform API Reference"},{"location":"api/floxide-transform/#overview","text":"The floxide-transform crate provides extensions to the core Floxide framework for building more complex workflows, including: Asynchronous node execution Transformation operations Batch processing capabilities Advanced workflow patterns","title":"Overview"},{"location":"api/floxide-transform/#core-modules","text":"","title":"Core Modules"},{"location":"api/floxide-transform/#transform-module","text":"The Transform module provides utilities for transforming data within workflows: use floxide_transform :: transform ::{ TransformNode , Transformer }; // Create a transformer function let uppercase_transformer = | input : String | -> Result < String , FloxideError > { Ok ( input . to_uppercase ()) }; // Create a transform node let transform_node = TransformNode :: new ( uppercase_transformer );","title":"Transform Module"},{"location":"api/floxide-transform/#async-module","text":"The Async module provides asynchronous execution capabilities: use floxide_transform :: async_ext ::{ AsyncNode , AsyncFlow }; use async_trait :: async_trait ; struct MyAsyncNode ; #[async_trait] impl AsyncNode for MyAsyncNode { type Context = MyContext ; async fn exec_async ( & self , ctx : & mut Self :: Context ) -> Result < (), FloxideError > { // Perform async operations tokio :: time :: sleep ( tokio :: time :: Duration :: from_millis ( 100 )). await ; Ok (()) } } // Create an async flow let async_flow = AsyncFlow :: new ( MyAsyncNode );","title":"Async Module"},{"location":"api/floxide-transform/#batch-module","text":"The Batch module provides utilities for batch processing: use floxide_transform :: batch ::{ BatchNode , BatchFlow , BatchContext }; struct MyBatchNode ; impl Node for MyBatchNode { type Context = MyBatchContext ; fn exec ( & self , ctx : & mut Self :: Context ) -> Result < (), FloxideError > { // Process a single item Ok (()) } } impl BatchNode < MyItem > for MyBatchNode {} // Create a batch flow with concurrency limit of 4 let batch_flow = BatchFlow :: new ( MyBatchNode , 4 );","title":"Batch Module"},{"location":"api/floxide-transform/#key-types","text":"","title":"Key Types"},{"location":"api/floxide-transform/#transformnode","text":"A node that applies a transformation function to input data: pub struct TransformNode < F , I , O > where F : Fn ( I ) -> Result < O , FloxideError > + Send + Sync + ' static , I : Send + ' static , O : Send + ' static , { transformer : F , _phantom : PhantomData < ( I , O ) > , } impl < F , I , O > TransformNode < F , I , O > where F : Fn ( I ) -> Result < O , FloxideError > + Send + Sync + ' static , I : Send + ' static , O : Send + ' static , { pub fn new ( transformer : F ) -> Self { Self { transformer , _phantom : PhantomData , } } }","title":"TransformNode"},{"location":"api/floxide-transform/#asyncflow","text":"A wrapper for executing nodes asynchronously: pub struct AsyncFlow < N > where N : AsyncNode , { node : N , } impl < N > AsyncFlow < N > where N : AsyncNode , { pub fn new ( node : N ) -> Self { Self { node } } pub async fn execute ( & self , mut ctx : N :: Context ) -> Result < N :: Context , FloxideError > { self . node . exec_async ( & mut ctx ). await ? ; Ok ( ctx ) } }","title":"AsyncFlow"},{"location":"api/floxide-transform/#batchflow","text":"A wrapper for executing batch processing nodes: pub struct BatchFlow < T , N > where N : BatchNode < T > , { node : N , concurrency : usize , _phantom : PhantomData < T > , } impl < T , N > BatchFlow < T , N > where N : BatchNode < T > , { pub fn new ( node : N , concurrency : usize ) -> Self { Self { node , concurrency , _phantom : PhantomData , } } pub fn execute < C > ( & self , mut context : C ) -> Result < C , FloxideError > where C : BatchContext < T > , { self . node . process_batch ( & mut context , self . concurrency ) ? ; Ok ( context ) } }","title":"BatchFlow"},{"location":"api/floxide-transform/#usage-examples","text":"","title":"Usage Examples"},{"location":"api/floxide-transform/#basic-transformation","text":"use floxide_core :: prelude :: * ; use floxide_transform :: prelude :: * ; // Define a state type struct MyState { value : String , } // Create a context let state = MyState { value : \"hello\" . to_string () }; let mut context = Context :: new ( state ); // Create a transform node let transform_node = TransformNode :: new ( | state : & mut MyState | -> Result < (), FloxideError > { state . value = state . value . to_uppercase (); Ok (()) }); // Execute the node transform_node . exec ( & mut context ) ? ; // Check the result assert_eq! ( context . state (). value , \"HELLO\" );","title":"Basic Transformation"},{"location":"api/floxide-transform/#async-execution","text":"use floxide_core :: prelude :: * ; use floxide_transform :: prelude :: * ; use async_trait :: async_trait ; // Define an async node struct DelayedTransformNode ; #[async_trait] impl AsyncNode for DelayedTransformNode { type Context = Context < MyState > ; async fn exec_async ( & self , ctx : & mut Self :: Context ) -> Result < (), FloxideError > { // Simulate a delay tokio :: time :: sleep ( tokio :: time :: Duration :: from_millis ( 100 )). await ; // Transform the state ctx . state_mut (). value = ctx . state (). value . to_uppercase (); Ok (()) } } // Create and execute the async flow #[tokio::main] async fn main () -> Result < (), FloxideError > { let state = MyState { value : \"hello\" . to_string () }; let context = Context :: new ( state ); let async_flow = AsyncFlow :: new ( DelayedTransformNode ); let result_context = async_flow . execute ( context ). await ? ; assert_eq! ( result_context . state (). value , \"HELLO\" ); Ok (()) }","title":"Async Execution"},{"location":"api/floxide-transform/#best-practices","text":"When using the floxide-transform crate, consider these best practices: Use Appropriate Concurrency Limits : Set batch processing concurrency limits based on your system's capabilities. Handle Async Errors : Properly handle errors in async code using the ? operator or explicit error handling. Compose Transformations : Chain multiple transform nodes for complex data transformations. Leverage Type Safety : Use Rust's type system to ensure type safety in transformations. Consider Resource Usage : Be mindful of resource usage in batch processing operations.","title":"Best Practices"},{"location":"api/floxide-transform/#related-documentation","text":"Async Runtime Selection Batch Processing Implementation Node Lifecycle Methods Batch Processing Example","title":"Related Documentation"},{"location":"architecture/adr-process/","text":"ADR Process \u00b6 This document describes the Architectural Decision Records (ADRs) process used in the Floxide framework. What are ADRs? \u00b6 Architectural Decision Records (ADRs) are documents that capture important architectural decisions made during the development of a software system. They provide a record of what decisions were made, why they were made, and what alternatives were considered. Why Use ADRs? \u00b6 ADRs serve several important purposes in the Floxide project: Documentation : They provide a clear record of architectural decisions. Context Preservation : They capture the context and reasoning behind decisions. Knowledge Sharing : They help new contributors understand the architecture. Decision Making : They provide a structured process for making and reviewing architectural decisions. ADR Process in Floxide \u00b6 In the Floxide framework, we follow a specific process for creating and managing ADRs: ADR Creation and Process \u00b6 Numbering : ADRs are numbered sequentially with a four-digit number prefix (e.g., 0001 , 0002 ). Format : ADRs are stored as Markdown files in the /docs/adrs/ directory. Naming Convention : ADR files are named with their number and a kebab-case title, e.g., 0001-adr-process-and-format.md . Creation Timing : ADRs must be created before implementing any architectural decision, not after. Incremental Creation : ADRs can be created incrementally as decisions are made throughout the project. ADR Content Structure \u00b6 Each ADR includes: Title : A clear, descriptive title following the format \"ADR-NNNN: Title\" Status : One of: Proposed : Initial state when the ADR is first drafted Accepted : Approved for implementation Rejected : Declined, with reasons documented Superseded : Replaced by a newer ADR (with reference to the new ADR) Amended : Modified after implementation Date : The date the ADR was last updated Context : The problem being addressed and relevant background information Decision : The architectural decision that was made and the reasoning Consequences : The results of the decision, both positive and negative Alternatives Considered : Other options that were evaluated and why they were not chosen ADR Lifecycle Management \u00b6 Review Process : ADRs are reviewed before they are accepted. Amendments : Existing ADRs can be amended with additional information, but the core decisions should not be changed after implementation. Superseding : If a decision changes fundamentally, a new ADR should be created that supersedes the old one. The old ADR should be updated to reference the new one. Retrospective Updates : Consequences that were not anticipated can be added to ADRs after implementation to serve as a record for future reference. ADR Index \u00b6 An index of all ADRs is maintained in /docs/adrs/README.md to provide a central reference point. You can also view all ADRs in the ADRs section of the documentation. Creating a New ADR \u00b6 To create a new ADR: Identify an architectural decision that needs to be documented. Determine the next available ADR number. Create a new Markdown file in the /docs/adrs/ directory using the naming convention. Use the ADR template as a starting point. Fill in the sections of the ADR with the relevant information. Submit the ADR for review. Conclusion \u00b6 The ADR process is a critical part of the Floxide framework's development approach. By documenting architectural decisions in a structured way, we ensure that the project's architecture is well-understood, maintainable, and evolves in a controlled manner.","title":"ADR Process"},{"location":"architecture/adr-process/#adr-process","text":"This document describes the Architectural Decision Records (ADRs) process used in the Floxide framework.","title":"ADR Process"},{"location":"architecture/adr-process/#what-are-adrs","text":"Architectural Decision Records (ADRs) are documents that capture important architectural decisions made during the development of a software system. They provide a record of what decisions were made, why they were made, and what alternatives were considered.","title":"What are ADRs?"},{"location":"architecture/adr-process/#why-use-adrs","text":"ADRs serve several important purposes in the Floxide project: Documentation : They provide a clear record of architectural decisions. Context Preservation : They capture the context and reasoning behind decisions. Knowledge Sharing : They help new contributors understand the architecture. Decision Making : They provide a structured process for making and reviewing architectural decisions.","title":"Why Use ADRs?"},{"location":"architecture/adr-process/#adr-process-in-floxide","text":"In the Floxide framework, we follow a specific process for creating and managing ADRs:","title":"ADR Process in Floxide"},{"location":"architecture/adr-process/#adr-creation-and-process","text":"Numbering : ADRs are numbered sequentially with a four-digit number prefix (e.g., 0001 , 0002 ). Format : ADRs are stored as Markdown files in the /docs/adrs/ directory. Naming Convention : ADR files are named with their number and a kebab-case title, e.g., 0001-adr-process-and-format.md . Creation Timing : ADRs must be created before implementing any architectural decision, not after. Incremental Creation : ADRs can be created incrementally as decisions are made throughout the project.","title":"ADR Creation and Process"},{"location":"architecture/adr-process/#adr-content-structure","text":"Each ADR includes: Title : A clear, descriptive title following the format \"ADR-NNNN: Title\" Status : One of: Proposed : Initial state when the ADR is first drafted Accepted : Approved for implementation Rejected : Declined, with reasons documented Superseded : Replaced by a newer ADR (with reference to the new ADR) Amended : Modified after implementation Date : The date the ADR was last updated Context : The problem being addressed and relevant background information Decision : The architectural decision that was made and the reasoning Consequences : The results of the decision, both positive and negative Alternatives Considered : Other options that were evaluated and why they were not chosen","title":"ADR Content Structure"},{"location":"architecture/adr-process/#adr-lifecycle-management","text":"Review Process : ADRs are reviewed before they are accepted. Amendments : Existing ADRs can be amended with additional information, but the core decisions should not be changed after implementation. Superseding : If a decision changes fundamentally, a new ADR should be created that supersedes the old one. The old ADR should be updated to reference the new one. Retrospective Updates : Consequences that were not anticipated can be added to ADRs after implementation to serve as a record for future reference.","title":"ADR Lifecycle Management"},{"location":"architecture/adr-process/#adr-index","text":"An index of all ADRs is maintained in /docs/adrs/README.md to provide a central reference point. You can also view all ADRs in the ADRs section of the documentation.","title":"ADR Index"},{"location":"architecture/adr-process/#creating-a-new-adr","text":"To create a new ADR: Identify an architectural decision that needs to be documented. Determine the next available ADR number. Create a new Markdown file in the /docs/adrs/ directory using the naming convention. Use the ADR template as a starting point. Fill in the sections of the ADR with the relevant information. Submit the ADR for review.","title":"Creating a New ADR"},{"location":"architecture/adr-process/#conclusion","text":"The ADR process is a critical part of the Floxide framework's development approach. By documenting architectural decisions in a structured way, we ensure that the project's architecture is well-understood, maintainable, and evolves in a controlled manner.","title":"Conclusion"},{"location":"architecture/async-runtime-selection/","text":"Async Runtime Selection \u00b6 This document describes the async runtime selection for the Floxide framework. Overview \u00b6 The Floxide framework is designed around asynchronous operations to efficiently handle workflow execution. In Rust, async operations require an explicit runtime to execute futures. This document explains the choice of async runtime and how it's used in the framework. Async Runtime Options \u00b6 There are several viable async runtimes available in the Rust ecosystem, each with different trade-offs: Tokio : Full-featured, production-ready, widely adopted async-std : Similar to the standard library, focused on ergonomics smol : Small and simple runtime Custom runtimes : Roll-our-own or specialized solutions Selected Runtime: Tokio \u00b6 The Floxide framework uses Tokio as its primary async runtime, with the full feature set enabled. Tokio was selected for several reasons: Maturity and Stability : Tokio is a mature, production-ready runtime with a stable API. Performance : Tokio offers excellent performance characteristics for the types of workloads the framework handles. Ecosystem Integration : Tokio has broad adoption and integrates well with many other Rust crates. Feature Set : Tokio provides a comprehensive set of features including timers, task scheduling, and I/O operations. Community Support : Tokio has a large community and extensive documentation. Implementation Details \u00b6 Core Runtime Usage \u00b6 The framework uses Tokio's runtime to execute async workflows: // In floxide-transform crate pub fn run_flow < S , F > ( flow : F , shared_state : & mut S ) -> Result < (), FloxideError > where F : BaseNode < S > , { let rt = tokio :: runtime :: Runtime :: new () ? ; rt . block_on ( async { flow . run ( shared_state ). await }) } Async Trait Implementation \u00b6 The framework uses the async_trait crate to support async methods in traits: use async_trait :: async_trait ; #[async_trait] pub trait AsyncNode : Send + Sync + ' static { type Context : Send + ' static ; async fn exec_async ( & self , ctx : & mut Self :: Context ) -> NodeResult ; } Concurrency Control \u00b6 Tokio's features are used for concurrency control in batch processing: use tokio :: sync :: Semaphore ; pub struct BatchProcessor { concurrency_limit : usize , semaphore : Semaphore , } impl BatchProcessor { pub async fn process_batch < T , F , Fut > ( & self , items : Vec < T > , processor : F ) -> Vec < Result < T , Error >> where F : Fn ( T ) -> Fut + Send + Sync , Fut : Future < Output = Result < T , Error >> + Send , T : Send + ' static , { let mut handles = Vec :: with_capacity ( items . len ()); for item in items { let permit = self . semaphore . acquire (). await . unwrap (); let processor = & processor ; let handle = tokio :: spawn ( async move { let result = processor ( item ). await ; drop ( permit ); result }); handles . push ( handle ); } // Collect results... } } Runtime Configuration \u00b6 The framework allows for configuration of the Tokio runtime: pub struct RuntimeConfig { worker_threads : Option < usize > , thread_name : Option < String > , thread_stack_size : Option < usize > , } impl RuntimeConfig { pub fn build_runtime ( & self ) -> Result < Runtime , FloxideError > { let mut builder = tokio :: runtime :: Builder :: new_multi_thread (); if let Some ( threads ) = self . worker_threads { builder . worker_threads ( threads ); } if let Some ( name ) = & self . thread_name { builder . thread_name ( name ); } if let Some ( stack_size ) = self . thread_stack_size { builder . thread_stack_size ( stack_size ); } builder . enable_all () . build () . map_err ( | e | FloxideError :: RuntimeCreationFailed ( e . to_string ())) } } Best Practices \u00b6 When working with async code in the Floxide framework, consider these best practices: Avoid Blocking Operations : Don't perform blocking operations directly in async functions. Use tokio::task::spawn_blocking for CPU-intensive or blocking operations. Proper Error Handling : Use the ? operator or explicit error handling in async functions. Cancellation Safety : Ensure that your async code handles cancellation gracefully. Resource Management : Use RAII patterns to ensure resources are properly cleaned up, even if tasks are cancelled. Concurrency Limits : Use semaphores or other mechanisms to limit concurrency when appropriate. Conclusion \u00b6 The selection of Tokio as the async runtime for the Floxide framework provides a solid foundation for building efficient, concurrent workflow applications. By leveraging Tokio's features and the async_trait crate, the framework offers a powerful and flexible approach to asynchronous workflow execution. For more detailed information on the async runtime selection, refer to the Async Runtime Selection ADR .","title":"Async Runtime Selection"},{"location":"architecture/async-runtime-selection/#async-runtime-selection","text":"This document describes the async runtime selection for the Floxide framework.","title":"Async Runtime Selection"},{"location":"architecture/async-runtime-selection/#overview","text":"The Floxide framework is designed around asynchronous operations to efficiently handle workflow execution. In Rust, async operations require an explicit runtime to execute futures. This document explains the choice of async runtime and how it's used in the framework.","title":"Overview"},{"location":"architecture/async-runtime-selection/#async-runtime-options","text":"There are several viable async runtimes available in the Rust ecosystem, each with different trade-offs: Tokio : Full-featured, production-ready, widely adopted async-std : Similar to the standard library, focused on ergonomics smol : Small and simple runtime Custom runtimes : Roll-our-own or specialized solutions","title":"Async Runtime Options"},{"location":"architecture/async-runtime-selection/#selected-runtime-tokio","text":"The Floxide framework uses Tokio as its primary async runtime, with the full feature set enabled. Tokio was selected for several reasons: Maturity and Stability : Tokio is a mature, production-ready runtime with a stable API. Performance : Tokio offers excellent performance characteristics for the types of workloads the framework handles. Ecosystem Integration : Tokio has broad adoption and integrates well with many other Rust crates. Feature Set : Tokio provides a comprehensive set of features including timers, task scheduling, and I/O operations. Community Support : Tokio has a large community and extensive documentation.","title":"Selected Runtime: Tokio"},{"location":"architecture/async-runtime-selection/#implementation-details","text":"","title":"Implementation Details"},{"location":"architecture/async-runtime-selection/#core-runtime-usage","text":"The framework uses Tokio's runtime to execute async workflows: // In floxide-transform crate pub fn run_flow < S , F > ( flow : F , shared_state : & mut S ) -> Result < (), FloxideError > where F : BaseNode < S > , { let rt = tokio :: runtime :: Runtime :: new () ? ; rt . block_on ( async { flow . run ( shared_state ). await }) }","title":"Core Runtime Usage"},{"location":"architecture/async-runtime-selection/#async-trait-implementation","text":"The framework uses the async_trait crate to support async methods in traits: use async_trait :: async_trait ; #[async_trait] pub trait AsyncNode : Send + Sync + ' static { type Context : Send + ' static ; async fn exec_async ( & self , ctx : & mut Self :: Context ) -> NodeResult ; }","title":"Async Trait Implementation"},{"location":"architecture/async-runtime-selection/#concurrency-control","text":"Tokio's features are used for concurrency control in batch processing: use tokio :: sync :: Semaphore ; pub struct BatchProcessor { concurrency_limit : usize , semaphore : Semaphore , } impl BatchProcessor { pub async fn process_batch < T , F , Fut > ( & self , items : Vec < T > , processor : F ) -> Vec < Result < T , Error >> where F : Fn ( T ) -> Fut + Send + Sync , Fut : Future < Output = Result < T , Error >> + Send , T : Send + ' static , { let mut handles = Vec :: with_capacity ( items . len ()); for item in items { let permit = self . semaphore . acquire (). await . unwrap (); let processor = & processor ; let handle = tokio :: spawn ( async move { let result = processor ( item ). await ; drop ( permit ); result }); handles . push ( handle ); } // Collect results... } }","title":"Concurrency Control"},{"location":"architecture/async-runtime-selection/#runtime-configuration","text":"The framework allows for configuration of the Tokio runtime: pub struct RuntimeConfig { worker_threads : Option < usize > , thread_name : Option < String > , thread_stack_size : Option < usize > , } impl RuntimeConfig { pub fn build_runtime ( & self ) -> Result < Runtime , FloxideError > { let mut builder = tokio :: runtime :: Builder :: new_multi_thread (); if let Some ( threads ) = self . worker_threads { builder . worker_threads ( threads ); } if let Some ( name ) = & self . thread_name { builder . thread_name ( name ); } if let Some ( stack_size ) = self . thread_stack_size { builder . thread_stack_size ( stack_size ); } builder . enable_all () . build () . map_err ( | e | FloxideError :: RuntimeCreationFailed ( e . to_string ())) } }","title":"Runtime Configuration"},{"location":"architecture/async-runtime-selection/#best-practices","text":"When working with async code in the Floxide framework, consider these best practices: Avoid Blocking Operations : Don't perform blocking operations directly in async functions. Use tokio::task::spawn_blocking for CPU-intensive or blocking operations. Proper Error Handling : Use the ? operator or explicit error handling in async functions. Cancellation Safety : Ensure that your async code handles cancellation gracefully. Resource Management : Use RAII patterns to ensure resources are properly cleaned up, even if tasks are cancelled. Concurrency Limits : Use semaphores or other mechanisms to limit concurrency when appropriate.","title":"Best Practices"},{"location":"architecture/async-runtime-selection/#conclusion","text":"The selection of Tokio as the async runtime for the Floxide framework provides a solid foundation for building efficient, concurrent workflow applications. By leveraging Tokio's features and the async_trait crate, the framework offers a powerful and flexible approach to asynchronous workflow execution. For more detailed information on the async runtime selection, refer to the Async Runtime Selection ADR .","title":"Conclusion"},{"location":"architecture/batch-processing-implementation/","text":"Batch Processing Implementation \u00b6 This document describes the batch processing implementation in the Floxide framework. Overview \u00b6 Batch processing in the Floxide framework enables efficient parallel execution of workflows on collections of items. This capability is essential for handling large datasets and maximizing throughput in workflow applications. Core Concepts \u00b6 The batch processing implementation is built around several key concepts: BatchContext : A specialized context that manages collections of items BatchNode : A node that can process items in parallel BatchFlow : A workflow orchestrator for batch processing BatchContext \u00b6 The BatchContext trait defines how contexts that support batch operations should behave: /// Trait for contexts that support batch processing pub trait BatchContext < T > { /// Get the items to process in batch fn get_batch_items ( & self ) -> Result < Vec < T > , FloxideError > ; /// Create a context for a single item fn create_item_context ( & self , item : T ) -> Result < Self , FloxideError > where Self : Sized ; /// Update the main context with results from item processing fn update_with_results ( & mut self , results : Vec < Result < T , FloxideError >> ) -> Result < (), FloxideError > ; } A concrete implementation of BatchContext typically wraps a standard context and adds batch-specific functionality: pub struct SimpleBatchContext < S , T > { inner_context : Context < S > , items : Vec < T > , results : Vec < Result < T , FloxideError >> , } impl < S , T > BatchContext < T > for SimpleBatchContext < S , T > where S : Clone + Send + ' static , T : Clone + Send + ' static , { fn get_batch_items ( & self ) -> Result < Vec < T > , FloxideError > { Ok ( self . items . clone ()) } fn create_item_context ( & self , item : T ) -> Result < Self , FloxideError > { Ok ( SimpleBatchContext { inner_context : Context :: new ( self . inner_context . state (). clone ()), items : vec ! [ item ], results : Vec :: new (), }) } fn update_with_results ( & mut self , results : Vec < Result < T , FloxideError >> ) -> Result < (), FloxideError > { self . results = results ; Ok (()) } } BatchNode \u00b6 The BatchNode trait extends the standard Node trait with batch processing capabilities: pub trait BatchNode < T > : Node { /// Process a batch of items concurrently fn process_batch ( & self , ctx : & mut Self :: Context , concurrency : usize ) -> Result < (), FloxideError > where Self :: Context : BatchContext < T > , { let items = ctx . get_batch_items () ? ; let processor = BatchProcessor :: new ( concurrency ); let results = processor . process_items ( items , | item | { let mut item_ctx = ctx . create_item_context ( item ) ? ; self . exec ( & mut item_ctx ) ? ; Ok ( item ) }) ? ; ctx . update_with_results ( results ) ? ; Ok (()) } } BatchProcessor \u00b6 The BatchProcessor handles the actual parallel execution of items: pub struct BatchProcessor { concurrency_limit : usize , } impl BatchProcessor { pub fn new ( concurrency_limit : usize ) -> Self { Self { concurrency_limit } } pub fn process_items < T , F > ( & self , items : Vec < T > , processor : F ) -> Result < Vec < Result < T , FloxideError >> , FloxideError > where F : Fn ( T ) -> Result < T , FloxideError > + Send + Sync + ' static , T : Send + ' static , { let semaphore = Arc :: new ( Semaphore :: new ( self . concurrency_limit )); let processor = Arc :: new ( processor ); let handles : Vec < _ > = items . into_iter () . map ( | item | { let semaphore = Arc :: clone ( & semaphore ); let processor = Arc :: clone ( & processor ); thread :: spawn ( move || { let _permit = semaphore . acquire (); processor ( item ) }) }) . collect (); let results = handles . into_iter () . map ( | handle | match handle . join () { Ok ( result ) => result , Err ( _ ) => Err ( FloxideError :: ThreadPanicked ), }) . collect (); Ok ( results ) } } BatchFlow \u00b6 The BatchFlow provides a simplified API for batch processing workflows: pub struct BatchFlow < T , N > where N : BatchNode < T > , { node : N , concurrency : usize , _phantom : PhantomData < T > , } impl < T , N > BatchFlow < T , N > where N : BatchNode < T > , { pub fn new ( node : N , concurrency : usize ) -> Self { Self { node , concurrency , _phantom : PhantomData , } } pub fn execute < C > ( & self , mut context : C ) -> Result < C , FloxideError > where C : BatchContext < T > , { self . node . process_batch ( & mut context , self . concurrency ) ? ; Ok ( context ) } } Usage Example \u00b6 Here's an example of using the batch processing capabilities: // Define a state type struct MyState { processed_count : usize , } // Define an item type struct MyItem { id : usize , value : String , } // Create a batch context let state = MyState { processed_count : 0 }; let items = vec! [ MyItem { id : 1 , value : \"item1\" . to_string () }, MyItem { id : 2 , value : \"item2\" . to_string () }, MyItem { id : 3 , value : \"item3\" . to_string () }, ]; let context = SimpleBatchContext :: new ( state , items ); // Create a batch node struct MyBatchNode ; impl Node for MyBatchNode { type Context = SimpleBatchContext < MyState , MyItem > ; fn exec ( & self , ctx : & mut Self :: Context ) -> NodeResult { // Process a single item let item = & ctx . get_batch_items () ? [ 0 ]; println! ( \"Processing item {}: {}\" , item . id , item . value ); ctx . inner_context . state_mut (). processed_count += 1 ; Ok (()) } } impl BatchNode < MyItem > for MyBatchNode {} // Create and execute a batch flow let batch_flow = BatchFlow :: new ( MyBatchNode , 4 ); let result = batch_flow . execute ( context ); Conclusion \u00b6 The batch processing implementation in the Floxide framework provides a powerful and flexible approach to parallel execution of workflows. By leveraging Rust's concurrency features and the framework's core abstractions, it enables efficient processing of large datasets while maintaining type safety and proper error handling. For more detailed information on the batch processing implementation, refer to the Batch Processing Implementation ADR .","title":"Batch Processing Implementation"},{"location":"architecture/batch-processing-implementation/#batch-processing-implementation","text":"This document describes the batch processing implementation in the Floxide framework.","title":"Batch Processing Implementation"},{"location":"architecture/batch-processing-implementation/#overview","text":"Batch processing in the Floxide framework enables efficient parallel execution of workflows on collections of items. This capability is essential for handling large datasets and maximizing throughput in workflow applications.","title":"Overview"},{"location":"architecture/batch-processing-implementation/#core-concepts","text":"The batch processing implementation is built around several key concepts: BatchContext : A specialized context that manages collections of items BatchNode : A node that can process items in parallel BatchFlow : A workflow orchestrator for batch processing","title":"Core Concepts"},{"location":"architecture/batch-processing-implementation/#batchcontext","text":"The BatchContext trait defines how contexts that support batch operations should behave: /// Trait for contexts that support batch processing pub trait BatchContext < T > { /// Get the items to process in batch fn get_batch_items ( & self ) -> Result < Vec < T > , FloxideError > ; /// Create a context for a single item fn create_item_context ( & self , item : T ) -> Result < Self , FloxideError > where Self : Sized ; /// Update the main context with results from item processing fn update_with_results ( & mut self , results : Vec < Result < T , FloxideError >> ) -> Result < (), FloxideError > ; } A concrete implementation of BatchContext typically wraps a standard context and adds batch-specific functionality: pub struct SimpleBatchContext < S , T > { inner_context : Context < S > , items : Vec < T > , results : Vec < Result < T , FloxideError >> , } impl < S , T > BatchContext < T > for SimpleBatchContext < S , T > where S : Clone + Send + ' static , T : Clone + Send + ' static , { fn get_batch_items ( & self ) -> Result < Vec < T > , FloxideError > { Ok ( self . items . clone ()) } fn create_item_context ( & self , item : T ) -> Result < Self , FloxideError > { Ok ( SimpleBatchContext { inner_context : Context :: new ( self . inner_context . state (). clone ()), items : vec ! [ item ], results : Vec :: new (), }) } fn update_with_results ( & mut self , results : Vec < Result < T , FloxideError >> ) -> Result < (), FloxideError > { self . results = results ; Ok (()) } }","title":"BatchContext"},{"location":"architecture/batch-processing-implementation/#batchnode","text":"The BatchNode trait extends the standard Node trait with batch processing capabilities: pub trait BatchNode < T > : Node { /// Process a batch of items concurrently fn process_batch ( & self , ctx : & mut Self :: Context , concurrency : usize ) -> Result < (), FloxideError > where Self :: Context : BatchContext < T > , { let items = ctx . get_batch_items () ? ; let processor = BatchProcessor :: new ( concurrency ); let results = processor . process_items ( items , | item | { let mut item_ctx = ctx . create_item_context ( item ) ? ; self . exec ( & mut item_ctx ) ? ; Ok ( item ) }) ? ; ctx . update_with_results ( results ) ? ; Ok (()) } }","title":"BatchNode"},{"location":"architecture/batch-processing-implementation/#batchprocessor","text":"The BatchProcessor handles the actual parallel execution of items: pub struct BatchProcessor { concurrency_limit : usize , } impl BatchProcessor { pub fn new ( concurrency_limit : usize ) -> Self { Self { concurrency_limit } } pub fn process_items < T , F > ( & self , items : Vec < T > , processor : F ) -> Result < Vec < Result < T , FloxideError >> , FloxideError > where F : Fn ( T ) -> Result < T , FloxideError > + Send + Sync + ' static , T : Send + ' static , { let semaphore = Arc :: new ( Semaphore :: new ( self . concurrency_limit )); let processor = Arc :: new ( processor ); let handles : Vec < _ > = items . into_iter () . map ( | item | { let semaphore = Arc :: clone ( & semaphore ); let processor = Arc :: clone ( & processor ); thread :: spawn ( move || { let _permit = semaphore . acquire (); processor ( item ) }) }) . collect (); let results = handles . into_iter () . map ( | handle | match handle . join () { Ok ( result ) => result , Err ( _ ) => Err ( FloxideError :: ThreadPanicked ), }) . collect (); Ok ( results ) } }","title":"BatchProcessor"},{"location":"architecture/batch-processing-implementation/#batchflow","text":"The BatchFlow provides a simplified API for batch processing workflows: pub struct BatchFlow < T , N > where N : BatchNode < T > , { node : N , concurrency : usize , _phantom : PhantomData < T > , } impl < T , N > BatchFlow < T , N > where N : BatchNode < T > , { pub fn new ( node : N , concurrency : usize ) -> Self { Self { node , concurrency , _phantom : PhantomData , } } pub fn execute < C > ( & self , mut context : C ) -> Result < C , FloxideError > where C : BatchContext < T > , { self . node . process_batch ( & mut context , self . concurrency ) ? ; Ok ( context ) } }","title":"BatchFlow"},{"location":"architecture/batch-processing-implementation/#usage-example","text":"Here's an example of using the batch processing capabilities: // Define a state type struct MyState { processed_count : usize , } // Define an item type struct MyItem { id : usize , value : String , } // Create a batch context let state = MyState { processed_count : 0 }; let items = vec! [ MyItem { id : 1 , value : \"item1\" . to_string () }, MyItem { id : 2 , value : \"item2\" . to_string () }, MyItem { id : 3 , value : \"item3\" . to_string () }, ]; let context = SimpleBatchContext :: new ( state , items ); // Create a batch node struct MyBatchNode ; impl Node for MyBatchNode { type Context = SimpleBatchContext < MyState , MyItem > ; fn exec ( & self , ctx : & mut Self :: Context ) -> NodeResult { // Process a single item let item = & ctx . get_batch_items () ? [ 0 ]; println! ( \"Processing item {}: {}\" , item . id , item . value ); ctx . inner_context . state_mut (). processed_count += 1 ; Ok (()) } } impl BatchNode < MyItem > for MyBatchNode {} // Create and execute a batch flow let batch_flow = BatchFlow :: new ( MyBatchNode , 4 ); let result = batch_flow . execute ( context );","title":"Usage Example"},{"location":"architecture/batch-processing-implementation/#conclusion","text":"The batch processing implementation in the Floxide framework provides a powerful and flexible approach to parallel execution of workflows. By leveraging Rust's concurrency features and the framework's core abstractions, it enables efficient processing of large datasets while maintaining type safety and proper error handling. For more detailed information on the batch processing implementation, refer to the Batch Processing Implementation ADR .","title":"Conclusion"},{"location":"architecture/core-framework-abstractions/","text":"Core Framework Abstractions \u00b6 This document describes the core abstractions that form the foundation of the Floxide framework. Overview \u00b6 The Floxide framework is designed as a directed graph workflow system built on several key abstractions: Node Interface : The core building block for workflow steps Action Types : Type-safe transitions between nodes Context : State container passed between nodes Workflow : The orchestrator that manages node execution Batch Processing : Capability for parallel execution These abstractions work together to create a flexible, type-safe, and composable workflow system. Node Interface \u00b6 The Node trait is the fundamental building block of the Floxide framework. It defines the lifecycle methods that all nodes must implement: pub trait Node : Send + Sync + ' static { /// The context type this node operates on type Context : Send + ' static ; /// Prepare the node for execution (optional) fn prep ( & self , _ctx : & mut Self :: Context ) -> NodeResult { Ok (()) } /// Execute the node's main logic fn exec ( & self , ctx : & mut Self :: Context ) -> NodeResult ; /// Perform post-execution cleanup (optional) fn post ( & self , _ctx : & mut Self :: Context ) -> NodeResult { Ok (()) } } The Node trait follows a three-phase lifecycle: Preparation ( prep ) : Optional setup before execution Execution ( exec ) : The main operation of the node Post-processing ( post ) : Optional cleanup after execution Action Types \u00b6 Actions define the transitions between nodes in a workflow. The Floxide framework uses a trait-based approach for actions that allows for type-safe, domain-specific action types: /// Trait for types that can be used as actions in workflow transitions pub trait ActionType : Debug + Clone + PartialEq + Eq + Hash + Send + Sync + ' static {} /// Standard action types provided by the framework #[derive(Debug, Clone, PartialEq, Eq, Hash)] pub enum DefaultAction { /// Default transition to the next node Next , /// Successfully complete the workflow Complete , /// Signal an error condition Error , } impl ActionType for DefaultAction {} This approach allows users to define their own custom action types that are fully type-safe at compile time. Context \u00b6 Contexts are containers for state that is passed between nodes during workflow execution. The basic Context type is generic over the state it contains: pub struct Context < S > { state : S , } impl < S > Context < S > { /// Create a new context with the given state pub fn new ( state : S ) -> Self { Self { state } } /// Get a reference to the state pub fn state ( & self ) -> & S { & self . state } /// Get a mutable reference to the state pub fn state_mut ( & mut self ) -> & mut S { & mut self . state } } Specialized context types like BatchContext and EventContext extend this basic context with additional functionality. Workflow \u00b6 The Workflow struct is the orchestrator that manages the execution of nodes. It maintains a directed graph of nodes and handles the flow of execution based on the actions returned by nodes: pub struct Workflow < A : ActionType , C > { nodes : HashMap < NodeId , Box < dyn Node < Context = C >>> , transitions : HashMap < ( NodeId , A ), NodeId > , start_node : Option < NodeId > , } The Workflow provides methods for: Adding nodes to the workflow Defining transitions between nodes Executing the workflow from start to finish Handling errors and retries Batch Processing \u00b6 The batch processing capability allows for parallel execution of workflow steps on multiple items. It extends the core abstractions with: BatchContext : A specialized context that manages a collection of items BatchNode : A trait for nodes that can process items in parallel BatchWorkflow : An orchestrator for batch processing workflows Conclusion \u00b6 These core abstractions provide the foundation for the Floxide framework. By leveraging Rust's type system and ownership model, they enable the creation of robust, type-safe, and composable workflow applications. For more detailed information on these abstractions, refer to the Core Framework Abstractions ADR .","title":"Core Framework Abstractions"},{"location":"architecture/core-framework-abstractions/#core-framework-abstractions","text":"This document describes the core abstractions that form the foundation of the Floxide framework.","title":"Core Framework Abstractions"},{"location":"architecture/core-framework-abstractions/#overview","text":"The Floxide framework is designed as a directed graph workflow system built on several key abstractions: Node Interface : The core building block for workflow steps Action Types : Type-safe transitions between nodes Context : State container passed between nodes Workflow : The orchestrator that manages node execution Batch Processing : Capability for parallel execution These abstractions work together to create a flexible, type-safe, and composable workflow system.","title":"Overview"},{"location":"architecture/core-framework-abstractions/#node-interface","text":"The Node trait is the fundamental building block of the Floxide framework. It defines the lifecycle methods that all nodes must implement: pub trait Node : Send + Sync + ' static { /// The context type this node operates on type Context : Send + ' static ; /// Prepare the node for execution (optional) fn prep ( & self , _ctx : & mut Self :: Context ) -> NodeResult { Ok (()) } /// Execute the node's main logic fn exec ( & self , ctx : & mut Self :: Context ) -> NodeResult ; /// Perform post-execution cleanup (optional) fn post ( & self , _ctx : & mut Self :: Context ) -> NodeResult { Ok (()) } } The Node trait follows a three-phase lifecycle: Preparation ( prep ) : Optional setup before execution Execution ( exec ) : The main operation of the node Post-processing ( post ) : Optional cleanup after execution","title":"Node Interface"},{"location":"architecture/core-framework-abstractions/#action-types","text":"Actions define the transitions between nodes in a workflow. The Floxide framework uses a trait-based approach for actions that allows for type-safe, domain-specific action types: /// Trait for types that can be used as actions in workflow transitions pub trait ActionType : Debug + Clone + PartialEq + Eq + Hash + Send + Sync + ' static {} /// Standard action types provided by the framework #[derive(Debug, Clone, PartialEq, Eq, Hash)] pub enum DefaultAction { /// Default transition to the next node Next , /// Successfully complete the workflow Complete , /// Signal an error condition Error , } impl ActionType for DefaultAction {} This approach allows users to define their own custom action types that are fully type-safe at compile time.","title":"Action Types"},{"location":"architecture/core-framework-abstractions/#context","text":"Contexts are containers for state that is passed between nodes during workflow execution. The basic Context type is generic over the state it contains: pub struct Context < S > { state : S , } impl < S > Context < S > { /// Create a new context with the given state pub fn new ( state : S ) -> Self { Self { state } } /// Get a reference to the state pub fn state ( & self ) -> & S { & self . state } /// Get a mutable reference to the state pub fn state_mut ( & mut self ) -> & mut S { & mut self . state } } Specialized context types like BatchContext and EventContext extend this basic context with additional functionality.","title":"Context"},{"location":"architecture/core-framework-abstractions/#workflow","text":"The Workflow struct is the orchestrator that manages the execution of nodes. It maintains a directed graph of nodes and handles the flow of execution based on the actions returned by nodes: pub struct Workflow < A : ActionType , C > { nodes : HashMap < NodeId , Box < dyn Node < Context = C >>> , transitions : HashMap < ( NodeId , A ), NodeId > , start_node : Option < NodeId > , } The Workflow provides methods for: Adding nodes to the workflow Defining transitions between nodes Executing the workflow from start to finish Handling errors and retries","title":"Workflow"},{"location":"architecture/core-framework-abstractions/#batch-processing","text":"The batch processing capability allows for parallel execution of workflow steps on multiple items. It extends the core abstractions with: BatchContext : A specialized context that manages a collection of items BatchNode : A trait for nodes that can process items in parallel BatchWorkflow : An orchestrator for batch processing workflows","title":"Batch Processing"},{"location":"architecture/core-framework-abstractions/#conclusion","text":"These core abstractions provide the foundation for the Floxide framework. By leveraging Rust's type system and ownership model, they enable the creation of robust, type-safe, and composable workflow applications. For more detailed information on these abstractions, refer to the Core Framework Abstractions ADR .","title":"Conclusion"},{"location":"architecture/event-driven-workflow-pattern/","text":"Event-Driven Workflow Pattern \u00b6 This document describes the event-driven workflow pattern in the Floxide framework. Overview \u00b6 The event-driven workflow pattern extends the Floxide framework to support asynchronous, event-based processing. This pattern is essential for building reactive systems that respond to external events in real-time. Core Concepts \u00b6 The event-driven workflow pattern is built around several key concepts: EventDrivenNode : A specialized node that waits for and processes events Event Sources : Components that provide events from external systems EventDrivenWorkflow : A workflow that orchestrates event-driven nodes Event Routing : Mechanisms for routing events between nodes EventDrivenNode \u00b6 The EventDrivenNode trait defines how nodes that respond to events should behave: #[async_trait] pub trait EventDrivenNode < E > : Send + Sync + ' static where E : Event + Send + ' static , { /// Get the node's unique identifier fn id ( & self ) -> NodeId ; /// Wait for an event to arrive async fn wait_for_event ( & self ) -> Result < E , FloxideError > ; /// Process an event and return an action async fn process_event ( & self , event : E ) -> Result < EventAction , FloxideError > ; } A concrete implementation of EventDrivenNode typically connects to an event source and processes events: pub struct SensorMonitorNode { id : NodeId , event_source : Arc < dyn EventSource < SensorEvent >> , threshold : f64 , } #[async_trait] impl EventDrivenNode < SensorEvent > for SensorMonitorNode { fn id ( & self ) -> NodeId { self . id } async fn wait_for_event ( & self ) -> Result < SensorEvent , FloxideError > { self . event_source . next_event (). await } async fn process_event ( & self , event : SensorEvent ) -> Result < EventAction , FloxideError > { if event . value > self . threshold { Ok ( EventAction :: Route ( \"high_value_handler\" , event )) } else { Ok ( EventAction :: Route ( \"standard_handler\" , event )) } } } Event Sources \u00b6 Event sources provide events from external systems: #[async_trait] pub trait EventSource < E > : Send + Sync + ' static where E : Event + Send + ' static , { /// Get the next event from the source async fn next_event ( & self ) -> Result < E , FloxideError > ; /// Check if the source has more events async fn has_more_events ( & self ) -> Result < bool , FloxideError > ; } The framework provides several built-in event sources: ChannelEventSource \u00b6 Uses Tokio MPSC channels to receive events: pub struct ChannelEventSource < E > where E : Event + Send + ' static , { receiver : Mutex < mpsc :: Receiver < E >> , } impl < E > ChannelEventSource < E > where E : Event + Send + ' static , { pub fn new ( receiver : mpsc :: Receiver < E > ) -> Self { Self { receiver : Mutex :: new ( receiver ), } } pub fn sender ( & self ) -> mpsc :: Sender < E > { // Return a sender connected to this receiver } } #[async_trait] impl < E > EventSource < E > for ChannelEventSource < E > where E : Event + Send + ' static , { async fn next_event ( & self ) -> Result < E , FloxideError > { let mut receiver = self . receiver . lock (). await ; receiver . recv (). await . ok_or ( FloxideError :: EventSourceClosed ) } async fn has_more_events ( & self ) -> Result < bool , FloxideError > { let receiver = self . receiver . lock (). await ; Ok ( ! receiver . is_closed ()) } } EventDrivenWorkflow \u00b6 The EventDrivenWorkflow orchestrates event-driven nodes: pub struct EventDrivenWorkflow < E > where E : Event + Send + Clone + ' static , { nodes : HashMap < NodeId , Box < dyn EventDrivenNode < E >>> , routes : HashMap < String , NodeId > , initial_node : NodeId , timeout : Option < Duration > , } impl < E > EventDrivenWorkflow < E > where E : Event + Send + Clone + ' static , { pub fn new () -> Self { Self { nodes : HashMap :: new (), routes : HashMap :: new (), initial_node : NodeId :: new (), timeout : None , } } pub fn add_node ( & mut self , node : impl EventDrivenNode < E > + ' static ) -> NodeId { let id = node . id (); self . nodes . insert ( id , Box :: new ( node )); id } pub fn set_route ( & mut self , name : & str , target_node : NodeId ) { self . routes . insert ( name . to_string (), target_node ); } pub fn set_initial_node ( & mut self , node_id : NodeId ) { self . initial_node = node_id ; } pub fn set_timeout ( & mut self , timeout : Duration ) { self . timeout = Some ( timeout ); } pub async fn run ( & self ) -> Result < (), FloxideError > { let mut current_node_id = self . initial_node ; loop { let node = self . nodes . get ( & current_node_id ) . ok_or_else ( || FloxideError :: NodeNotFound ( current_node_id )) ? ; let event = node . wait_for_event (). await ? ; let action = node . process_event ( event ). await ? ; match action { EventAction :: Route ( route , event ) => { if let Some ( next_node_id ) = self . routes . get ( & route ) { current_node_id = * next_node_id ; } else { return Err ( FloxideError :: RouteNotFound ( route )); } }, EventAction :: Terminate => { break ; }, } } Ok (()) } } Event Routing \u00b6 Events are routed between nodes based on the actions returned by process_event : pub enum EventAction < E > where E : Event + Send + ' static , { /// Route the event to another node Route ( String , E ), /// Terminate the workflow Terminate , } Integration with Standard Workflows \u00b6 The event-driven pattern can be integrated with standard workflows: EventDrivenNodeAdapter \u00b6 Adapts an event-driven node to be used in a standard workflow: pub struct EventDrivenNodeAdapter < E , N > where E : Event + Send + ' static , N : EventDrivenNode < E > , { node : N , _phantom : PhantomData < E > , } impl < E , N > Node for EventDrivenNodeAdapter < E , N > where E : Event + Send + ' static , N : EventDrivenNode < E > , { type Context = EventContext < E > ; async fn run ( & self , ctx : & mut Self :: Context ) -> Result < NextAction , FloxideError > { let event = self . node . wait_for_event (). await ? ; let action = self . node . process_event ( event ). await ? ; // Convert EventAction to NextAction match action { EventAction :: Route ( route , event ) => { ctx . set_event ( event ); Ok ( NextAction :: Route ( route )) }, EventAction :: Terminate => { Ok ( NextAction :: Terminate ) }, } } } NestedEventDrivenWorkflow \u00b6 Uses an event-driven workflow as a node in a standard workflow: pub struct NestedEventDrivenWorkflow < E > where E : Event + Send + Clone + ' static , { workflow : EventDrivenWorkflow < E > , } impl < E > Node for NestedEventDrivenWorkflow < E > where E : Event + Send + Clone + ' static , { type Context = Context < () > ; async fn run ( & self , _ctx : & mut Self :: Context ) -> Result < NextAction , FloxideError > { self . workflow . run (). await ? ; Ok ( NextAction :: Continue ) } } Usage Example \u00b6 Here's an example of using the event-driven workflow pattern: // Define an event type #[derive(Clone)] struct SensorEvent { id : String , value : f64 , timestamp : DateTime < Utc > , } impl Event for SensorEvent {} // Create event sources let ( tx1 , rx1 ) = mpsc :: channel ( 100 ); let ( tx2 , rx2 ) = mpsc :: channel ( 100 ); let source1 = ChannelEventSource :: new ( rx1 ); let source2 = ChannelEventSource :: new ( rx2 ); // Create event-driven nodes let sensor_monitor = SensorMonitorNode :: new ( \"sensor1\" , Arc :: new ( source1 ), 100.0 ); let high_value_handler = HighValueHandlerNode :: new ( \"high_value\" , Arc :: new ( source2 )); let standard_handler = StandardHandlerNode :: new ( \"standard\" ); // Create and configure the workflow let mut workflow = EventDrivenWorkflow :: new (); let monitor_id = workflow . add_node ( sensor_monitor ); let high_value_id = workflow . add_node ( high_value_handler ); let standard_id = workflow . add_node ( standard_handler ); workflow . set_initial_node ( monitor_id ); workflow . set_route ( \"high_value_handler\" , high_value_id ); workflow . set_route ( \"standard_handler\" , standard_id ); workflow . set_timeout ( Duration :: from_secs ( 60 )); // Run the workflow tokio :: spawn ( async move { workflow . run (). await . unwrap (); }); // Send events to the workflow tx1 . send ( SensorEvent { id : \"sensor1\" . to_string (), value : 150.0 , timestamp : Utc :: now (), }). await . unwrap (); Conclusion \u00b6 The event-driven workflow pattern in the Floxide framework provides a powerful and flexible approach to building reactive systems. By leveraging Rust's async capabilities and the framework's core abstractions, it enables efficient processing of asynchronous events while maintaining type safety and proper error handling. For more detailed information on the event-driven workflow pattern, refer to the Event-Driven Workflow Pattern ADR .","title":"Event-Driven Workflow Pattern"},{"location":"architecture/event-driven-workflow-pattern/#event-driven-workflow-pattern","text":"This document describes the event-driven workflow pattern in the Floxide framework.","title":"Event-Driven Workflow Pattern"},{"location":"architecture/event-driven-workflow-pattern/#overview","text":"The event-driven workflow pattern extends the Floxide framework to support asynchronous, event-based processing. This pattern is essential for building reactive systems that respond to external events in real-time.","title":"Overview"},{"location":"architecture/event-driven-workflow-pattern/#core-concepts","text":"The event-driven workflow pattern is built around several key concepts: EventDrivenNode : A specialized node that waits for and processes events Event Sources : Components that provide events from external systems EventDrivenWorkflow : A workflow that orchestrates event-driven nodes Event Routing : Mechanisms for routing events between nodes","title":"Core Concepts"},{"location":"architecture/event-driven-workflow-pattern/#eventdrivennode","text":"The EventDrivenNode trait defines how nodes that respond to events should behave: #[async_trait] pub trait EventDrivenNode < E > : Send + Sync + ' static where E : Event + Send + ' static , { /// Get the node's unique identifier fn id ( & self ) -> NodeId ; /// Wait for an event to arrive async fn wait_for_event ( & self ) -> Result < E , FloxideError > ; /// Process an event and return an action async fn process_event ( & self , event : E ) -> Result < EventAction , FloxideError > ; } A concrete implementation of EventDrivenNode typically connects to an event source and processes events: pub struct SensorMonitorNode { id : NodeId , event_source : Arc < dyn EventSource < SensorEvent >> , threshold : f64 , } #[async_trait] impl EventDrivenNode < SensorEvent > for SensorMonitorNode { fn id ( & self ) -> NodeId { self . id } async fn wait_for_event ( & self ) -> Result < SensorEvent , FloxideError > { self . event_source . next_event (). await } async fn process_event ( & self , event : SensorEvent ) -> Result < EventAction , FloxideError > { if event . value > self . threshold { Ok ( EventAction :: Route ( \"high_value_handler\" , event )) } else { Ok ( EventAction :: Route ( \"standard_handler\" , event )) } } }","title":"EventDrivenNode"},{"location":"architecture/event-driven-workflow-pattern/#event-sources","text":"Event sources provide events from external systems: #[async_trait] pub trait EventSource < E > : Send + Sync + ' static where E : Event + Send + ' static , { /// Get the next event from the source async fn next_event ( & self ) -> Result < E , FloxideError > ; /// Check if the source has more events async fn has_more_events ( & self ) -> Result < bool , FloxideError > ; } The framework provides several built-in event sources:","title":"Event Sources"},{"location":"architecture/event-driven-workflow-pattern/#channeleventsource","text":"Uses Tokio MPSC channels to receive events: pub struct ChannelEventSource < E > where E : Event + Send + ' static , { receiver : Mutex < mpsc :: Receiver < E >> , } impl < E > ChannelEventSource < E > where E : Event + Send + ' static , { pub fn new ( receiver : mpsc :: Receiver < E > ) -> Self { Self { receiver : Mutex :: new ( receiver ), } } pub fn sender ( & self ) -> mpsc :: Sender < E > { // Return a sender connected to this receiver } } #[async_trait] impl < E > EventSource < E > for ChannelEventSource < E > where E : Event + Send + ' static , { async fn next_event ( & self ) -> Result < E , FloxideError > { let mut receiver = self . receiver . lock (). await ; receiver . recv (). await . ok_or ( FloxideError :: EventSourceClosed ) } async fn has_more_events ( & self ) -> Result < bool , FloxideError > { let receiver = self . receiver . lock (). await ; Ok ( ! receiver . is_closed ()) } }","title":"ChannelEventSource"},{"location":"architecture/event-driven-workflow-pattern/#eventdrivenworkflow","text":"The EventDrivenWorkflow orchestrates event-driven nodes: pub struct EventDrivenWorkflow < E > where E : Event + Send + Clone + ' static , { nodes : HashMap < NodeId , Box < dyn EventDrivenNode < E >>> , routes : HashMap < String , NodeId > , initial_node : NodeId , timeout : Option < Duration > , } impl < E > EventDrivenWorkflow < E > where E : Event + Send + Clone + ' static , { pub fn new () -> Self { Self { nodes : HashMap :: new (), routes : HashMap :: new (), initial_node : NodeId :: new (), timeout : None , } } pub fn add_node ( & mut self , node : impl EventDrivenNode < E > + ' static ) -> NodeId { let id = node . id (); self . nodes . insert ( id , Box :: new ( node )); id } pub fn set_route ( & mut self , name : & str , target_node : NodeId ) { self . routes . insert ( name . to_string (), target_node ); } pub fn set_initial_node ( & mut self , node_id : NodeId ) { self . initial_node = node_id ; } pub fn set_timeout ( & mut self , timeout : Duration ) { self . timeout = Some ( timeout ); } pub async fn run ( & self ) -> Result < (), FloxideError > { let mut current_node_id = self . initial_node ; loop { let node = self . nodes . get ( & current_node_id ) . ok_or_else ( || FloxideError :: NodeNotFound ( current_node_id )) ? ; let event = node . wait_for_event (). await ? ; let action = node . process_event ( event ). await ? ; match action { EventAction :: Route ( route , event ) => { if let Some ( next_node_id ) = self . routes . get ( & route ) { current_node_id = * next_node_id ; } else { return Err ( FloxideError :: RouteNotFound ( route )); } }, EventAction :: Terminate => { break ; }, } } Ok (()) } }","title":"EventDrivenWorkflow"},{"location":"architecture/event-driven-workflow-pattern/#event-routing","text":"Events are routed between nodes based on the actions returned by process_event : pub enum EventAction < E > where E : Event + Send + ' static , { /// Route the event to another node Route ( String , E ), /// Terminate the workflow Terminate , }","title":"Event Routing"},{"location":"architecture/event-driven-workflow-pattern/#integration-with-standard-workflows","text":"The event-driven pattern can be integrated with standard workflows:","title":"Integration with Standard Workflows"},{"location":"architecture/event-driven-workflow-pattern/#eventdrivennodeadapter","text":"Adapts an event-driven node to be used in a standard workflow: pub struct EventDrivenNodeAdapter < E , N > where E : Event + Send + ' static , N : EventDrivenNode < E > , { node : N , _phantom : PhantomData < E > , } impl < E , N > Node for EventDrivenNodeAdapter < E , N > where E : Event + Send + ' static , N : EventDrivenNode < E > , { type Context = EventContext < E > ; async fn run ( & self , ctx : & mut Self :: Context ) -> Result < NextAction , FloxideError > { let event = self . node . wait_for_event (). await ? ; let action = self . node . process_event ( event ). await ? ; // Convert EventAction to NextAction match action { EventAction :: Route ( route , event ) => { ctx . set_event ( event ); Ok ( NextAction :: Route ( route )) }, EventAction :: Terminate => { Ok ( NextAction :: Terminate ) }, } } }","title":"EventDrivenNodeAdapter"},{"location":"architecture/event-driven-workflow-pattern/#nestedeventdrivenworkflow","text":"Uses an event-driven workflow as a node in a standard workflow: pub struct NestedEventDrivenWorkflow < E > where E : Event + Send + Clone + ' static , { workflow : EventDrivenWorkflow < E > , } impl < E > Node for NestedEventDrivenWorkflow < E > where E : Event + Send + Clone + ' static , { type Context = Context < () > ; async fn run ( & self , _ctx : & mut Self :: Context ) -> Result < NextAction , FloxideError > { self . workflow . run (). await ? ; Ok ( NextAction :: Continue ) } }","title":"NestedEventDrivenWorkflow"},{"location":"architecture/event-driven-workflow-pattern/#usage-example","text":"Here's an example of using the event-driven workflow pattern: // Define an event type #[derive(Clone)] struct SensorEvent { id : String , value : f64 , timestamp : DateTime < Utc > , } impl Event for SensorEvent {} // Create event sources let ( tx1 , rx1 ) = mpsc :: channel ( 100 ); let ( tx2 , rx2 ) = mpsc :: channel ( 100 ); let source1 = ChannelEventSource :: new ( rx1 ); let source2 = ChannelEventSource :: new ( rx2 ); // Create event-driven nodes let sensor_monitor = SensorMonitorNode :: new ( \"sensor1\" , Arc :: new ( source1 ), 100.0 ); let high_value_handler = HighValueHandlerNode :: new ( \"high_value\" , Arc :: new ( source2 )); let standard_handler = StandardHandlerNode :: new ( \"standard\" ); // Create and configure the workflow let mut workflow = EventDrivenWorkflow :: new (); let monitor_id = workflow . add_node ( sensor_monitor ); let high_value_id = workflow . add_node ( high_value_handler ); let standard_id = workflow . add_node ( standard_handler ); workflow . set_initial_node ( monitor_id ); workflow . set_route ( \"high_value_handler\" , high_value_id ); workflow . set_route ( \"standard_handler\" , standard_id ); workflow . set_timeout ( Duration :: from_secs ( 60 )); // Run the workflow tokio :: spawn ( async move { workflow . run (). await . unwrap (); }); // Send events to the workflow tx1 . send ( SensorEvent { id : \"sensor1\" . to_string (), value : 150.0 , timestamp : Utc :: now (), }). await . unwrap ();","title":"Usage Example"},{"location":"architecture/event-driven-workflow-pattern/#conclusion","text":"The event-driven workflow pattern in the Floxide framework provides a powerful and flexible approach to building reactive systems. By leveraging Rust's async capabilities and the framework's core abstractions, it enables efficient processing of asynchronous events while maintaining type safety and proper error handling. For more detailed information on the event-driven workflow pattern, refer to the Event-Driven Workflow Pattern ADR .","title":"Conclusion"},{"location":"architecture/longrunning-node-implementation/","text":"Long-Running Node Implementation \u00b6 This document describes the implementation details of long-running nodes in the Floxide framework. Overview \u00b6 Long-running nodes in Floxide provide support for tasks that may take significant time to complete, with proper progress tracking and cancellation support. Core Components \u00b6 LongRunningNode Trait \u00b6 The LongRunningNode trait defines the core interface for long-running tasks: #[async_trait] pub trait LongRunningNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static + Debug , { async fn start ( & self , ctx : & mut Context ) -> Result < (), FloxideError > ; async fn check_progress ( & self , ctx : & mut Context ) -> Result < Progress , FloxideError > ; async fn cancel ( & self , ctx : & mut Context ) -> Result < (), FloxideError > ; fn id ( & self ) -> NodeId ; } Progress Tracking \u00b6 The Progress enum represents the current state of a long-running task: pub enum Progress { Running ( f32 ), // 0.0 to 1.0 Complete ( Action ), Failed ( FloxideError ), } Implementation Details \u00b6 Task Management \u00b6 Execution Control Task initialization Progress monitoring Graceful cancellation State Management Progress tracking State persistence Recovery mechanisms Resource Control Resource allocation Cleanup procedures Memory management Error Handling \u00b6 Execution Errors Error propagation Recovery strategies Error reporting Cancellation Handling Safe cancellation Resource cleanup State consistency Resource Management \u00b6 Memory Usage Efficient state tracking Resource cleanup Memory leak prevention Thread Safety Thread-safe execution Safe progress updates Proper synchronization Usage Patterns \u00b6 Basic Usage \u00b6 let node = LongRunningTask :: new ( | ctx | { /* start implementation */ }, | ctx | { /* progress check implementation */ }, | ctx | { /* cancellation implementation */ }, ); With Progress Tracking \u00b6 let node = LongRunningTask :: new ( | ctx | async { for i in 0 .. 100 { process_chunk ( i ) ? ; update_progress ( i as f32 / 100.0 ); } Ok (()) }, | ctx | async { let progress = get_current_progress (); if progress >= 1.0 { Ok ( Progress :: Complete ( DefaultAction :: Next )) } else { Ok ( Progress :: Running ( progress )) } }, | ctx | async { /* cancellation implementation */ }, ); With Error Handling \u00b6 let node = LongRunningTask :: new ( | ctx | async { if let Err ( e ) = check_preconditions () { return Err ( e . into ()); } start_processing () }, | ctx | async { match check_task_progress () { Ok ( progress ) => Ok ( Progress :: Running ( progress )), Err ( e ) => Ok ( Progress :: Failed ( e . into ())), } }, | ctx | async { cleanup_resources () ? ; Ok (()) }, ); Testing \u00b6 The implementation includes comprehensive tests: Unit Tests Task execution Progress tracking Cancellation handling Resource cleanup Integration Tests Complex workflows Error scenarios Performance tests Performance Considerations \u00b6 Execution Efficiency Minimal overhead Efficient progress tracking Resource optimization Memory Usage Optimized state storage Efficient progress updates Minimal allocations Future Improvements \u00b6 Enhanced Features More progress metrics Advanced cancellation strategies Extended recovery options Performance Optimizations Improved state tracking Better resource utilization Enhanced concurrency Related ADRs \u00b6 ADR-0008: Node Lifecycle Methods ADR-0013: Workflow Patterns","title":"Long-Running Node Implementation"},{"location":"architecture/longrunning-node-implementation/#long-running-node-implementation","text":"This document describes the implementation details of long-running nodes in the Floxide framework.","title":"Long-Running Node Implementation"},{"location":"architecture/longrunning-node-implementation/#overview","text":"Long-running nodes in Floxide provide support for tasks that may take significant time to complete, with proper progress tracking and cancellation support.","title":"Overview"},{"location":"architecture/longrunning-node-implementation/#core-components","text":"","title":"Core Components"},{"location":"architecture/longrunning-node-implementation/#longrunningnode-trait","text":"The LongRunningNode trait defines the core interface for long-running tasks: #[async_trait] pub trait LongRunningNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static + Debug , { async fn start ( & self , ctx : & mut Context ) -> Result < (), FloxideError > ; async fn check_progress ( & self , ctx : & mut Context ) -> Result < Progress , FloxideError > ; async fn cancel ( & self , ctx : & mut Context ) -> Result < (), FloxideError > ; fn id ( & self ) -> NodeId ; }","title":"LongRunningNode Trait"},{"location":"architecture/longrunning-node-implementation/#progress-tracking","text":"The Progress enum represents the current state of a long-running task: pub enum Progress { Running ( f32 ), // 0.0 to 1.0 Complete ( Action ), Failed ( FloxideError ), }","title":"Progress Tracking"},{"location":"architecture/longrunning-node-implementation/#implementation-details","text":"","title":"Implementation Details"},{"location":"architecture/longrunning-node-implementation/#task-management","text":"Execution Control Task initialization Progress monitoring Graceful cancellation State Management Progress tracking State persistence Recovery mechanisms Resource Control Resource allocation Cleanup procedures Memory management","title":"Task Management"},{"location":"architecture/longrunning-node-implementation/#error-handling","text":"Execution Errors Error propagation Recovery strategies Error reporting Cancellation Handling Safe cancellation Resource cleanup State consistency","title":"Error Handling"},{"location":"architecture/longrunning-node-implementation/#resource-management","text":"Memory Usage Efficient state tracking Resource cleanup Memory leak prevention Thread Safety Thread-safe execution Safe progress updates Proper synchronization","title":"Resource Management"},{"location":"architecture/longrunning-node-implementation/#usage-patterns","text":"","title":"Usage Patterns"},{"location":"architecture/longrunning-node-implementation/#basic-usage","text":"let node = LongRunningTask :: new ( | ctx | { /* start implementation */ }, | ctx | { /* progress check implementation */ }, | ctx | { /* cancellation implementation */ }, );","title":"Basic Usage"},{"location":"architecture/longrunning-node-implementation/#with-progress-tracking","text":"let node = LongRunningTask :: new ( | ctx | async { for i in 0 .. 100 { process_chunk ( i ) ? ; update_progress ( i as f32 / 100.0 ); } Ok (()) }, | ctx | async { let progress = get_current_progress (); if progress >= 1.0 { Ok ( Progress :: Complete ( DefaultAction :: Next )) } else { Ok ( Progress :: Running ( progress )) } }, | ctx | async { /* cancellation implementation */ }, );","title":"With Progress Tracking"},{"location":"architecture/longrunning-node-implementation/#with-error-handling","text":"let node = LongRunningTask :: new ( | ctx | async { if let Err ( e ) = check_preconditions () { return Err ( e . into ()); } start_processing () }, | ctx | async { match check_task_progress () { Ok ( progress ) => Ok ( Progress :: Running ( progress )), Err ( e ) => Ok ( Progress :: Failed ( e . into ())), } }, | ctx | async { cleanup_resources () ? ; Ok (()) }, );","title":"With Error Handling"},{"location":"architecture/longrunning-node-implementation/#testing","text":"The implementation includes comprehensive tests: Unit Tests Task execution Progress tracking Cancellation handling Resource cleanup Integration Tests Complex workflows Error scenarios Performance tests","title":"Testing"},{"location":"architecture/longrunning-node-implementation/#performance-considerations","text":"Execution Efficiency Minimal overhead Efficient progress tracking Resource optimization Memory Usage Optimized state storage Efficient progress updates Minimal allocations","title":"Performance Considerations"},{"location":"architecture/longrunning-node-implementation/#future-improvements","text":"Enhanced Features More progress metrics Advanced cancellation strategies Extended recovery options Performance Optimizations Improved state tracking Better resource utilization Enhanced concurrency","title":"Future Improvements"},{"location":"architecture/longrunning-node-implementation/#related-adrs","text":"ADR-0008: Node Lifecycle Methods ADR-0013: Workflow Patterns","title":"Related ADRs"},{"location":"architecture/node-lifecycle-methods/","text":"Node Lifecycle Methods \u00b6 This document describes the node lifecycle methods in the Floxide framework. Overview \u00b6 The Floxide framework implements a three-phase lifecycle for nodes, providing a clear separation of concerns and enabling specialized behaviors at each stage of node execution. This approach enhances maintainability, testability, and flexibility in workflow design. Lifecycle Phases \u00b6 Each node in the Floxide framework goes through three distinct phases during execution: Preparation ( prep ) : Setup and validation phase Execution ( exec ) : Core execution with potential retry mechanisms Post-processing ( post ) : Determines routing and handles results LifecycleNode Trait \u00b6 The LifecycleNode trait explicitly models this three-phase lifecycle: #[async_trait] pub trait LifecycleNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , Self :: PrepOutput : Clone + Send + Sync + ' static , Self :: ExecOutput : Clone + Send + Sync + ' static , { /// Output type from the preparation phase type PrepOutput ; /// Output type from the execution phase type ExecOutput ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; /// Preparation phase - perform setup and validation async fn prep ( & self , ctx : & mut Context ) -> Result < Self :: PrepOutput , FloxideError > ; /// Execution phase - perform the main work async fn exec ( & self , prep_result : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > ; /// Post-processing phase - determine routing async fn post ( & self , exec_result : Self :: ExecOutput ) -> Result < Action , FloxideError > ; } Phase Responsibilities \u00b6 Preparation Phase \u00b6 The preparation phase is responsible for: Validating input data Setting up resources Performing preliminary checks Preparing data for execution Example implementation: async fn prep ( & self , ctx : & mut MyContext ) -> Result < PrepData , FloxideError > { // Validate input if ctx . input . is_empty () { return Err ( FloxideError :: ValidationFailed ( \"Input cannot be empty\" . to_string ())); } // Prepare data for execution let prep_data = PrepData { input : ctx . input . clone (), timestamp : Utc :: now (), }; Ok ( prep_data ) } Execution Phase \u00b6 The execution phase is responsible for: Performing the main work of the node Handling retries for transient failures Processing data Producing execution results Example implementation: async fn exec ( & self , prep_result : PrepData ) -> Result < ExecData , FloxideError > { // Perform main work let result = process_data ( & prep_result . input ) ? ; // Create execution result let exec_data = ExecData { result , processing_time : Utc :: now () - prep_result . timestamp , }; Ok ( exec_data ) } Post-processing Phase \u00b6 The post-processing phase is responsible for: Determining the next action (routing) Cleaning up resources Logging results Preparing data for the next node Example implementation: async fn post ( & self , exec_result : ExecData ) -> Result < Action , FloxideError > { // Determine routing based on execution result let action = if exec_result . result > 100 { Action :: Route ( \"high_value_path\" ) } else { Action :: Route ( \"standard_path\" ) }; // Log processing time log :: info ! ( \"Node {} completed in {:?}\" , self . id (), exec_result . processing_time ); Ok ( action ) } Adapter Pattern \u00b6 To maintain compatibility with the existing Node trait, the framework uses adapter patterns: impl < T , C , A > Node for T where T : LifecycleNode < C , A > , C : Send + Sync + ' static , A : ActionType + Send + Sync + ' static , { type Context = C ; type Action = A ; async fn run ( & self , ctx : & mut Self :: Context ) -> Result < Self :: Action , FloxideError > { let prep_result = self . prep ( ctx ). await ? ; let exec_result = self . exec ( prep_result ). await ? ; let action = self . post ( exec_result ). await ? ; Ok ( action ) } } Benefits \u00b6 The three-phase lifecycle approach offers several benefits: Separation of Concerns : Each phase has a clear, distinct responsibility Testability : Each phase can be tested independently Flexibility : Specialized behaviors can be implemented for each phase Error Handling : Different error handling strategies can be applied to each phase Observability : Metrics and logging can be added at phase boundaries Best Practices \u00b6 When implementing nodes with the lifecycle pattern: Keep Phases Focused : Each phase should have a single responsibility Minimize State : Pass necessary data between phases through return values Handle Errors Appropriately : Use different error handling strategies for each phase Consider Retries : Implement retries in the execution phase for transient failures Add Observability : Log important events at phase boundaries Conclusion \u00b6 The node lifecycle methods in the Floxide framework provide a powerful pattern for implementing workflow nodes with clear separation of concerns. By following this pattern, developers can create maintainable, testable, and flexible workflow components. For more detailed information on the node lifecycle methods, refer to the Node Lifecycle Methods ADR .","title":"Node Lifecycle Methods"},{"location":"architecture/node-lifecycle-methods/#node-lifecycle-methods","text":"This document describes the node lifecycle methods in the Floxide framework.","title":"Node Lifecycle Methods"},{"location":"architecture/node-lifecycle-methods/#overview","text":"The Floxide framework implements a three-phase lifecycle for nodes, providing a clear separation of concerns and enabling specialized behaviors at each stage of node execution. This approach enhances maintainability, testability, and flexibility in workflow design.","title":"Overview"},{"location":"architecture/node-lifecycle-methods/#lifecycle-phases","text":"Each node in the Floxide framework goes through three distinct phases during execution: Preparation ( prep ) : Setup and validation phase Execution ( exec ) : Core execution with potential retry mechanisms Post-processing ( post ) : Determines routing and handles results","title":"Lifecycle Phases"},{"location":"architecture/node-lifecycle-methods/#lifecyclenode-trait","text":"The LifecycleNode trait explicitly models this three-phase lifecycle: #[async_trait] pub trait LifecycleNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static , Self :: PrepOutput : Clone + Send + Sync + ' static , Self :: ExecOutput : Clone + Send + Sync + ' static , { /// Output type from the preparation phase type PrepOutput ; /// Output type from the execution phase type ExecOutput ; /// Get the node's unique identifier fn id ( & self ) -> NodeId ; /// Preparation phase - perform setup and validation async fn prep ( & self , ctx : & mut Context ) -> Result < Self :: PrepOutput , FloxideError > ; /// Execution phase - perform the main work async fn exec ( & self , prep_result : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > ; /// Post-processing phase - determine routing async fn post ( & self , exec_result : Self :: ExecOutput ) -> Result < Action , FloxideError > ; }","title":"LifecycleNode Trait"},{"location":"architecture/node-lifecycle-methods/#phase-responsibilities","text":"","title":"Phase Responsibilities"},{"location":"architecture/node-lifecycle-methods/#preparation-phase","text":"The preparation phase is responsible for: Validating input data Setting up resources Performing preliminary checks Preparing data for execution Example implementation: async fn prep ( & self , ctx : & mut MyContext ) -> Result < PrepData , FloxideError > { // Validate input if ctx . input . is_empty () { return Err ( FloxideError :: ValidationFailed ( \"Input cannot be empty\" . to_string ())); } // Prepare data for execution let prep_data = PrepData { input : ctx . input . clone (), timestamp : Utc :: now (), }; Ok ( prep_data ) }","title":"Preparation Phase"},{"location":"architecture/node-lifecycle-methods/#execution-phase","text":"The execution phase is responsible for: Performing the main work of the node Handling retries for transient failures Processing data Producing execution results Example implementation: async fn exec ( & self , prep_result : PrepData ) -> Result < ExecData , FloxideError > { // Perform main work let result = process_data ( & prep_result . input ) ? ; // Create execution result let exec_data = ExecData { result , processing_time : Utc :: now () - prep_result . timestamp , }; Ok ( exec_data ) }","title":"Execution Phase"},{"location":"architecture/node-lifecycle-methods/#post-processing-phase","text":"The post-processing phase is responsible for: Determining the next action (routing) Cleaning up resources Logging results Preparing data for the next node Example implementation: async fn post ( & self , exec_result : ExecData ) -> Result < Action , FloxideError > { // Determine routing based on execution result let action = if exec_result . result > 100 { Action :: Route ( \"high_value_path\" ) } else { Action :: Route ( \"standard_path\" ) }; // Log processing time log :: info ! ( \"Node {} completed in {:?}\" , self . id (), exec_result . processing_time ); Ok ( action ) }","title":"Post-processing Phase"},{"location":"architecture/node-lifecycle-methods/#adapter-pattern","text":"To maintain compatibility with the existing Node trait, the framework uses adapter patterns: impl < T , C , A > Node for T where T : LifecycleNode < C , A > , C : Send + Sync + ' static , A : ActionType + Send + Sync + ' static , { type Context = C ; type Action = A ; async fn run ( & self , ctx : & mut Self :: Context ) -> Result < Self :: Action , FloxideError > { let prep_result = self . prep ( ctx ). await ? ; let exec_result = self . exec ( prep_result ). await ? ; let action = self . post ( exec_result ). await ? ; Ok ( action ) } }","title":"Adapter Pattern"},{"location":"architecture/node-lifecycle-methods/#benefits","text":"The three-phase lifecycle approach offers several benefits: Separation of Concerns : Each phase has a clear, distinct responsibility Testability : Each phase can be tested independently Flexibility : Specialized behaviors can be implemented for each phase Error Handling : Different error handling strategies can be applied to each phase Observability : Metrics and logging can be added at phase boundaries","title":"Benefits"},{"location":"architecture/node-lifecycle-methods/#best-practices","text":"When implementing nodes with the lifecycle pattern: Keep Phases Focused : Each phase should have a single responsibility Minimize State : Pass necessary data between phases through return values Handle Errors Appropriately : Use different error handling strategies for each phase Consider Retries : Implement retries in the execution phase for transient failures Add Observability : Log important events at phase boundaries","title":"Best Practices"},{"location":"architecture/node-lifecycle-methods/#conclusion","text":"The node lifecycle methods in the Floxide framework provide a powerful pattern for implementing workflow nodes with clear separation of concerns. By following this pattern, developers can create maintainable, testable, and flexible workflow components. For more detailed information on the node lifecycle methods, refer to the Node Lifecycle Methods ADR .","title":"Conclusion"},{"location":"architecture/project-structure/","text":"Project Structure \u00b6 This document describes the overall structure of the Floxide framework codebase. Workspace Organization \u00b6 The Floxide framework is organized as a Cargo workspace with multiple crates, each with a specific responsibility: floxide/ \u251c\u2500\u2500 Cargo.toml # Workspace definition \u251c\u2500\u2500 floxide-core/ # Core abstractions and interfaces \u251c\u2500\u2500 floxide-transform/ # Functional transformation capabilities \u251c\u2500\u2500 floxide-event/ # Event-driven workflow support \u251c\u2500\u2500 floxide-timer/ # Timer and scheduling functionality \u251c\u2500\u2500 floxide-reactive/ # Reactive programming patterns \u251c\u2500\u2500 floxide-longrunning/ # Long-running task support \u251c\u2500\u2500 floxide-async/ # (Deprecated) Alias for floxide-transform \u2514\u2500\u2500 docs/ # Documentation \u251c\u2500\u2500 adrs/ # Architectural Decision Records \u251c\u2500\u2500 api/ # API Documentation \u251c\u2500\u2500 architecture/ # Architecture Documentation \u2514\u2500\u2500 examples/ # Example Code Crate Responsibilities \u00b6 floxide-core \u00b6 The floxide-core crate provides the fundamental abstractions and interfaces for the framework: Node trait and lifecycle methods (prep, exec, post) Context and state management Workflow orchestration Action types and transitions Error handling with custom error types floxide-transform \u00b6 The floxide-transform crate (formerly floxide-async ) provides functional transformation capabilities: Explicit input/output type transformations Custom error types per transformation Three-phase transformation lifecycle Functional composition patterns floxide-event \u00b6 The floxide-event crate provides event-driven workflow support: Event emission and handling Event-based routing Event context management Pub/sub patterns floxide-timer \u00b6 The floxide-timer crate provides timer and scheduling functionality: Multiple schedule types (Once, Periodic, Cron) Timer workflow composition Timeout handling Scheduled task orchestration floxide-reactive \u00b6 The floxide-reactive crate provides reactive programming patterns: Stream-based processing Backpressure handling Change detection Reactive node composition floxide-longrunning \u00b6 The floxide-longrunning crate provides support for long-running tasks: Background processing Progress tracking Cancellation support Resource cleanup floxide-async (Deprecated) \u00b6 This crate is deprecated and exists only for backward compatibility. It re-exports everything from floxide-transform . Users should migrate to floxide-transform . Documentation Structure \u00b6 API Documentation ( /docs/api/ ) \u00b6 Detailed API documentation for each crate: - floxide-core.md - Core abstractions and interfaces - floxide-reactive.md - Reactive programming patterns - floxide-timer.md - Timer and scheduling - floxide-transform.md - Transformation patterns Architecture Documentation ( /docs/architecture/ ) \u00b6 High-level architectural documentation: - Design decisions and patterns - Component interactions - Implementation guidelines - Performance considerations Examples ( /docs/examples/ ) \u00b6 Comprehensive examples demonstrating framework usage: - basic-workflow.md - Core workflow concepts - reactive-node.md - Reactive programming patterns - timer-node.md - Timer and scheduling - transform-node.md - Data transformation - event-driven-workflow.md - Event handling - longrunning-node.md - Long-running tasks ADRs ( /docs/adrs/ ) \u00b6 Architectural Decision Records documenting all significant decisions: - Numbered sequentially (e.g., ADR-0001) - Each ADR covers one architectural decision - Includes context, consequences, and alternatives - Marks superseded decisions Dependencies Between Crates \u00b6 The crates have the following dependency relationships: All crates depend on floxide-core floxide-async depends on floxide-transform Dependencies are kept minimal to allow users to include only what they need Each crate can be used independently (except floxide-async ) Testing Structure \u00b6 Each crate follows a consistent testing structure: Unit tests in src/tests/ modules Integration tests in tests/ directory Example code in /docs/examples/ Property-based tests where appropriate Future Development \u00b6 Planned future additions: - Batch processing capabilities - Enhanced monitoring and metrics - Additional node types and patterns - Extended workflow composition features","title":"Project Structure"},{"location":"architecture/project-structure/#project-structure","text":"This document describes the overall structure of the Floxide framework codebase.","title":"Project Structure"},{"location":"architecture/project-structure/#workspace-organization","text":"The Floxide framework is organized as a Cargo workspace with multiple crates, each with a specific responsibility: floxide/ \u251c\u2500\u2500 Cargo.toml # Workspace definition \u251c\u2500\u2500 floxide-core/ # Core abstractions and interfaces \u251c\u2500\u2500 floxide-transform/ # Functional transformation capabilities \u251c\u2500\u2500 floxide-event/ # Event-driven workflow support \u251c\u2500\u2500 floxide-timer/ # Timer and scheduling functionality \u251c\u2500\u2500 floxide-reactive/ # Reactive programming patterns \u251c\u2500\u2500 floxide-longrunning/ # Long-running task support \u251c\u2500\u2500 floxide-async/ # (Deprecated) Alias for floxide-transform \u2514\u2500\u2500 docs/ # Documentation \u251c\u2500\u2500 adrs/ # Architectural Decision Records \u251c\u2500\u2500 api/ # API Documentation \u251c\u2500\u2500 architecture/ # Architecture Documentation \u2514\u2500\u2500 examples/ # Example Code","title":"Workspace Organization"},{"location":"architecture/project-structure/#crate-responsibilities","text":"","title":"Crate Responsibilities"},{"location":"architecture/project-structure/#floxide-core","text":"The floxide-core crate provides the fundamental abstractions and interfaces for the framework: Node trait and lifecycle methods (prep, exec, post) Context and state management Workflow orchestration Action types and transitions Error handling with custom error types","title":"floxide-core"},{"location":"architecture/project-structure/#floxide-transform","text":"The floxide-transform crate (formerly floxide-async ) provides functional transformation capabilities: Explicit input/output type transformations Custom error types per transformation Three-phase transformation lifecycle Functional composition patterns","title":"floxide-transform"},{"location":"architecture/project-structure/#floxide-event","text":"The floxide-event crate provides event-driven workflow support: Event emission and handling Event-based routing Event context management Pub/sub patterns","title":"floxide-event"},{"location":"architecture/project-structure/#floxide-timer","text":"The floxide-timer crate provides timer and scheduling functionality: Multiple schedule types (Once, Periodic, Cron) Timer workflow composition Timeout handling Scheduled task orchestration","title":"floxide-timer"},{"location":"architecture/project-structure/#floxide-reactive","text":"The floxide-reactive crate provides reactive programming patterns: Stream-based processing Backpressure handling Change detection Reactive node composition","title":"floxide-reactive"},{"location":"architecture/project-structure/#floxide-longrunning","text":"The floxide-longrunning crate provides support for long-running tasks: Background processing Progress tracking Cancellation support Resource cleanup","title":"floxide-longrunning"},{"location":"architecture/project-structure/#floxide-async-deprecated","text":"This crate is deprecated and exists only for backward compatibility. It re-exports everything from floxide-transform . Users should migrate to floxide-transform .","title":"floxide-async (Deprecated)"},{"location":"architecture/project-structure/#documentation-structure","text":"","title":"Documentation Structure"},{"location":"architecture/project-structure/#api-documentation-docsapi","text":"Detailed API documentation for each crate: - floxide-core.md - Core abstractions and interfaces - floxide-reactive.md - Reactive programming patterns - floxide-timer.md - Timer and scheduling - floxide-transform.md - Transformation patterns","title":"API Documentation (/docs/api/)"},{"location":"architecture/project-structure/#architecture-documentation-docsarchitecture","text":"High-level architectural documentation: - Design decisions and patterns - Component interactions - Implementation guidelines - Performance considerations","title":"Architecture Documentation (/docs/architecture/)"},{"location":"architecture/project-structure/#examples-docsexamples","text":"Comprehensive examples demonstrating framework usage: - basic-workflow.md - Core workflow concepts - reactive-node.md - Reactive programming patterns - timer-node.md - Timer and scheduling - transform-node.md - Data transformation - event-driven-workflow.md - Event handling - longrunning-node.md - Long-running tasks","title":"Examples (/docs/examples/)"},{"location":"architecture/project-structure/#adrs-docsadrs","text":"Architectural Decision Records documenting all significant decisions: - Numbered sequentially (e.g., ADR-0001) - Each ADR covers one architectural decision - Includes context, consequences, and alternatives - Marks superseded decisions","title":"ADRs (/docs/adrs/)"},{"location":"architecture/project-structure/#dependencies-between-crates","text":"The crates have the following dependency relationships: All crates depend on floxide-core floxide-async depends on floxide-transform Dependencies are kept minimal to allow users to include only what they need Each crate can be used independently (except floxide-async )","title":"Dependencies Between Crates"},{"location":"architecture/project-structure/#testing-structure","text":"Each crate follows a consistent testing structure: Unit tests in src/tests/ modules Integration tests in tests/ directory Example code in /docs/examples/ Property-based tests where appropriate","title":"Testing Structure"},{"location":"architecture/project-structure/#future-development","text":"Planned future additions: - Batch processing capabilities - Enhanced monitoring and metrics - Additional node types and patterns - Extended workflow composition features","title":"Future Development"},{"location":"architecture/reactive-node-implementation/","text":"Reactive Node Implementation \u00b6 This document describes the implementation details of reactive nodes in the Floxide framework. Overview \u00b6 Reactive nodes in Floxide provide stream-based processing capabilities with proper backpressure handling and error management. Core Components \u00b6 ReactiveNode Trait \u00b6 The ReactiveNode trait defines the core interface for reactive processing: #[async_trait] pub trait ReactiveNode < Change , Context , Action > : Send + Sync where Change : Send + Sync + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static + Debug , { async fn watch ( & self ) -> Result < Box < dyn Stream < Item = Change > + Send + Unpin > , FloxideError > ; async fn react_to_change ( & self , change : Change , ctx : & mut Context ) -> Result < Action , FloxideError > ; fn id ( & self ) -> NodeId ; } CustomReactiveNode \u00b6 The CustomReactiveNode provides a flexible implementation that allows users to define their own watch and react functions: pub struct CustomReactiveNode < Change , Context , Action , WatchFn , ReactFn > { watch_fn : WatchFn , react_fn : ReactFn , id : NodeId , _phantom : PhantomData < ( Change , Context , Action ) > , } Implementation Details \u00b6 Stream Management \u00b6 Watch Function Creates and manages the underlying stream Handles backpressure through Tokio's async streams Provides proper cleanup on drop Change Detection Processes changes as they arrive Maintains order of changes Handles errors gracefully Context Updates Updates context based on changes Maintains thread safety Provides atomic updates when needed Error Handling \u00b6 Stream Errors Proper error propagation Recovery mechanisms Error context preservation Processing Errors Custom error types Error recovery strategies Error reporting Resource Management \u00b6 Memory Usage Efficient stream buffering Proper cleanup of resources Memory leak prevention Thread Safety Thread-safe context access Safe concurrent processing Proper synchronization Usage Patterns \u00b6 Basic Usage \u00b6 let node = CustomReactiveNode :: new ( || { /* watch implementation */ }, | change , ctx | { /* react implementation */ }, ); With Error Handling \u00b6 let node = CustomReactiveNode :: new ( || { if let Err ( e ) = check_preconditions () { return Err ( e . into ()); } Ok ( create_stream ()) }, | change , ctx | { match process_change ( change ) { Ok ( result ) => Ok ( DefaultAction :: change_detected ()), Err ( e ) => Err ( e . into ()), } }, ); With Backpressure \u00b6 let node = CustomReactiveNode :: new ( || { Ok ( Box :: new ( stream :: iter ( 0 .. 100 ) . throttle ( Duration :: from_millis ( 100 )) )) }, | change , ctx | { /* react implementation */ }, ); Testing \u00b6 The implementation includes comprehensive tests: Unit Tests Stream creation Change processing Error handling Resource cleanup Integration Tests End-to-end workflows Complex scenarios Performance tests Performance Considerations \u00b6 Stream Efficiency Minimal allocations Efficient buffering Proper backpressure Processing Overhead Optimized change detection Efficient context updates Minimal locking Future Improvements \u00b6 Enhanced Features More stream combinators Additional error recovery strategies Extended composition patterns Performance Optimizations Improved buffering strategies Better resource utilization Enhanced concurrency Related ADRs \u00b6 ADR-0008: Event-Driven Node Extensions ADR-0013: Workflow Patterns","title":"Reactive Node Implementation"},{"location":"architecture/reactive-node-implementation/#reactive-node-implementation","text":"This document describes the implementation details of reactive nodes in the Floxide framework.","title":"Reactive Node Implementation"},{"location":"architecture/reactive-node-implementation/#overview","text":"Reactive nodes in Floxide provide stream-based processing capabilities with proper backpressure handling and error management.","title":"Overview"},{"location":"architecture/reactive-node-implementation/#core-components","text":"","title":"Core Components"},{"location":"architecture/reactive-node-implementation/#reactivenode-trait","text":"The ReactiveNode trait defines the core interface for reactive processing: #[async_trait] pub trait ReactiveNode < Change , Context , Action > : Send + Sync where Change : Send + Sync + ' static , Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static + Debug , { async fn watch ( & self ) -> Result < Box < dyn Stream < Item = Change > + Send + Unpin > , FloxideError > ; async fn react_to_change ( & self , change : Change , ctx : & mut Context ) -> Result < Action , FloxideError > ; fn id ( & self ) -> NodeId ; }","title":"ReactiveNode Trait"},{"location":"architecture/reactive-node-implementation/#customreactivenode","text":"The CustomReactiveNode provides a flexible implementation that allows users to define their own watch and react functions: pub struct CustomReactiveNode < Change , Context , Action , WatchFn , ReactFn > { watch_fn : WatchFn , react_fn : ReactFn , id : NodeId , _phantom : PhantomData < ( Change , Context , Action ) > , }","title":"CustomReactiveNode"},{"location":"architecture/reactive-node-implementation/#implementation-details","text":"","title":"Implementation Details"},{"location":"architecture/reactive-node-implementation/#stream-management","text":"Watch Function Creates and manages the underlying stream Handles backpressure through Tokio's async streams Provides proper cleanup on drop Change Detection Processes changes as they arrive Maintains order of changes Handles errors gracefully Context Updates Updates context based on changes Maintains thread safety Provides atomic updates when needed","title":"Stream Management"},{"location":"architecture/reactive-node-implementation/#error-handling","text":"Stream Errors Proper error propagation Recovery mechanisms Error context preservation Processing Errors Custom error types Error recovery strategies Error reporting","title":"Error Handling"},{"location":"architecture/reactive-node-implementation/#resource-management","text":"Memory Usage Efficient stream buffering Proper cleanup of resources Memory leak prevention Thread Safety Thread-safe context access Safe concurrent processing Proper synchronization","title":"Resource Management"},{"location":"architecture/reactive-node-implementation/#usage-patterns","text":"","title":"Usage Patterns"},{"location":"architecture/reactive-node-implementation/#basic-usage","text":"let node = CustomReactiveNode :: new ( || { /* watch implementation */ }, | change , ctx | { /* react implementation */ }, );","title":"Basic Usage"},{"location":"architecture/reactive-node-implementation/#with-error-handling","text":"let node = CustomReactiveNode :: new ( || { if let Err ( e ) = check_preconditions () { return Err ( e . into ()); } Ok ( create_stream ()) }, | change , ctx | { match process_change ( change ) { Ok ( result ) => Ok ( DefaultAction :: change_detected ()), Err ( e ) => Err ( e . into ()), } }, );","title":"With Error Handling"},{"location":"architecture/reactive-node-implementation/#with-backpressure","text":"let node = CustomReactiveNode :: new ( || { Ok ( Box :: new ( stream :: iter ( 0 .. 100 ) . throttle ( Duration :: from_millis ( 100 )) )) }, | change , ctx | { /* react implementation */ }, );","title":"With Backpressure"},{"location":"architecture/reactive-node-implementation/#testing","text":"The implementation includes comprehensive tests: Unit Tests Stream creation Change processing Error handling Resource cleanup Integration Tests End-to-end workflows Complex scenarios Performance tests","title":"Testing"},{"location":"architecture/reactive-node-implementation/#performance-considerations","text":"Stream Efficiency Minimal allocations Efficient buffering Proper backpressure Processing Overhead Optimized change detection Efficient context updates Minimal locking","title":"Performance Considerations"},{"location":"architecture/reactive-node-implementation/#future-improvements","text":"Enhanced Features More stream combinators Additional error recovery strategies Extended composition patterns Performance Optimizations Improved buffering strategies Better resource utilization Enhanced concurrency","title":"Future Improvements"},{"location":"architecture/reactive-node-implementation/#related-adrs","text":"ADR-0008: Event-Driven Node Extensions ADR-0013: Workflow Patterns","title":"Related ADRs"},{"location":"architecture/timer-node-implementation/","text":"Timer Node Implementation \u00b6 This document describes the implementation details of timer nodes in the Floxide framework. Overview \u00b6 Timer nodes in Floxide provide scheduling capabilities with support for various schedule types and proper error handling. Core Components \u00b6 TimerNode Trait \u00b6 The TimerNode trait defines the core interface for scheduled execution: #[async_trait] pub trait TimerNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static + Default + Debug , { fn schedule ( & self ) -> Schedule ; async fn execute_on_schedule ( & self , ctx : & mut Context ) -> Result < Action , FloxideError > ; fn id ( & self ) -> NodeId ; } Schedule Types \u00b6 The Schedule enum supports different scheduling patterns: pub enum Schedule { Once ( DateTime < Utc > ), Periodic ( ChronoDuration ), Cron ( String ), } Implementation Details \u00b6 Schedule Management \u00b6 Schedule Types One-time execution Periodic execution Cron-based scheduling Time Handling UTC time management Timezone considerations Leap second handling Execution Control Start/stop capabilities Pause/resume support Graceful shutdown Error Handling \u00b6 Execution Errors Proper error propagation Retry mechanisms Error reporting Schedule Errors Invalid schedule detection Schedule parsing errors Recovery strategies Resource Management \u00b6 Timer Resources Efficient timer allocation Resource cleanup Memory management Thread Safety Thread-safe execution Safe schedule updates Proper synchronization Usage Patterns \u00b6 Basic Usage \u00b6 let node = SimpleTimer :: new ( Schedule :: Periodic ( ChronoDuration :: minutes ( 5 )), | ctx | { /* execution implementation */ }, ); With Cron Schedule \u00b6 let node = SimpleTimer :: new ( Schedule :: Cron ( \"0 2 * * *\" . to_string ()), // Run at 2 AM daily | ctx | { match perform_daily_task ( ctx ) { Ok ( _ ) => Ok ( DefaultAction :: Next ), Err ( e ) => Err ( e . into ()), } }, ); With Error Handling \u00b6 let node = SimpleTimer :: new ( Schedule :: Once ( Utc :: now () + ChronoDuration :: hours ( 1 )), | ctx | { if let Err ( e ) = check_preconditions () { return Err ( e . into ()); } Ok ( DefaultAction :: complete ()) }, ); Testing \u00b6 The implementation includes comprehensive tests: Unit Tests Schedule parsing Execution timing Error handling Resource cleanup Integration Tests Complex schedules Long-running scenarios Edge cases Performance Considerations \u00b6 Timer Efficiency Minimal allocations Efficient scheduling Low overhead Resource Usage Optimized timer pools Efficient thread usage Minimal locking Future Improvements \u00b6 Enhanced Features More schedule types Advanced retry strategies Schedule composition Performance Optimizations Improved timer allocation Better resource sharing Enhanced concurrency Related ADRs \u00b6 ADR-0008: Node Lifecycle Methods ADR-0013: Workflow Patterns","title":"Timer Node Implementation"},{"location":"architecture/timer-node-implementation/#timer-node-implementation","text":"This document describes the implementation details of timer nodes in the Floxide framework.","title":"Timer Node Implementation"},{"location":"architecture/timer-node-implementation/#overview","text":"Timer nodes in Floxide provide scheduling capabilities with support for various schedule types and proper error handling.","title":"Overview"},{"location":"architecture/timer-node-implementation/#core-components","text":"","title":"Core Components"},{"location":"architecture/timer-node-implementation/#timernode-trait","text":"The TimerNode trait defines the core interface for scheduled execution: #[async_trait] pub trait TimerNode < Context , Action > : Send + Sync where Context : Send + Sync + ' static , Action : ActionType + Send + Sync + ' static + Default + Debug , { fn schedule ( & self ) -> Schedule ; async fn execute_on_schedule ( & self , ctx : & mut Context ) -> Result < Action , FloxideError > ; fn id ( & self ) -> NodeId ; }","title":"TimerNode Trait"},{"location":"architecture/timer-node-implementation/#schedule-types","text":"The Schedule enum supports different scheduling patterns: pub enum Schedule { Once ( DateTime < Utc > ), Periodic ( ChronoDuration ), Cron ( String ), }","title":"Schedule Types"},{"location":"architecture/timer-node-implementation/#implementation-details","text":"","title":"Implementation Details"},{"location":"architecture/timer-node-implementation/#schedule-management","text":"Schedule Types One-time execution Periodic execution Cron-based scheduling Time Handling UTC time management Timezone considerations Leap second handling Execution Control Start/stop capabilities Pause/resume support Graceful shutdown","title":"Schedule Management"},{"location":"architecture/timer-node-implementation/#error-handling","text":"Execution Errors Proper error propagation Retry mechanisms Error reporting Schedule Errors Invalid schedule detection Schedule parsing errors Recovery strategies","title":"Error Handling"},{"location":"architecture/timer-node-implementation/#resource-management","text":"Timer Resources Efficient timer allocation Resource cleanup Memory management Thread Safety Thread-safe execution Safe schedule updates Proper synchronization","title":"Resource Management"},{"location":"architecture/timer-node-implementation/#usage-patterns","text":"","title":"Usage Patterns"},{"location":"architecture/timer-node-implementation/#basic-usage","text":"let node = SimpleTimer :: new ( Schedule :: Periodic ( ChronoDuration :: minutes ( 5 )), | ctx | { /* execution implementation */ }, );","title":"Basic Usage"},{"location":"architecture/timer-node-implementation/#with-cron-schedule","text":"let node = SimpleTimer :: new ( Schedule :: Cron ( \"0 2 * * *\" . to_string ()), // Run at 2 AM daily | ctx | { match perform_daily_task ( ctx ) { Ok ( _ ) => Ok ( DefaultAction :: Next ), Err ( e ) => Err ( e . into ()), } }, );","title":"With Cron Schedule"},{"location":"architecture/timer-node-implementation/#with-error-handling","text":"let node = SimpleTimer :: new ( Schedule :: Once ( Utc :: now () + ChronoDuration :: hours ( 1 )), | ctx | { if let Err ( e ) = check_preconditions () { return Err ( e . into ()); } Ok ( DefaultAction :: complete ()) }, );","title":"With Error Handling"},{"location":"architecture/timer-node-implementation/#testing","text":"The implementation includes comprehensive tests: Unit Tests Schedule parsing Execution timing Error handling Resource cleanup Integration Tests Complex schedules Long-running scenarios Edge cases","title":"Testing"},{"location":"architecture/timer-node-implementation/#performance-considerations","text":"Timer Efficiency Minimal allocations Efficient scheduling Low overhead Resource Usage Optimized timer pools Efficient thread usage Minimal locking","title":"Performance Considerations"},{"location":"architecture/timer-node-implementation/#future-improvements","text":"Enhanced Features More schedule types Advanced retry strategies Schedule composition Performance Optimizations Improved timer allocation Better resource sharing Enhanced concurrency","title":"Future Improvements"},{"location":"architecture/timer-node-implementation/#related-adrs","text":"ADR-0008: Node Lifecycle Methods ADR-0013: Workflow Patterns","title":"Related ADRs"},{"location":"architecture/transform-node-implementation/","text":"Transform Node Implementation \u00b6 This document describes the implementation details of transform nodes in the Floxide framework. Overview \u00b6 Transform nodes in Floxide provide a functional approach to data transformation with explicit input and output types, proper error handling, and a three-phase lifecycle. Core Components \u00b6 TransformNode Trait \u00b6 The TransformNode trait defines the core interface for data transformation: #[async_trait] pub trait TransformNode < Input , Output , Error > : Send + Sync where Input : Send + Sync + ' static , Output : Send + Sync + ' static , Error : std :: error :: Error + Send + Sync + ' static , { async fn prep ( & self , input : Input ) -> Result < Input , Error > ; async fn exec ( & self , input : Input ) -> Result < Output , Error > ; async fn post ( & self , output : Output ) -> Result < Output , Error > ; } Custom Transform Node \u00b6 The CustomTransform provides a flexible implementation: pub struct CustomTransform < Input , Output , Error , P , E , T > { prep_fn : P , exec_fn : E , post_fn : T , _phantom : PhantomData < ( Input , Output , Error ) > , } Implementation Details \u00b6 Transformation Phases \u00b6 Preparation Phase (prep) Input validation Data normalization Resource initialization Execution Phase (exec) Core transformation logic Error handling State management Post-Processing Phase (post) Output validation Resource cleanup Result formatting Error Handling \u00b6 Custom Error Types Type-safe error handling Error context preservation Error recovery strategies Phase-Specific Errors Preparation errors Execution errors Post-processing errors Resource Management \u00b6 Memory Usage Efficient transformations Resource cleanup Memory leak prevention Thread Safety Thread-safe execution Safe state updates Proper synchronization Usage Patterns \u00b6 Basic Usage \u00b6 let node = CustomTransform :: new ( | input : Input | async { Ok ( input ) }, | input : Input | async { transform ( input ) }, | output : Output | async { Ok ( output ) }, ); With Validation \u00b6 let node = CustomTransform :: new ( | input : Input | async { if ! is_valid ( & input ) { return Err ( TransformError :: ValidationError ( \"Invalid input\" . into ())); } Ok ( input ) }, | input : Input | async { transform ( input ) }, | output : Output | async { validate_output ( & output ) ? ; Ok ( output ) }, ); With Resource Management \u00b6 let node = CustomTransform :: new ( | input : Input | async { initialize_resources () ? ; Ok ( input ) }, | input : Input | async { transform ( input ) }, | output : Output | async { cleanup_resources () ? ; Ok ( output ) }, ); Testing \u00b6 The implementation includes comprehensive tests: Unit Tests Phase execution Error handling Resource management Integration Tests Complex transformations Error scenarios Performance tests Performance Considerations \u00b6 Transformation Efficiency Minimal allocations Efficient algorithms Resource optimization Memory Usage Optimized data structures Proper cleanup Minimal copying Future Improvements \u00b6 Enhanced Features More transformation utilities Additional validation patterns Extended error handling Performance Optimizations Improved algorithms Better resource usage Enhanced concurrency Related Documentation \u00b6 Transform Node Example Error Handling Guide Node Lifecycle Methods","title":"Transform Node Implementation"},{"location":"architecture/transform-node-implementation/#transform-node-implementation","text":"This document describes the implementation details of transform nodes in the Floxide framework.","title":"Transform Node Implementation"},{"location":"architecture/transform-node-implementation/#overview","text":"Transform nodes in Floxide provide a functional approach to data transformation with explicit input and output types, proper error handling, and a three-phase lifecycle.","title":"Overview"},{"location":"architecture/transform-node-implementation/#core-components","text":"","title":"Core Components"},{"location":"architecture/transform-node-implementation/#transformnode-trait","text":"The TransformNode trait defines the core interface for data transformation: #[async_trait] pub trait TransformNode < Input , Output , Error > : Send + Sync where Input : Send + Sync + ' static , Output : Send + Sync + ' static , Error : std :: error :: Error + Send + Sync + ' static , { async fn prep ( & self , input : Input ) -> Result < Input , Error > ; async fn exec ( & self , input : Input ) -> Result < Output , Error > ; async fn post ( & self , output : Output ) -> Result < Output , Error > ; }","title":"TransformNode Trait"},{"location":"architecture/transform-node-implementation/#custom-transform-node","text":"The CustomTransform provides a flexible implementation: pub struct CustomTransform < Input , Output , Error , P , E , T > { prep_fn : P , exec_fn : E , post_fn : T , _phantom : PhantomData < ( Input , Output , Error ) > , }","title":"Custom Transform Node"},{"location":"architecture/transform-node-implementation/#implementation-details","text":"","title":"Implementation Details"},{"location":"architecture/transform-node-implementation/#transformation-phases","text":"Preparation Phase (prep) Input validation Data normalization Resource initialization Execution Phase (exec) Core transformation logic Error handling State management Post-Processing Phase (post) Output validation Resource cleanup Result formatting","title":"Transformation Phases"},{"location":"architecture/transform-node-implementation/#error-handling","text":"Custom Error Types Type-safe error handling Error context preservation Error recovery strategies Phase-Specific Errors Preparation errors Execution errors Post-processing errors","title":"Error Handling"},{"location":"architecture/transform-node-implementation/#resource-management","text":"Memory Usage Efficient transformations Resource cleanup Memory leak prevention Thread Safety Thread-safe execution Safe state updates Proper synchronization","title":"Resource Management"},{"location":"architecture/transform-node-implementation/#usage-patterns","text":"","title":"Usage Patterns"},{"location":"architecture/transform-node-implementation/#basic-usage","text":"let node = CustomTransform :: new ( | input : Input | async { Ok ( input ) }, | input : Input | async { transform ( input ) }, | output : Output | async { Ok ( output ) }, );","title":"Basic Usage"},{"location":"architecture/transform-node-implementation/#with-validation","text":"let node = CustomTransform :: new ( | input : Input | async { if ! is_valid ( & input ) { return Err ( TransformError :: ValidationError ( \"Invalid input\" . into ())); } Ok ( input ) }, | input : Input | async { transform ( input ) }, | output : Output | async { validate_output ( & output ) ? ; Ok ( output ) }, );","title":"With Validation"},{"location":"architecture/transform-node-implementation/#with-resource-management","text":"let node = CustomTransform :: new ( | input : Input | async { initialize_resources () ? ; Ok ( input ) }, | input : Input | async { transform ( input ) }, | output : Output | async { cleanup_resources () ? ; Ok ( output ) }, );","title":"With Resource Management"},{"location":"architecture/transform-node-implementation/#testing","text":"The implementation includes comprehensive tests: Unit Tests Phase execution Error handling Resource management Integration Tests Complex transformations Error scenarios Performance tests","title":"Testing"},{"location":"architecture/transform-node-implementation/#performance-considerations","text":"Transformation Efficiency Minimal allocations Efficient algorithms Resource optimization Memory Usage Optimized data structures Proper cleanup Minimal copying","title":"Performance Considerations"},{"location":"architecture/transform-node-implementation/#future-improvements","text":"Enhanced Features More transformation utilities Additional validation patterns Extended error handling Performance Optimizations Improved algorithms Better resource usage Enhanced concurrency","title":"Future Improvements"},{"location":"architecture/transform-node-implementation/#related-documentation","text":"Transform Node Example Error Handling Guide Node Lifecycle Methods","title":"Related Documentation"},{"location":"core-concepts/actions/","text":"Actions \u00b6 Actions in Floxide determine the flow of execution in a workflow. After a node completes its execution, it returns an action that indicates what should happen next. This page explains the different types of actions and how to use them to control workflow execution. The Action Concept \u00b6 Actions are the mechanism by which nodes communicate their execution results and control the flow of a workflow. When a node completes its execution, it returns an action that the workflow uses to determine the next step. In Floxide, actions are typically represented as enum variants, allowing for a clear and type-safe way to express different execution paths. Default Actions \u00b6 Floxide provides a DefaultAction enum that covers the most common workflow control patterns: pub enum DefaultAction { Next , Stop , } Next : Continue to the next node in the workflow Stop : Stop the workflow execution Here's how to use the default actions: use floxide_core ::{ lifecycle_node , LifecycleNode , Workflow , DefaultAction }; fn create_processor_node () -> impl LifecycleNode < MessageContext , DefaultAction > { lifecycle_node ( Some ( \"processor\" ), | ctx : & mut MessageContext | async move { Ok ( ctx . input . clone ()) }, | input : String | async move { Ok ( format! ( \"Processed: {}\" , input )) }, | _prep , exec_result , ctx : & mut MessageContext | async move { ctx . result = Some ( exec_result ); // Continue to the next node Ok ( DefaultAction :: Next ) // Or stop the workflow // Ok(DefaultAction::Stop) }, ) } Custom Actions \u00b6 For more complex workflows, you can define custom actions using enums: #[derive(Debug, Clone, PartialEq, Eq)] pub enum OrderAction { Approve , Reject , Review , Error , } fn create_validation_node () -> impl LifecycleNode < OrderContext , OrderAction > { lifecycle_node ( Some ( \"validator\" ), | ctx : & mut OrderContext | async move { Ok ( ctx . order . clone ()) }, | order : Order | async move { // Validate the order let validation_result = validate_order ( & order ); Ok ( validation_result ) }, | _prep , validation_result , ctx : & mut OrderContext | async move { match validation_result { ValidationResult :: Valid => Ok ( OrderAction :: Approve ), ValidationResult :: Invalid => Ok ( OrderAction :: Reject ), ValidationResult :: NeedsReview => Ok ( OrderAction :: Review ), ValidationResult :: Error => Ok ( OrderAction :: Error ), } }, ) } Using Actions in Workflows \u00b6 Actions are used to control the flow of execution in a workflow. You can use the on method to specify which node to execute for each action: let validation_node = Arc :: new ( create_validation_node ()); let approval_node = Arc :: new ( create_approval_node ()); let rejection_node = Arc :: new ( create_rejection_node ()); let review_node = Arc :: new ( create_review_node ()); let error_node = Arc :: new ( create_error_node ()); let mut workflow = Workflow :: new ( validation_node ) . on ( OrderAction :: Approve , approval_node ) . on ( OrderAction :: Reject , rejection_node ) . on ( OrderAction :: Review , review_node ) . on ( OrderAction :: Error , error_node ); This creates a workflow that executes different nodes based on the action returned by the validation node. Action Patterns \u00b6 Linear Flow \u00b6 The simplest action pattern is a linear flow, where each node returns DefaultAction::Next to continue to the next node: let node1 = Arc :: new ( create_node1 ()); let node2 = Arc :: new ( create_node2 ()); let node3 = Arc :: new ( create_node3 ()); let mut workflow = Workflow :: new ( node1 ) . then ( node2 ) . then ( node3 ); Conditional Branching \u00b6 Actions enable conditional branching, where the execution path depends on the result of a node: enum PaymentAction { Success , Failure , Pending , } let payment_node = Arc :: new ( create_payment_node ()); let success_node = Arc :: new ( create_success_node ()); let failure_node = Arc :: new ( create_failure_node ()); let pending_node = Arc :: new ( create_pending_node ()); let mut workflow = Workflow :: new ( payment_node ) . on ( PaymentAction :: Success , success_node ) . on ( PaymentAction :: Failure , failure_node ) . on ( PaymentAction :: Pending , pending_node ); Error Handling \u00b6 Actions can be used to implement error handling patterns: enum ProcessAction { Success , Error , Retry , } let process_node = Arc :: new ( create_process_node ()); let success_node = Arc :: new ( create_success_node ()); let error_node = Arc :: new ( create_error_node ()); let retry_node = Arc :: new ( create_retry_node ()); let mut workflow = Workflow :: new ( process_node ) . on ( ProcessAction :: Success , success_node ) . on ( ProcessAction :: Error , error_node ) . on ( ProcessAction :: Retry , retry_node ); State Machine \u00b6 Actions can be used to implement a state machine, where each node represents a state and actions represent transitions: enum OrderState { Created , Validated , Paid , Shipped , Delivered , Cancelled , } let created_node = Arc :: new ( create_created_node ()); let validated_node = Arc :: new ( create_validated_node ()); let paid_node = Arc :: new ( create_paid_node ()); let shipped_node = Arc :: new ( create_shipped_node ()); let delivered_node = Arc :: new ( create_delivered_node ()); let cancelled_node = Arc :: new ( create_cancelled_node ()); let mut workflow = Workflow :: new ( created_node ) . on ( OrderState :: Validated , validated_node ) . on ( OrderState :: Paid , paid_node ) . on ( OrderState :: Shipped , shipped_node ) . on ( OrderState :: Delivered , delivered_node ) . on ( OrderState :: Cancelled , cancelled_node ); Best Practices \u00b6 When using actions, consider the following best practices: Keep actions focused : Each action should have a clear meaning and purpose. Use descriptive names : Choose action names that clearly communicate their intent. Consider all possible paths : Ensure that all possible actions are handled in your workflow. Leverage type safety : Use Rust's type system to ensure that actions are used correctly. Document actions : Clearly document what each action means and when it should be used. Next Steps \u00b6 Now that you understand actions, you can learn about: Contexts : How to define and use contexts in your workflows Nodes : Learn more about the different types of nodes Workflows : How to compose nodes into workflows","title":"Actions"},{"location":"core-concepts/actions/#actions","text":"Actions in Floxide determine the flow of execution in a workflow. After a node completes its execution, it returns an action that indicates what should happen next. This page explains the different types of actions and how to use them to control workflow execution.","title":"Actions"},{"location":"core-concepts/actions/#the-action-concept","text":"Actions are the mechanism by which nodes communicate their execution results and control the flow of a workflow. When a node completes its execution, it returns an action that the workflow uses to determine the next step. In Floxide, actions are typically represented as enum variants, allowing for a clear and type-safe way to express different execution paths.","title":"The Action Concept"},{"location":"core-concepts/actions/#default-actions","text":"Floxide provides a DefaultAction enum that covers the most common workflow control patterns: pub enum DefaultAction { Next , Stop , } Next : Continue to the next node in the workflow Stop : Stop the workflow execution Here's how to use the default actions: use floxide_core ::{ lifecycle_node , LifecycleNode , Workflow , DefaultAction }; fn create_processor_node () -> impl LifecycleNode < MessageContext , DefaultAction > { lifecycle_node ( Some ( \"processor\" ), | ctx : & mut MessageContext | async move { Ok ( ctx . input . clone ()) }, | input : String | async move { Ok ( format! ( \"Processed: {}\" , input )) }, | _prep , exec_result , ctx : & mut MessageContext | async move { ctx . result = Some ( exec_result ); // Continue to the next node Ok ( DefaultAction :: Next ) // Or stop the workflow // Ok(DefaultAction::Stop) }, ) }","title":"Default Actions"},{"location":"core-concepts/actions/#custom-actions","text":"For more complex workflows, you can define custom actions using enums: #[derive(Debug, Clone, PartialEq, Eq)] pub enum OrderAction { Approve , Reject , Review , Error , } fn create_validation_node () -> impl LifecycleNode < OrderContext , OrderAction > { lifecycle_node ( Some ( \"validator\" ), | ctx : & mut OrderContext | async move { Ok ( ctx . order . clone ()) }, | order : Order | async move { // Validate the order let validation_result = validate_order ( & order ); Ok ( validation_result ) }, | _prep , validation_result , ctx : & mut OrderContext | async move { match validation_result { ValidationResult :: Valid => Ok ( OrderAction :: Approve ), ValidationResult :: Invalid => Ok ( OrderAction :: Reject ), ValidationResult :: NeedsReview => Ok ( OrderAction :: Review ), ValidationResult :: Error => Ok ( OrderAction :: Error ), } }, ) }","title":"Custom Actions"},{"location":"core-concepts/actions/#using-actions-in-workflows","text":"Actions are used to control the flow of execution in a workflow. You can use the on method to specify which node to execute for each action: let validation_node = Arc :: new ( create_validation_node ()); let approval_node = Arc :: new ( create_approval_node ()); let rejection_node = Arc :: new ( create_rejection_node ()); let review_node = Arc :: new ( create_review_node ()); let error_node = Arc :: new ( create_error_node ()); let mut workflow = Workflow :: new ( validation_node ) . on ( OrderAction :: Approve , approval_node ) . on ( OrderAction :: Reject , rejection_node ) . on ( OrderAction :: Review , review_node ) . on ( OrderAction :: Error , error_node ); This creates a workflow that executes different nodes based on the action returned by the validation node.","title":"Using Actions in Workflows"},{"location":"core-concepts/actions/#action-patterns","text":"","title":"Action Patterns"},{"location":"core-concepts/actions/#linear-flow","text":"The simplest action pattern is a linear flow, where each node returns DefaultAction::Next to continue to the next node: let node1 = Arc :: new ( create_node1 ()); let node2 = Arc :: new ( create_node2 ()); let node3 = Arc :: new ( create_node3 ()); let mut workflow = Workflow :: new ( node1 ) . then ( node2 ) . then ( node3 );","title":"Linear Flow"},{"location":"core-concepts/actions/#conditional-branching","text":"Actions enable conditional branching, where the execution path depends on the result of a node: enum PaymentAction { Success , Failure , Pending , } let payment_node = Arc :: new ( create_payment_node ()); let success_node = Arc :: new ( create_success_node ()); let failure_node = Arc :: new ( create_failure_node ()); let pending_node = Arc :: new ( create_pending_node ()); let mut workflow = Workflow :: new ( payment_node ) . on ( PaymentAction :: Success , success_node ) . on ( PaymentAction :: Failure , failure_node ) . on ( PaymentAction :: Pending , pending_node );","title":"Conditional Branching"},{"location":"core-concepts/actions/#error-handling","text":"Actions can be used to implement error handling patterns: enum ProcessAction { Success , Error , Retry , } let process_node = Arc :: new ( create_process_node ()); let success_node = Arc :: new ( create_success_node ()); let error_node = Arc :: new ( create_error_node ()); let retry_node = Arc :: new ( create_retry_node ()); let mut workflow = Workflow :: new ( process_node ) . on ( ProcessAction :: Success , success_node ) . on ( ProcessAction :: Error , error_node ) . on ( ProcessAction :: Retry , retry_node );","title":"Error Handling"},{"location":"core-concepts/actions/#state-machine","text":"Actions can be used to implement a state machine, where each node represents a state and actions represent transitions: enum OrderState { Created , Validated , Paid , Shipped , Delivered , Cancelled , } let created_node = Arc :: new ( create_created_node ()); let validated_node = Arc :: new ( create_validated_node ()); let paid_node = Arc :: new ( create_paid_node ()); let shipped_node = Arc :: new ( create_shipped_node ()); let delivered_node = Arc :: new ( create_delivered_node ()); let cancelled_node = Arc :: new ( create_cancelled_node ()); let mut workflow = Workflow :: new ( created_node ) . on ( OrderState :: Validated , validated_node ) . on ( OrderState :: Paid , paid_node ) . on ( OrderState :: Shipped , shipped_node ) . on ( OrderState :: Delivered , delivered_node ) . on ( OrderState :: Cancelled , cancelled_node );","title":"State Machine"},{"location":"core-concepts/actions/#best-practices","text":"When using actions, consider the following best practices: Keep actions focused : Each action should have a clear meaning and purpose. Use descriptive names : Choose action names that clearly communicate their intent. Consider all possible paths : Ensure that all possible actions are handled in your workflow. Leverage type safety : Use Rust's type system to ensure that actions are used correctly. Document actions : Clearly document what each action means and when it should be used.","title":"Best Practices"},{"location":"core-concepts/actions/#next-steps","text":"Now that you understand actions, you can learn about: Contexts : How to define and use contexts in your workflows Nodes : Learn more about the different types of nodes Workflows : How to compose nodes into workflows","title":"Next Steps"},{"location":"core-concepts/contexts/","text":"Contexts \u00b6 Contexts are a fundamental concept in the Floxide framework that provide a way to share state between nodes in a workflow. They serve as the primary mechanism for passing data through the workflow execution pipeline. What is a Context? \u00b6 A context in Floxide is a container for state that is passed between nodes during workflow execution. It encapsulates all the data needed for a node to perform its operations and allows nodes to communicate with each other by modifying and passing along this state. Contexts are strongly typed, which means that the type of data they can contain is defined at compile time. This provides type safety and ensures that nodes can only access and modify data in ways that are compatible with the defined types. Context Lifecycle \u00b6 Contexts follow a lifecycle that aligns with the node execution lifecycle: Creation : A context is created when a workflow begins execution. Preparation : During the prep phase, nodes can read from and write to the context to prepare for execution. Execution : During the exec phase, nodes can read from and write to the context to perform their main operations. Post-processing : During the post phase, nodes can read from and write to the context to perform cleanup or finalization operations. Completion : When the workflow completes, the final state of the context represents the result of the workflow execution. Context Types \u00b6 Floxide supports several types of contexts to accommodate different workflow patterns: Basic Context \u00b6 The basic context is used for simple workflows where a single state object is passed between nodes. It is defined using a generic type parameter that specifies the type of state it can contain. use floxide_core :: context :: Context ; // Define a state type struct MyState { value : i32 , } // Create a context with the state let context = Context :: new ( MyState { value : 42 }); Batch Context \u00b6 The batch context is used for batch processing workflows where multiple items need to be processed in parallel. It extends the basic context with functionality for managing a collection of items. use floxide_batch :: context :: BatchContext ; // Define a state type struct MyState { values : Vec < i32 > , } // Create a batch context with the state let context = BatchContext :: new ( MyState { values : vec ! [ 1 , 2 , 3 , 4 , 5 ] }); Event Context \u00b6 The event context is used for event-driven workflows where nodes can emit and respond to events. It extends the basic context with functionality for event handling. use floxide_event :: context :: EventContext ; // Define a state type struct MyState { value : i32 , } // Create an event context with the state let context = EventContext :: new ( MyState { value : 42 }); Accessing Context State \u00b6 Nodes can access and modify the state contained in a context using the context's methods. The most common methods are: state() : Returns a reference to the state. state_mut() : Returns a mutable reference to the state. use floxide_core :: context :: Context ; use floxide_core :: node ::{ Node , NodeResult }; struct MyNode ; impl Node for MyNode { type Context = Context < MyState > ; fn exec ( & self , ctx : & mut Self :: Context ) -> NodeResult { // Access the state let value = ctx . state (). value ; // Modify the state ctx . state_mut (). value += 1 ; Ok (()) } } Context Extensions \u00b6 Contexts can be extended with additional functionality through the use of traits. This allows for specialized behavior while maintaining a consistent interface. For example, the BatchContext extends the basic Context with methods for batch processing, and the EventContext extends it with methods for event handling. Best Practices \u00b6 When working with contexts in Floxide, consider the following best practices: Keep state minimal : Include only the data that is necessary for the workflow execution. Use appropriate context types : Choose the context type that best matches your workflow pattern. Respect immutability : Only modify state when necessary, and prefer immutable access when possible. Handle errors gracefully : Ensure that context state remains valid even when errors occur. Document state requirements : Clearly document what state a node expects and how it modifies that state. Related Concepts \u00b6 Nodes : The processing units that operate on contexts. Workflows : The orchestrators that manage the flow of contexts through nodes. Actions : The operations that nodes perform on contexts. Conclusion \u00b6 Contexts are a powerful abstraction in the Floxide framework that enable type-safe state management and communication between nodes in a workflow. By understanding how to use contexts effectively, you can build robust and maintainable workflow applications.","title":"Contexts"},{"location":"core-concepts/contexts/#contexts","text":"Contexts are a fundamental concept in the Floxide framework that provide a way to share state between nodes in a workflow. They serve as the primary mechanism for passing data through the workflow execution pipeline.","title":"Contexts"},{"location":"core-concepts/contexts/#what-is-a-context","text":"A context in Floxide is a container for state that is passed between nodes during workflow execution. It encapsulates all the data needed for a node to perform its operations and allows nodes to communicate with each other by modifying and passing along this state. Contexts are strongly typed, which means that the type of data they can contain is defined at compile time. This provides type safety and ensures that nodes can only access and modify data in ways that are compatible with the defined types.","title":"What is a Context?"},{"location":"core-concepts/contexts/#context-lifecycle","text":"Contexts follow a lifecycle that aligns with the node execution lifecycle: Creation : A context is created when a workflow begins execution. Preparation : During the prep phase, nodes can read from and write to the context to prepare for execution. Execution : During the exec phase, nodes can read from and write to the context to perform their main operations. Post-processing : During the post phase, nodes can read from and write to the context to perform cleanup or finalization operations. Completion : When the workflow completes, the final state of the context represents the result of the workflow execution.","title":"Context Lifecycle"},{"location":"core-concepts/contexts/#context-types","text":"Floxide supports several types of contexts to accommodate different workflow patterns:","title":"Context Types"},{"location":"core-concepts/contexts/#basic-context","text":"The basic context is used for simple workflows where a single state object is passed between nodes. It is defined using a generic type parameter that specifies the type of state it can contain. use floxide_core :: context :: Context ; // Define a state type struct MyState { value : i32 , } // Create a context with the state let context = Context :: new ( MyState { value : 42 });","title":"Basic Context"},{"location":"core-concepts/contexts/#batch-context","text":"The batch context is used for batch processing workflows where multiple items need to be processed in parallel. It extends the basic context with functionality for managing a collection of items. use floxide_batch :: context :: BatchContext ; // Define a state type struct MyState { values : Vec < i32 > , } // Create a batch context with the state let context = BatchContext :: new ( MyState { values : vec ! [ 1 , 2 , 3 , 4 , 5 ] });","title":"Batch Context"},{"location":"core-concepts/contexts/#event-context","text":"The event context is used for event-driven workflows where nodes can emit and respond to events. It extends the basic context with functionality for event handling. use floxide_event :: context :: EventContext ; // Define a state type struct MyState { value : i32 , } // Create an event context with the state let context = EventContext :: new ( MyState { value : 42 });","title":"Event Context"},{"location":"core-concepts/contexts/#accessing-context-state","text":"Nodes can access and modify the state contained in a context using the context's methods. The most common methods are: state() : Returns a reference to the state. state_mut() : Returns a mutable reference to the state. use floxide_core :: context :: Context ; use floxide_core :: node ::{ Node , NodeResult }; struct MyNode ; impl Node for MyNode { type Context = Context < MyState > ; fn exec ( & self , ctx : & mut Self :: Context ) -> NodeResult { // Access the state let value = ctx . state (). value ; // Modify the state ctx . state_mut (). value += 1 ; Ok (()) } }","title":"Accessing Context State"},{"location":"core-concepts/contexts/#context-extensions","text":"Contexts can be extended with additional functionality through the use of traits. This allows for specialized behavior while maintaining a consistent interface. For example, the BatchContext extends the basic Context with methods for batch processing, and the EventContext extends it with methods for event handling.","title":"Context Extensions"},{"location":"core-concepts/contexts/#best-practices","text":"When working with contexts in Floxide, consider the following best practices: Keep state minimal : Include only the data that is necessary for the workflow execution. Use appropriate context types : Choose the context type that best matches your workflow pattern. Respect immutability : Only modify state when necessary, and prefer immutable access when possible. Handle errors gracefully : Ensure that context state remains valid even when errors occur. Document state requirements : Clearly document what state a node expects and how it modifies that state.","title":"Best Practices"},{"location":"core-concepts/contexts/#related-concepts","text":"Nodes : The processing units that operate on contexts. Workflows : The orchestrators that manage the flow of contexts through nodes. Actions : The operations that nodes perform on contexts.","title":"Related Concepts"},{"location":"core-concepts/contexts/#conclusion","text":"Contexts are a powerful abstraction in the Floxide framework that enable type-safe state management and communication between nodes in a workflow. By understanding how to use contexts effectively, you can build robust and maintainable workflow applications.","title":"Conclusion"},{"location":"core-concepts/nodes/","text":"Nodes \u00b6 Nodes are the fundamental building blocks of workflows in Floxide. Each node represents a discrete unit of work that can be executed as part of a workflow. This page explains the different types of nodes and how to create and use them. The Node Trait \u00b6 At the core of Floxide is the Node trait, which defines the interface for executing a node: #[async_trait] pub trait Node < C , A > { async fn execute ( & self , context : & mut C ) -> Result < A , FloxideError > ; } Where: C is the context type that the node operates on A is the action type that the node returns This simple interface allows for a wide variety of node implementations, from simple function-based nodes to complex stateful nodes. LifecycleNode \u00b6 The most common type of node in Floxide is the LifecycleNode , which follows a three-phase lifecycle: Preparation (Prep) : Extract data from the context Execution (Exec) : Process the data Post-processing (Post) : Update the context with the result and determine the next action The LifecycleNode trait is defined as: #[async_trait] pub trait LifecycleNode < C , A > : Send + Sync { type PrepOutput : Send ; type ExecOutput : Send ; async fn prep ( & self , context : & mut C ) -> Result < Self :: PrepOutput , FloxideError > ; async fn exec ( & self , prep_output : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > ; async fn post ( & self , prep_output : Self :: PrepOutput , exec_output : Self :: ExecOutput , context : & mut C , ) -> Result < A , FloxideError > ; } This trait allows for a clear separation of concerns between the different phases of node execution. Creating Nodes \u00b6 Using the lifecycle_node Function \u00b6 The easiest way to create a LifecycleNode is using the lifecycle_node function: use floxide_core ::{ lifecycle_node , LifecycleNode , DefaultAction }; // Define your context type #[derive(Debug, Clone)] struct MyContext { input : String , result : Option < String > , } // Create a node fn create_processor_node () -> impl LifecycleNode < MyContext , DefaultAction > { lifecycle_node ( Some ( \"processor\" ), // Node ID | ctx : & mut MyContext | async move { // Preparation phase: extract data Ok ( ctx . input . clone ()) }, | input : String | async move { // Execution phase: process data Ok ( input . to_uppercase ()) }, | ctx : & mut MyContext , result : String | async move { // Post-processing phase: update context ctx . result = Some ( result ); Ok ( DefaultAction :: Next ) }, ) } Implementing the Trait Manually \u00b6 For more complex nodes, you can implement the LifecycleNode trait directly: use std :: sync :: atomic ::{ AtomicUsize , Ordering }; struct CounterNode { id : String , counter : AtomicUsize , } #[async_trait] impl LifecycleNode < MyContext , DefaultAction > for CounterNode { type PrepOutput = usize ; type ExecOutput = usize ; async fn prep ( & self , _context : & mut MyContext ) -> Result < Self :: PrepOutput , FloxideError > { Ok ( self . counter . load ( Ordering :: Relaxed )) } async fn exec ( & self , current : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > { let new_value = current + 1 ; self . counter . store ( new_value , Ordering :: Relaxed ); Ok ( new_value ) } async fn post ( & self , _prep : Self :: PrepOutput , exec : Self :: ExecOutput , context : & mut MyContext , ) -> Result < DefaultAction , FloxideError > { context . result = Some ( format! ( \"Count: {}\" , exec )); Ok ( DefaultAction :: Next ) } } Node Types \u00b6 Transform Node \u00b6 Transform nodes are specialized for data transformation operations: use floxide_transform ::{ transform_node , TransformNode }; fn create_transform_node () -> impl TransformNode < String , String > { transform_node ( | input : String | async move { Ok ( input . to_uppercase ()) }) } Batch Node \u00b6 Batch nodes process collections of items concurrently: use floxide_batch ::{ batch_node , BatchNode }; fn create_batch_node () -> impl BatchNode < Vec < String > , Vec < String >> { batch_node ( 10 , // Concurrency limit | item : String | async move { Ok ( item . to_uppercase ()) } ) } Event Node \u00b6 Event nodes handle asynchronous events: use floxide_event ::{ event_node , EventNode }; fn create_event_node () -> impl EventNode < MyContext , DefaultAction > { event_node ( | event : Event | async move { match event { Event :: Message ( msg ) => Ok ( msg . process ()), Event :: Timeout => Ok ( DefaultAction :: Stop ), _ => Ok ( DefaultAction :: Next ), } } ) } Best Practices \u00b6 1. Error Handling \u00b6 Always implement proper error handling in your nodes: fn create_robust_node () -> impl LifecycleNode < MyContext , DefaultAction > { lifecycle_node ( Some ( \"robust_processor\" ), | ctx : & mut MyContext | async move { ctx . input . parse :: < i32 > () . map_err ( | e | FloxideError :: new ( format! ( \"Invalid input: {}\" , e ))) }, | num : i32 | async move { if num < 0 { Err ( FloxideError :: new ( \"Negative numbers not allowed\" )) } else { Ok ( num * 2 ) } }, | ctx : & mut MyContext , result : i32 | async move { ctx . result = Some ( result . to_string ()); Ok ( DefaultAction :: Next ) }, ) } 2. Resource Management \u00b6 For nodes that manage resources, implement proper cleanup: struct ResourceNode { connection : Arc < Mutex < Connection >> , } impl ResourceNode { fn new () -> Self { Self { connection : Arc :: new ( Mutex :: new ( Connection :: new ())), } } } #[async_trait] impl LifecycleNode < MyContext , DefaultAction > for ResourceNode { // ... implementation ... async fn post ( & self , _prep : Self :: PrepOutput , exec : Self :: ExecOutput , context : & mut MyContext , ) -> Result < DefaultAction , FloxideError > { // Clean up resources self . connection . lock (). await . cleanup (); Ok ( DefaultAction :: Next ) } } 3. Context Management \u00b6 Keep context modifications focused and explicit: fn create_focused_node () -> impl LifecycleNode < MyContext , DefaultAction > { lifecycle_node ( Some ( \"focused_processor\" ), | ctx : & mut MyContext | async move { // Only access what you need Ok ( ctx . input . clone ()) }, | input : String | async move { Ok ( input . to_uppercase ()) }, | ctx : & mut MyContext , result : String | async move { // Only modify what you need ctx . result = Some ( result ); Ok ( DefaultAction :: Next ) }, ) } 4. Testing \u00b6 Make your nodes testable: #[cfg(test)] mod tests { use super :: * ; #[tokio::test] async fn test_processor_node () { let node = create_processor_node (); let mut context = MyContext { input : \"test\" . to_string (), result : None , }; let result = node . execute ( & mut context ). await ; assert! ( result . is_ok ()); assert_eq! ( context . result , Some ( \"TEST\" . to_string ())); } } Common Patterns \u00b6 1. Conditional Execution \u00b6 fn create_conditional_node () -> impl LifecycleNode < MyContext , CustomAction > { lifecycle_node ( Some ( \"conditional\" ), | ctx : & mut MyContext | async move { Ok ( ctx . input . clone ()) }, | input : String | async move { if input . is_empty () { Ok ( CustomAction :: Skip ) } else { Ok ( CustomAction :: Process ( input )) } }, | ctx : & mut MyContext , action : CustomAction | async move { match action { CustomAction :: Process ( result ) => { ctx . result = Some ( result ); Ok ( CustomAction :: Next ) } CustomAction :: Skip => Ok ( CustomAction :: Skip ), } }, ) } 2. State Management \u00b6 struct StatefulNode { state : Arc < RwLock < HashMap < String , String >>> , } impl StatefulNode { fn new () -> Self { Self { state : Arc :: new ( RwLock :: new ( HashMap :: new ())), } } } #[async_trait] impl LifecycleNode < MyContext , DefaultAction > for StatefulNode { // ... implementation with state management ... } 3. Retry Logic \u00b6 fn create_retry_node () -> impl LifecycleNode < MyContext , DefaultAction > { lifecycle_node ( Some ( \"retry_processor\" ), | ctx : & mut MyContext | async move { Ok (( ctx . input . clone (), 0 )) // Include retry count }, | input : ( String , i32 ) | async move { match process_with_retry ( input . 0 , input . 1 ). await { Ok ( result ) => Ok ( result ), Err ( e ) if input . 1 < 3 => { tokio :: time :: sleep ( Duration :: from_secs ( 1 )). await ; Err ( FloxideError :: new ( \"Retry\" )) } Err ( e ) => Err ( e ), } }, | ctx : & mut MyContext , result : String | async move { ctx . result = Some ( result ); Ok ( DefaultAction :: Next ) }, ) } Next Steps \u00b6 Now that you understand nodes, you can: Learn about Workflows to see how nodes work together Explore Actions for flow control Study Contexts for state management Check out the Examples section for more patterns Specialized Node Types \u00b6 Floxide provides several specialized node types for common workflow patterns: TransformNode \u00b6 A TransformNode is a simplified node that transforms an input to an output without the need for the full lifecycle. It's useful for data transformation steps in a workflow. use floxide_transform ::{ transform_node , TransformNode }; fn create_transform_node () -> impl TransformNode < String , String > { transform_node ( | input : String | async move { Ok ( input . to_uppercase ()) }) } BatchNode \u00b6 A BatchNode processes a collection of items in parallel, with configurable concurrency limits. use floxide_batch ::{ batch_node , BatchNode }; fn create_batch_node () -> impl BatchNode < Vec < String > , Vec < String >> { batch_node ( 10 , // Concurrency limit | item : String | async move { Ok ( item . to_uppercase ()) } ) } EventNode \u00b6 An EventNode responds to external events, allowing for event-driven workflows. use floxide_event ::{ event_node , EventNode }; fn create_event_node () -> impl EventNode < String , String > { event_node ( | event : String | async move { Ok ( format! ( \"Processed event: {}\" , event )) } ) } TimerNode \u00b6 A TimerNode executes based on time schedules, supporting one-time, interval, and calendar-based scheduling. use floxide_timer ::{ timer_node , TimerNode , TimerContext }; use std :: time :: Duration ; fn create_timer_node () -> impl TimerNode < (), String > { timer_node ( Duration :: from_secs ( 60 ), // Execute every 60 seconds | _ : () | async move { Ok ( \"Timer executed\" . to_string ()) } ) } ReactiveNode \u00b6 A ReactiveNode reacts to changes in external data sources, such as files, databases, or streams. use floxide_reactive ::{ reactive_node , ReactiveNode , ReactiveContext }; fn create_reactive_node () -> impl ReactiveNode < String , String > { reactive_node ( | change : String | async move { Ok ( format! ( \"Reacted to change: {}\" , change )) } ) } LongRunningNode \u00b6 A LongRunningNode is designed for processes that can be suspended and resumed, with state persistence between executions. use floxide_longrunning ::{ longrunning_node , LongRunningNode , LongRunningContext }; fn create_longrunning_node () -> impl LongRunningNode < String , String > { longrunning_node ( | state : Option < String > , input : String | async move { let current_state = state . unwrap_or_default (); let new_state = format! ( \"{} + {}\" , current_state , input ); Ok (( new_state . clone (), new_state )) } ) } Node Composition \u00b6 Nodes can be composed to create more complex workflows. The most common way to compose nodes is to use the Workflow struct, which we'll cover in the Workflows section. Best Practices \u00b6 When creating nodes, consider the following best practices: Keep nodes focused : Each node should have a single responsibility. Use appropriate node types : Choose the right node type for your use case. Handle errors gracefully : Properly handle errors in each phase of the node lifecycle. Consider performance : Be mindful of performance implications, especially for long-running or resource-intensive operations. Leverage type safety : Use Rust's type system to ensure type safety between nodes. Next Steps \u00b6 Now that you understand nodes, you can learn about: Workflows : How to compose nodes into workflows Actions : How to control the flow of execution Contexts : How to define and use contexts in your workflows","title":"Nodes"},{"location":"core-concepts/nodes/#nodes","text":"Nodes are the fundamental building blocks of workflows in Floxide. Each node represents a discrete unit of work that can be executed as part of a workflow. This page explains the different types of nodes and how to create and use them.","title":"Nodes"},{"location":"core-concepts/nodes/#the-node-trait","text":"At the core of Floxide is the Node trait, which defines the interface for executing a node: #[async_trait] pub trait Node < C , A > { async fn execute ( & self , context : & mut C ) -> Result < A , FloxideError > ; } Where: C is the context type that the node operates on A is the action type that the node returns This simple interface allows for a wide variety of node implementations, from simple function-based nodes to complex stateful nodes.","title":"The Node Trait"},{"location":"core-concepts/nodes/#lifecyclenode","text":"The most common type of node in Floxide is the LifecycleNode , which follows a three-phase lifecycle: Preparation (Prep) : Extract data from the context Execution (Exec) : Process the data Post-processing (Post) : Update the context with the result and determine the next action The LifecycleNode trait is defined as: #[async_trait] pub trait LifecycleNode < C , A > : Send + Sync { type PrepOutput : Send ; type ExecOutput : Send ; async fn prep ( & self , context : & mut C ) -> Result < Self :: PrepOutput , FloxideError > ; async fn exec ( & self , prep_output : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > ; async fn post ( & self , prep_output : Self :: PrepOutput , exec_output : Self :: ExecOutput , context : & mut C , ) -> Result < A , FloxideError > ; } This trait allows for a clear separation of concerns between the different phases of node execution.","title":"LifecycleNode"},{"location":"core-concepts/nodes/#creating-nodes","text":"","title":"Creating Nodes"},{"location":"core-concepts/nodes/#using-the-lifecycle_node-function","text":"The easiest way to create a LifecycleNode is using the lifecycle_node function: use floxide_core ::{ lifecycle_node , LifecycleNode , DefaultAction }; // Define your context type #[derive(Debug, Clone)] struct MyContext { input : String , result : Option < String > , } // Create a node fn create_processor_node () -> impl LifecycleNode < MyContext , DefaultAction > { lifecycle_node ( Some ( \"processor\" ), // Node ID | ctx : & mut MyContext | async move { // Preparation phase: extract data Ok ( ctx . input . clone ()) }, | input : String | async move { // Execution phase: process data Ok ( input . to_uppercase ()) }, | ctx : & mut MyContext , result : String | async move { // Post-processing phase: update context ctx . result = Some ( result ); Ok ( DefaultAction :: Next ) }, ) }","title":"Using the lifecycle_node Function"},{"location":"core-concepts/nodes/#implementing-the-trait-manually","text":"For more complex nodes, you can implement the LifecycleNode trait directly: use std :: sync :: atomic ::{ AtomicUsize , Ordering }; struct CounterNode { id : String , counter : AtomicUsize , } #[async_trait] impl LifecycleNode < MyContext , DefaultAction > for CounterNode { type PrepOutput = usize ; type ExecOutput = usize ; async fn prep ( & self , _context : & mut MyContext ) -> Result < Self :: PrepOutput , FloxideError > { Ok ( self . counter . load ( Ordering :: Relaxed )) } async fn exec ( & self , current : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > { let new_value = current + 1 ; self . counter . store ( new_value , Ordering :: Relaxed ); Ok ( new_value ) } async fn post ( & self , _prep : Self :: PrepOutput , exec : Self :: ExecOutput , context : & mut MyContext , ) -> Result < DefaultAction , FloxideError > { context . result = Some ( format! ( \"Count: {}\" , exec )); Ok ( DefaultAction :: Next ) } }","title":"Implementing the Trait Manually"},{"location":"core-concepts/nodes/#node-types","text":"","title":"Node Types"},{"location":"core-concepts/nodes/#transform-node","text":"Transform nodes are specialized for data transformation operations: use floxide_transform ::{ transform_node , TransformNode }; fn create_transform_node () -> impl TransformNode < String , String > { transform_node ( | input : String | async move { Ok ( input . to_uppercase ()) }) }","title":"Transform Node"},{"location":"core-concepts/nodes/#batch-node","text":"Batch nodes process collections of items concurrently: use floxide_batch ::{ batch_node , BatchNode }; fn create_batch_node () -> impl BatchNode < Vec < String > , Vec < String >> { batch_node ( 10 , // Concurrency limit | item : String | async move { Ok ( item . to_uppercase ()) } ) }","title":"Batch Node"},{"location":"core-concepts/nodes/#event-node","text":"Event nodes handle asynchronous events: use floxide_event ::{ event_node , EventNode }; fn create_event_node () -> impl EventNode < MyContext , DefaultAction > { event_node ( | event : Event | async move { match event { Event :: Message ( msg ) => Ok ( msg . process ()), Event :: Timeout => Ok ( DefaultAction :: Stop ), _ => Ok ( DefaultAction :: Next ), } } ) }","title":"Event Node"},{"location":"core-concepts/nodes/#best-practices","text":"","title":"Best Practices"},{"location":"core-concepts/nodes/#1-error-handling","text":"Always implement proper error handling in your nodes: fn create_robust_node () -> impl LifecycleNode < MyContext , DefaultAction > { lifecycle_node ( Some ( \"robust_processor\" ), | ctx : & mut MyContext | async move { ctx . input . parse :: < i32 > () . map_err ( | e | FloxideError :: new ( format! ( \"Invalid input: {}\" , e ))) }, | num : i32 | async move { if num < 0 { Err ( FloxideError :: new ( \"Negative numbers not allowed\" )) } else { Ok ( num * 2 ) } }, | ctx : & mut MyContext , result : i32 | async move { ctx . result = Some ( result . to_string ()); Ok ( DefaultAction :: Next ) }, ) }","title":"1. Error Handling"},{"location":"core-concepts/nodes/#2-resource-management","text":"For nodes that manage resources, implement proper cleanup: struct ResourceNode { connection : Arc < Mutex < Connection >> , } impl ResourceNode { fn new () -> Self { Self { connection : Arc :: new ( Mutex :: new ( Connection :: new ())), } } } #[async_trait] impl LifecycleNode < MyContext , DefaultAction > for ResourceNode { // ... implementation ... async fn post ( & self , _prep : Self :: PrepOutput , exec : Self :: ExecOutput , context : & mut MyContext , ) -> Result < DefaultAction , FloxideError > { // Clean up resources self . connection . lock (). await . cleanup (); Ok ( DefaultAction :: Next ) } }","title":"2. Resource Management"},{"location":"core-concepts/nodes/#3-context-management","text":"Keep context modifications focused and explicit: fn create_focused_node () -> impl LifecycleNode < MyContext , DefaultAction > { lifecycle_node ( Some ( \"focused_processor\" ), | ctx : & mut MyContext | async move { // Only access what you need Ok ( ctx . input . clone ()) }, | input : String | async move { Ok ( input . to_uppercase ()) }, | ctx : & mut MyContext , result : String | async move { // Only modify what you need ctx . result = Some ( result ); Ok ( DefaultAction :: Next ) }, ) }","title":"3. Context Management"},{"location":"core-concepts/nodes/#4-testing","text":"Make your nodes testable: #[cfg(test)] mod tests { use super :: * ; #[tokio::test] async fn test_processor_node () { let node = create_processor_node (); let mut context = MyContext { input : \"test\" . to_string (), result : None , }; let result = node . execute ( & mut context ). await ; assert! ( result . is_ok ()); assert_eq! ( context . result , Some ( \"TEST\" . to_string ())); } }","title":"4. Testing"},{"location":"core-concepts/nodes/#common-patterns","text":"","title":"Common Patterns"},{"location":"core-concepts/nodes/#1-conditional-execution","text":"fn create_conditional_node () -> impl LifecycleNode < MyContext , CustomAction > { lifecycle_node ( Some ( \"conditional\" ), | ctx : & mut MyContext | async move { Ok ( ctx . input . clone ()) }, | input : String | async move { if input . is_empty () { Ok ( CustomAction :: Skip ) } else { Ok ( CustomAction :: Process ( input )) } }, | ctx : & mut MyContext , action : CustomAction | async move { match action { CustomAction :: Process ( result ) => { ctx . result = Some ( result ); Ok ( CustomAction :: Next ) } CustomAction :: Skip => Ok ( CustomAction :: Skip ), } }, ) }","title":"1. Conditional Execution"},{"location":"core-concepts/nodes/#2-state-management","text":"struct StatefulNode { state : Arc < RwLock < HashMap < String , String >>> , } impl StatefulNode { fn new () -> Self { Self { state : Arc :: new ( RwLock :: new ( HashMap :: new ())), } } } #[async_trait] impl LifecycleNode < MyContext , DefaultAction > for StatefulNode { // ... implementation with state management ... }","title":"2. State Management"},{"location":"core-concepts/nodes/#3-retry-logic","text":"fn create_retry_node () -> impl LifecycleNode < MyContext , DefaultAction > { lifecycle_node ( Some ( \"retry_processor\" ), | ctx : & mut MyContext | async move { Ok (( ctx . input . clone (), 0 )) // Include retry count }, | input : ( String , i32 ) | async move { match process_with_retry ( input . 0 , input . 1 ). await { Ok ( result ) => Ok ( result ), Err ( e ) if input . 1 < 3 => { tokio :: time :: sleep ( Duration :: from_secs ( 1 )). await ; Err ( FloxideError :: new ( \"Retry\" )) } Err ( e ) => Err ( e ), } }, | ctx : & mut MyContext , result : String | async move { ctx . result = Some ( result ); Ok ( DefaultAction :: Next ) }, ) }","title":"3. Retry Logic"},{"location":"core-concepts/nodes/#next-steps","text":"Now that you understand nodes, you can: Learn about Workflows to see how nodes work together Explore Actions for flow control Study Contexts for state management Check out the Examples section for more patterns","title":"Next Steps"},{"location":"core-concepts/nodes/#specialized-node-types","text":"Floxide provides several specialized node types for common workflow patterns:","title":"Specialized Node Types"},{"location":"core-concepts/nodes/#transformnode","text":"A TransformNode is a simplified node that transforms an input to an output without the need for the full lifecycle. It's useful for data transformation steps in a workflow. use floxide_transform ::{ transform_node , TransformNode }; fn create_transform_node () -> impl TransformNode < String , String > { transform_node ( | input : String | async move { Ok ( input . to_uppercase ()) }) }","title":"TransformNode"},{"location":"core-concepts/nodes/#batchnode","text":"A BatchNode processes a collection of items in parallel, with configurable concurrency limits. use floxide_batch ::{ batch_node , BatchNode }; fn create_batch_node () -> impl BatchNode < Vec < String > , Vec < String >> { batch_node ( 10 , // Concurrency limit | item : String | async move { Ok ( item . to_uppercase ()) } ) }","title":"BatchNode"},{"location":"core-concepts/nodes/#eventnode","text":"An EventNode responds to external events, allowing for event-driven workflows. use floxide_event ::{ event_node , EventNode }; fn create_event_node () -> impl EventNode < String , String > { event_node ( | event : String | async move { Ok ( format! ( \"Processed event: {}\" , event )) } ) }","title":"EventNode"},{"location":"core-concepts/nodes/#timernode","text":"A TimerNode executes based on time schedules, supporting one-time, interval, and calendar-based scheduling. use floxide_timer ::{ timer_node , TimerNode , TimerContext }; use std :: time :: Duration ; fn create_timer_node () -> impl TimerNode < (), String > { timer_node ( Duration :: from_secs ( 60 ), // Execute every 60 seconds | _ : () | async move { Ok ( \"Timer executed\" . to_string ()) } ) }","title":"TimerNode"},{"location":"core-concepts/nodes/#reactivenode","text":"A ReactiveNode reacts to changes in external data sources, such as files, databases, or streams. use floxide_reactive ::{ reactive_node , ReactiveNode , ReactiveContext }; fn create_reactive_node () -> impl ReactiveNode < String , String > { reactive_node ( | change : String | async move { Ok ( format! ( \"Reacted to change: {}\" , change )) } ) }","title":"ReactiveNode"},{"location":"core-concepts/nodes/#longrunningnode","text":"A LongRunningNode is designed for processes that can be suspended and resumed, with state persistence between executions. use floxide_longrunning ::{ longrunning_node , LongRunningNode , LongRunningContext }; fn create_longrunning_node () -> impl LongRunningNode < String , String > { longrunning_node ( | state : Option < String > , input : String | async move { let current_state = state . unwrap_or_default (); let new_state = format! ( \"{} + {}\" , current_state , input ); Ok (( new_state . clone (), new_state )) } ) }","title":"LongRunningNode"},{"location":"core-concepts/nodes/#node-composition","text":"Nodes can be composed to create more complex workflows. The most common way to compose nodes is to use the Workflow struct, which we'll cover in the Workflows section.","title":"Node Composition"},{"location":"core-concepts/nodes/#best-practices_1","text":"When creating nodes, consider the following best practices: Keep nodes focused : Each node should have a single responsibility. Use appropriate node types : Choose the right node type for your use case. Handle errors gracefully : Properly handle errors in each phase of the node lifecycle. Consider performance : Be mindful of performance implications, especially for long-running or resource-intensive operations. Leverage type safety : Use Rust's type system to ensure type safety between nodes.","title":"Best Practices"},{"location":"core-concepts/nodes/#next-steps_1","text":"Now that you understand nodes, you can learn about: Workflows : How to compose nodes into workflows Actions : How to control the flow of execution Contexts : How to define and use contexts in your workflows","title":"Next Steps"},{"location":"core-concepts/overview/","text":"Core Concepts Overview \u00b6 Floxide is built around a set of core concepts that form the foundation of the framework. Understanding these concepts is essential for effectively using Floxide to build workflows. Key Abstractions \u00b6 Floxide is designed with a trait-based architecture that provides flexibility and type safety. The key abstractions are: Nodes \u00b6 Nodes are the fundamental building blocks of workflows. Each node represents a discrete unit of work that can be executed as part of a workflow. Nodes implement the Node trait, which defines the interface for executing a node. The most common type of node is the LifecycleNode , which follows a three-phase lifecycle: Preparation : Extract data from the context Execution : Process the data Post-processing : Update the context with the result and determine the next action Workflows \u00b6 Workflows orchestrate the execution of nodes. A workflow is a directed graph of nodes, where the edges represent transitions between nodes. Workflows are responsible for: Managing the execution flow between nodes Handling errors and retries Providing observability and monitoring Actions \u00b6 Actions determine the flow of execution in a workflow. After a node completes its execution, it returns an action that indicates what should happen next. Actions can be: Next : Continue to the next node in the workflow Branch : Take a specific branch in the workflow Stop : Stop the workflow execution Custom actions: User-defined actions for specific workflow patterns Contexts \u00b6 Contexts hold the state of a workflow execution. Each workflow operates on a context, which is passed from node to node. Contexts are user-defined types that implement the necessary traits for the workflow to operate on them. Workflow Patterns \u00b6 Floxide supports various workflow patterns through its modular crate system: Linear Workflows \u00b6 The simplest workflow pattern is a linear sequence of nodes, where each node is executed in order. graph LR A[\"Node 1\"] --> B[\"Node 2\"] --> C[\"Node 3\"] style A fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style B fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style C fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px Conditional Branching \u00b6 Workflows can include conditional branching, where the execution path depends on the result of a node. graph TD A[\"Decision Node\"] -->|Condition A| B[\"Node A\"] A -->|Condition B| C[\"Node B\"] B --> D[\"Join Node\"] C --> D style A fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style B fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style C fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style D fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px Batch Processing \u00b6 Batch processing allows for parallel execution of nodes on a collection of items. graph TD A[\"Batch Input\"] --> B[\"Split Batch\"] B --> C1[\"Process Item 1\"] B --> C2[\"Process Item 2\"] B --> C3[\"Process Item 3\"] C1 --> D[\"Aggregate Results\"] C2 --> D C3 --> D style A fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style B fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style C1 fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style C2 fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style C3 fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style D fill:#e3f2fd,stroke:#1565c0,stroke-width:2px Event-Driven Workflows \u00b6 Event-driven workflows respond to external events, allowing for reactive systems. graph TD A[\"Event Source\"] -->|Events| B[\"Event Handler\"] B --> C[\"Process Event\"] C --> D[\"Update State\"] D --> A style A fill:#e8eaf6,stroke:#3949ab,stroke-width:2px style B fill:#e8eaf6,stroke:#3949ab,stroke-width:2px style C fill:#e8eaf6,stroke:#3949ab,stroke-width:2px style D fill:#e8eaf6,stroke:#3949ab,stroke-width:2px Type Safety \u00b6 One of the key features of Floxide is its emphasis on type safety. The framework leverages Rust's type system to ensure that: Nodes can only be connected if their input and output types are compatible Actions are type-checked at compile time Contexts are properly typed for each node in the workflow This type safety helps catch errors at compile time rather than runtime, leading to more robust workflows. Async Execution \u00b6 Floxide is built with asynchronous execution in mind. All node execution is asynchronous, allowing for efficient handling of I/O-bound operations. The framework uses Tokio as its async runtime, providing a solid foundation for concurrent execution. Next Steps \u00b6 Now that you understand the core concepts of Floxide, you can explore each concept in more detail: Nodes : Learn about the different types of nodes and how to create them Workflows : Understand how to build and execute workflows Actions : Explore how actions control the flow of execution Contexts : Learn how to define and use contexts in your workflows","title":"Overview"},{"location":"core-concepts/overview/#core-concepts-overview","text":"Floxide is built around a set of core concepts that form the foundation of the framework. Understanding these concepts is essential for effectively using Floxide to build workflows.","title":"Core Concepts Overview"},{"location":"core-concepts/overview/#key-abstractions","text":"Floxide is designed with a trait-based architecture that provides flexibility and type safety. The key abstractions are:","title":"Key Abstractions"},{"location":"core-concepts/overview/#nodes","text":"Nodes are the fundamental building blocks of workflows. Each node represents a discrete unit of work that can be executed as part of a workflow. Nodes implement the Node trait, which defines the interface for executing a node. The most common type of node is the LifecycleNode , which follows a three-phase lifecycle: Preparation : Extract data from the context Execution : Process the data Post-processing : Update the context with the result and determine the next action","title":"Nodes"},{"location":"core-concepts/overview/#workflows","text":"Workflows orchestrate the execution of nodes. A workflow is a directed graph of nodes, where the edges represent transitions between nodes. Workflows are responsible for: Managing the execution flow between nodes Handling errors and retries Providing observability and monitoring","title":"Workflows"},{"location":"core-concepts/overview/#actions","text":"Actions determine the flow of execution in a workflow. After a node completes its execution, it returns an action that indicates what should happen next. Actions can be: Next : Continue to the next node in the workflow Branch : Take a specific branch in the workflow Stop : Stop the workflow execution Custom actions: User-defined actions for specific workflow patterns","title":"Actions"},{"location":"core-concepts/overview/#contexts","text":"Contexts hold the state of a workflow execution. Each workflow operates on a context, which is passed from node to node. Contexts are user-defined types that implement the necessary traits for the workflow to operate on them.","title":"Contexts"},{"location":"core-concepts/overview/#workflow-patterns","text":"Floxide supports various workflow patterns through its modular crate system:","title":"Workflow Patterns"},{"location":"core-concepts/overview/#linear-workflows","text":"The simplest workflow pattern is a linear sequence of nodes, where each node is executed in order. graph LR A[\"Node 1\"] --> B[\"Node 2\"] --> C[\"Node 3\"] style A fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style B fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style C fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px","title":"Linear Workflows"},{"location":"core-concepts/overview/#conditional-branching","text":"Workflows can include conditional branching, where the execution path depends on the result of a node. graph TD A[\"Decision Node\"] -->|Condition A| B[\"Node A\"] A -->|Condition B| C[\"Node B\"] B --> D[\"Join Node\"] C --> D style A fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style B fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style C fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px style D fill:#c4e6ff,stroke:#1a73e8,stroke-width:2px","title":"Conditional Branching"},{"location":"core-concepts/overview/#batch-processing","text":"Batch processing allows for parallel execution of nodes on a collection of items. graph TD A[\"Batch Input\"] --> B[\"Split Batch\"] B --> C1[\"Process Item 1\"] B --> C2[\"Process Item 2\"] B --> C3[\"Process Item 3\"] C1 --> D[\"Aggregate Results\"] C2 --> D C3 --> D style A fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style B fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style C1 fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style C2 fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style C3 fill:#e3f2fd,stroke:#1565c0,stroke-width:2px style D fill:#e3f2fd,stroke:#1565c0,stroke-width:2px","title":"Batch Processing"},{"location":"core-concepts/overview/#event-driven-workflows","text":"Event-driven workflows respond to external events, allowing for reactive systems. graph TD A[\"Event Source\"] -->|Events| B[\"Event Handler\"] B --> C[\"Process Event\"] C --> D[\"Update State\"] D --> A style A fill:#e8eaf6,stroke:#3949ab,stroke-width:2px style B fill:#e8eaf6,stroke:#3949ab,stroke-width:2px style C fill:#e8eaf6,stroke:#3949ab,stroke-width:2px style D fill:#e8eaf6,stroke:#3949ab,stroke-width:2px","title":"Event-Driven Workflows"},{"location":"core-concepts/overview/#type-safety","text":"One of the key features of Floxide is its emphasis on type safety. The framework leverages Rust's type system to ensure that: Nodes can only be connected if their input and output types are compatible Actions are type-checked at compile time Contexts are properly typed for each node in the workflow This type safety helps catch errors at compile time rather than runtime, leading to more robust workflows.","title":"Type Safety"},{"location":"core-concepts/overview/#async-execution","text":"Floxide is built with asynchronous execution in mind. All node execution is asynchronous, allowing for efficient handling of I/O-bound operations. The framework uses Tokio as its async runtime, providing a solid foundation for concurrent execution.","title":"Async Execution"},{"location":"core-concepts/overview/#next-steps","text":"Now that you understand the core concepts of Floxide, you can explore each concept in more detail: Nodes : Learn about the different types of nodes and how to create them Workflows : Understand how to build and execute workflows Actions : Explore how actions control the flow of execution Contexts : Learn how to define and use contexts in your workflows","title":"Next Steps"},{"location":"core-concepts/workflows/","text":"Workflows \u00b6 Workflows in Floxide orchestrate the execution of nodes. A workflow is a directed graph of nodes, where the edges represent transitions between nodes. This page explains how to create, configure, and execute workflows. The Workflow Struct \u00b6 The core of workflow orchestration in Floxide is the Workflow struct. This struct manages the execution flow between nodes, handles errors and retries, and provides observability and monitoring. pub struct Workflow < C , A > { // Internal implementation details } Where: C is the context type that the workflow operates on A is the action type that the nodes in the workflow return Creating Workflows \u00b6 Basic Workflow \u00b6 The simplest way to create a workflow is to use the new method, which takes a single node: use floxide_core ::{ lifecycle_node , LifecycleNode , Workflow , DefaultAction }; use std :: sync :: Arc ; // Create a node let node = Arc :: new ( create_processor_node ()); // Create a workflow with the node let mut workflow = Workflow :: new ( node ); Linear Workflow \u00b6 You can create a linear workflow by chaining nodes using the then method: let node1 = Arc :: new ( create_first_node ()); let node2 = Arc :: new ( create_second_node ()); let node3 = Arc :: new ( create_third_node ()); let mut workflow = Workflow :: new ( node1 ) . then ( node2 ) . then ( node3 ); This creates a workflow where node1 is executed first, followed by node2 , and then node3 . Conditional Branching \u00b6 You can create conditional branches in your workflow using the on method, which takes an action and a node: enum CustomAction { Success , Error , Retry , } let decision_node = Arc :: new ( create_decision_node ()); let success_node = Arc :: new ( create_success_node ()); let error_node = Arc :: new ( create_error_node ()); let retry_node = Arc :: new ( create_retry_node ()); let mut workflow = Workflow :: new ( decision_node ) . on ( CustomAction :: Success , success_node ) . on ( CustomAction :: Error , error_node ) . on ( CustomAction :: Retry , retry_node ); This creates a workflow where the execution path depends on the action returned by decision_node . Executing Workflows \u00b6 To execute a workflow, use the execute method, which takes a mutable reference to a context: #[tokio::main] async fn main () -> Result < (), Box < dyn std :: error :: Error >> { // Create a context let mut context = MessageContext { input : \"Hello, Floxide!\" . to_string (), result : None , }; // Create a workflow let node = Arc :: new ( create_processor_node ()); let mut workflow = Workflow :: new ( node ); // Execute the workflow workflow . execute ( & mut context ). await ? ; // Print the result println! ( \"Result: {:?}\" , context . result ); Ok (()) } Advanced Workflow Patterns \u00b6 Error Handling \u00b6 Floxide provides several ways to handle errors in workflows: Try-Catch Pattern \u00b6 You can implement a try-catch pattern using conditional branching: enum CustomAction { Success , Error , } let try_node = Arc :: new ( create_try_node ()); let success_node = Arc :: new ( create_success_node ()); let catch_node = Arc :: new ( create_catch_node ()); let mut workflow = Workflow :: new ( try_node ) . on ( CustomAction :: Success , success_node ) . on ( CustomAction :: Error , catch_node ); Retry Pattern \u00b6 You can implement a retry pattern by returning to a previous node: enum CustomAction { Success , Retry , Error , } let operation_node = Arc :: new ( create_operation_node ()); let success_node = Arc :: new ( create_success_node ()); let retry_node = Arc :: new ( create_retry_node ()); let error_node = Arc :: new ( create_error_node ()); let mut workflow = Workflow :: new ( operation_node ) . on ( CustomAction :: Success , success_node ) . on ( CustomAction :: Retry , retry_node ) . on ( CustomAction :: Error , error_node ); // Configure retry_node to return to operation_node Parallel Execution \u00b6 For parallel execution, you can use the BatchNode from the floxide-batch crate: use floxide_batch ::{ batch_node , BatchNode , BatchContext }; // Create a batch node that processes items in parallel let batch_node = Arc :: new ( batch_node ( 10 , // Concurrency limit | item : String | async move { Ok ( item . to_uppercase ()) } )); // Create a workflow with the batch node let mut workflow = Workflow :: new ( batch_node ); Event-Driven Workflows \u00b6 For event-driven workflows, you can use the EventNode from the floxide-event crate: use floxide_event ::{ event_node , EventNode , EventContext }; // Create an event node that responds to events let event_node = Arc :: new ( event_node ( | event : String | async move { Ok ( format! ( \"Processed event: {}\" , event )) } )); // Create a workflow with the event node let mut workflow = Workflow :: new ( event_node ); Workflow Composition \u00b6 You can compose workflows by treating a workflow as a node in a larger workflow: // Create a sub-workflow let sub_workflow_node = Arc :: new ( create_sub_workflow_node ()); let sub_workflow = Workflow :: new ( sub_workflow_node ); // Create a main workflow that includes the sub-workflow let main_workflow_node = Arc :: new ( create_main_workflow_node ()); let mut main_workflow = Workflow :: new ( main_workflow_node ) . then ( Arc :: new ( sub_workflow )); Workflow Persistence \u00b6 Floxide supports workflow persistence through serialization and deserialization: use serde ::{ Serialize , Deserialize }; #[derive(Debug, Clone, Serialize, Deserialize)] struct PersistableContext { input : String , result : Option < String > , state : WorkflowState , } // Serialize the context to save the workflow state let serialized = serde_json :: to_string ( & context ) ? ; // Later, deserialize the context to resume the workflow let mut context : PersistableContext = serde_json :: from_str ( & serialized ) ? ; workflow . execute ( & mut context ). await ? ; Best Practices \u00b6 When creating workflows, consider the following best practices: Keep workflows focused : Each workflow should have a clear purpose. Use appropriate branching : Choose the right branching pattern for your use case. Handle errors gracefully : Implement proper error handling and recovery mechanisms. Consider performance : Be mindful of performance implications, especially for parallel execution. Leverage type safety : Use Rust's type system to ensure type safety between nodes. Next Steps \u00b6 Now that you understand workflows, you can learn about: Actions : How to control the flow of execution Contexts : How to define and use contexts in your workflows Nodes : Learn more about the different types of nodes","title":"Workflows"},{"location":"core-concepts/workflows/#workflows","text":"Workflows in Floxide orchestrate the execution of nodes. A workflow is a directed graph of nodes, where the edges represent transitions between nodes. This page explains how to create, configure, and execute workflows.","title":"Workflows"},{"location":"core-concepts/workflows/#the-workflow-struct","text":"The core of workflow orchestration in Floxide is the Workflow struct. This struct manages the execution flow between nodes, handles errors and retries, and provides observability and monitoring. pub struct Workflow < C , A > { // Internal implementation details } Where: C is the context type that the workflow operates on A is the action type that the nodes in the workflow return","title":"The Workflow Struct"},{"location":"core-concepts/workflows/#creating-workflows","text":"","title":"Creating Workflows"},{"location":"core-concepts/workflows/#basic-workflow","text":"The simplest way to create a workflow is to use the new method, which takes a single node: use floxide_core ::{ lifecycle_node , LifecycleNode , Workflow , DefaultAction }; use std :: sync :: Arc ; // Create a node let node = Arc :: new ( create_processor_node ()); // Create a workflow with the node let mut workflow = Workflow :: new ( node );","title":"Basic Workflow"},{"location":"core-concepts/workflows/#linear-workflow","text":"You can create a linear workflow by chaining nodes using the then method: let node1 = Arc :: new ( create_first_node ()); let node2 = Arc :: new ( create_second_node ()); let node3 = Arc :: new ( create_third_node ()); let mut workflow = Workflow :: new ( node1 ) . then ( node2 ) . then ( node3 ); This creates a workflow where node1 is executed first, followed by node2 , and then node3 .","title":"Linear Workflow"},{"location":"core-concepts/workflows/#conditional-branching","text":"You can create conditional branches in your workflow using the on method, which takes an action and a node: enum CustomAction { Success , Error , Retry , } let decision_node = Arc :: new ( create_decision_node ()); let success_node = Arc :: new ( create_success_node ()); let error_node = Arc :: new ( create_error_node ()); let retry_node = Arc :: new ( create_retry_node ()); let mut workflow = Workflow :: new ( decision_node ) . on ( CustomAction :: Success , success_node ) . on ( CustomAction :: Error , error_node ) . on ( CustomAction :: Retry , retry_node ); This creates a workflow where the execution path depends on the action returned by decision_node .","title":"Conditional Branching"},{"location":"core-concepts/workflows/#executing-workflows","text":"To execute a workflow, use the execute method, which takes a mutable reference to a context: #[tokio::main] async fn main () -> Result < (), Box < dyn std :: error :: Error >> { // Create a context let mut context = MessageContext { input : \"Hello, Floxide!\" . to_string (), result : None , }; // Create a workflow let node = Arc :: new ( create_processor_node ()); let mut workflow = Workflow :: new ( node ); // Execute the workflow workflow . execute ( & mut context ). await ? ; // Print the result println! ( \"Result: {:?}\" , context . result ); Ok (()) }","title":"Executing Workflows"},{"location":"core-concepts/workflows/#advanced-workflow-patterns","text":"","title":"Advanced Workflow Patterns"},{"location":"core-concepts/workflows/#error-handling","text":"Floxide provides several ways to handle errors in workflows:","title":"Error Handling"},{"location":"core-concepts/workflows/#try-catch-pattern","text":"You can implement a try-catch pattern using conditional branching: enum CustomAction { Success , Error , } let try_node = Arc :: new ( create_try_node ()); let success_node = Arc :: new ( create_success_node ()); let catch_node = Arc :: new ( create_catch_node ()); let mut workflow = Workflow :: new ( try_node ) . on ( CustomAction :: Success , success_node ) . on ( CustomAction :: Error , catch_node );","title":"Try-Catch Pattern"},{"location":"core-concepts/workflows/#retry-pattern","text":"You can implement a retry pattern by returning to a previous node: enum CustomAction { Success , Retry , Error , } let operation_node = Arc :: new ( create_operation_node ()); let success_node = Arc :: new ( create_success_node ()); let retry_node = Arc :: new ( create_retry_node ()); let error_node = Arc :: new ( create_error_node ()); let mut workflow = Workflow :: new ( operation_node ) . on ( CustomAction :: Success , success_node ) . on ( CustomAction :: Retry , retry_node ) . on ( CustomAction :: Error , error_node ); // Configure retry_node to return to operation_node","title":"Retry Pattern"},{"location":"core-concepts/workflows/#parallel-execution","text":"For parallel execution, you can use the BatchNode from the floxide-batch crate: use floxide_batch ::{ batch_node , BatchNode , BatchContext }; // Create a batch node that processes items in parallel let batch_node = Arc :: new ( batch_node ( 10 , // Concurrency limit | item : String | async move { Ok ( item . to_uppercase ()) } )); // Create a workflow with the batch node let mut workflow = Workflow :: new ( batch_node );","title":"Parallel Execution"},{"location":"core-concepts/workflows/#event-driven-workflows","text":"For event-driven workflows, you can use the EventNode from the floxide-event crate: use floxide_event ::{ event_node , EventNode , EventContext }; // Create an event node that responds to events let event_node = Arc :: new ( event_node ( | event : String | async move { Ok ( format! ( \"Processed event: {}\" , event )) } )); // Create a workflow with the event node let mut workflow = Workflow :: new ( event_node );","title":"Event-Driven Workflows"},{"location":"core-concepts/workflows/#workflow-composition","text":"You can compose workflows by treating a workflow as a node in a larger workflow: // Create a sub-workflow let sub_workflow_node = Arc :: new ( create_sub_workflow_node ()); let sub_workflow = Workflow :: new ( sub_workflow_node ); // Create a main workflow that includes the sub-workflow let main_workflow_node = Arc :: new ( create_main_workflow_node ()); let mut main_workflow = Workflow :: new ( main_workflow_node ) . then ( Arc :: new ( sub_workflow ));","title":"Workflow Composition"},{"location":"core-concepts/workflows/#workflow-persistence","text":"Floxide supports workflow persistence through serialization and deserialization: use serde ::{ Serialize , Deserialize }; #[derive(Debug, Clone, Serialize, Deserialize)] struct PersistableContext { input : String , result : Option < String > , state : WorkflowState , } // Serialize the context to save the workflow state let serialized = serde_json :: to_string ( & context ) ? ; // Later, deserialize the context to resume the workflow let mut context : PersistableContext = serde_json :: from_str ( & serialized ) ? ; workflow . execute ( & mut context ). await ? ;","title":"Workflow Persistence"},{"location":"core-concepts/workflows/#best-practices","text":"When creating workflows, consider the following best practices: Keep workflows focused : Each workflow should have a clear purpose. Use appropriate branching : Choose the right branching pattern for your use case. Handle errors gracefully : Implement proper error handling and recovery mechanisms. Consider performance : Be mindful of performance implications, especially for parallel execution. Leverage type safety : Use Rust's type system to ensure type safety between nodes.","title":"Best Practices"},{"location":"core-concepts/workflows/#next-steps","text":"Now that you understand workflows, you can learn about: Actions : How to control the flow of execution Contexts : How to define and use contexts in your workflows Nodes : Learn more about the different types of nodes","title":"Next Steps"},{"location":"examples/basic-workflow/","text":"Basic Workflow Example \u00b6 This example demonstrates how to create a simple workflow using the Floxide framework, showcasing proper node lifecycle management and error handling. Overview \u00b6 In this example, we'll create a workflow that processes text data through multiple stages: Input validation and preparation Text transformation (uppercase) Analysis (character count) Summary generation The example demonstrates: - Three-phase node lifecycle (prep, exec, post) - Proper error handling and recovery - Type-safe context management - Node composition patterns Prerequisites \u00b6 Before running this example, make sure you have the Floxide framework installed. See the Installation Guide for details. Implementation \u00b6 First, let's define our context and error types: use floxide_core ::{ lifecycle_node , LifecycleNode , DefaultAction , FloxideError }; use thiserror :: Error ; #[derive(Debug, Clone)] struct TextContext { input : String , uppercase : Option < String > , char_count : Option < usize > , summary : Option < String > , } impl TextContext { fn new ( input : & str ) -> Self { Self { input : input . to_string (), uppercase : None , char_count : None , summary : None , } } } #[derive(Debug, Error)] enum TextProcessingError { #[error( \"Input text is empty\" )] EmptyInput , #[error( \"Input exceeds maximum length of {0} characters\" )] InputTooLong ( usize ), #[error( \"Processing failed: {0}\" )] ProcessingError ( String ), } impl From < TextProcessingError > for FloxideError { fn from ( e : TextProcessingError ) -> Self { FloxideError :: Other ( e . to_string ()) } } Now, let's create our nodes using the three-phase lifecycle: // Node that validates and prepares text struct ValidatorNode { max_length : usize , } #[async_trait] impl LifecycleNode < TextContext , DefaultAction > for ValidatorNode { type PrepOutput = String ; type ExecOutput = String ; fn id ( & self ) -> NodeId { NodeId :: new ( \"validator\" ) } async fn prep ( & self , ctx : & mut TextContext ) -> Result < Self :: PrepOutput , FloxideError > { if ctx . input . is_empty () { return Err ( TextProcessingError :: EmptyInput . into ()); } Ok ( ctx . input . clone ()) } async fn exec ( & self , input : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > { if input . len () > self . max_length { return Err ( TextProcessingError :: InputTooLong ( self . max_length ). into ()); } Ok ( input ) } async fn post ( & self , input : Self :: ExecOutput ) -> Result < DefaultAction , FloxideError > { Ok ( DefaultAction :: Next ) } } // Node that converts text to uppercase with retry logic struct UppercaseNode { retry_count : usize , } #[async_trait] impl LifecycleNode < TextContext , DefaultAction > for UppercaseNode { type PrepOutput = String ; type ExecOutput = String ; fn id ( & self ) -> NodeId { NodeId :: new ( \"uppercase\" ) } async fn prep ( & self , ctx : & mut TextContext ) -> Result < Self :: PrepOutput , FloxideError > { Ok ( ctx . input . clone ()) } async fn exec ( & self , input : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > { let mut attempts = 0 ; loop { match try_uppercase ( & input ) { Ok ( result ) => { return Ok ( result ); } Err ( e ) if attempts < self . retry_count => { attempts += 1 ; tracing :: warn ! ( \"Uppercase conversion failed, attempt {}/{}: {}\" , attempts , self . retry_count , e ); tokio :: time :: sleep ( std :: time :: Duration :: from_millis ( 100 )). await ; } Err ( e ) => { return Err ( TextProcessingError :: ProcessingError ( format! ( \"Failed after {} retries: {}\" , self . retry_count , e ) ). into ()); } } } } async fn post ( & self , result : Self :: ExecOutput ) -> Result < DefaultAction , FloxideError > { Ok ( DefaultAction :: Next ) } } // Node that counts characters with validation struct CounterNode ; #[async_trait] impl LifecycleNode < TextContext , DefaultAction > for CounterNode { type PrepOutput = String ; type ExecOutput = usize ; fn id ( & self ) -> NodeId { NodeId :: new ( \"counter\" ) } async fn prep ( & self , ctx : & mut TextContext ) -> Result < Self :: PrepOutput , FloxideError > { ctx . uppercase . clone (). ok_or_else ( || TextProcessingError :: ProcessingError ( \"No uppercase text available\" . to_string ()). into () ) } async fn exec ( & self , input : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > { Ok ( input . chars (). count ()) } async fn post ( & self , count : Self :: ExecOutput ) -> Result < DefaultAction , FloxideError > { Ok ( DefaultAction :: Next ) } } // Node that generates a summary struct SummaryNode ; #[async_trait] impl LifecycleNode < TextContext , DefaultAction > for SummaryNode { type PrepOutput = ( String , usize ); type ExecOutput = String ; fn id ( & self ) -> NodeId { NodeId :: new ( \"summary\" ) } async fn prep ( & self , ctx : & mut TextContext ) -> Result < Self :: PrepOutput , FloxideError > { let text = ctx . uppercase . clone (). ok_or_else ( || TextProcessingError :: ProcessingError ( \"No uppercase text available\" . to_string ()) ) ? ; let count = ctx . char_count . ok_or_else ( || TextProcessingError :: ProcessingError ( \"No character count available\" . to_string ()) ) ? ; Ok (( text , count )) } async fn exec ( & self , ( text , count ): Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > { Ok ( format! ( \"Processed text with {} characters: {}\" , count , text )) } async fn post ( & self , summary : Self :: ExecOutput ) -> Result < DefaultAction , FloxideError > { Ok ( DefaultAction :: Complete ) } } // Helper function that could fail fn try_uppercase ( input : & str ) -> Result < String , TextProcessingError > { // Simulate potential failures if rand :: random :: < f32 > () < 0.2 { return Err ( TextProcessingError :: ProcessingError ( \"Random processing failure\" . to_string () )); } Ok ( input . to_uppercase ()) } Now let's create and run the workflow: use floxide_core :: Workflow ; #[tokio::main] async fn main () -> Result < (), FloxideError > { // Initialize tracing tracing_subscriber :: fmt :: init (); // Create context let mut context = TextContext :: new ( \"Hello, Floxide!\" ); // Create nodes let validator = ValidatorNode { max_length : 100 }; let uppercase = UppercaseNode { retry_count : 3 }; let counter = CounterNode ; let summary = SummaryNode ; // Create workflow let mut workflow = Workflow :: new ( validator ) . then ( uppercase ) . then ( counter ) . then ( summary ); // Run workflow match workflow . run ( & mut context ). await { Ok ( _ ) => { println! ( \"Workflow completed successfully!\" ); println! ( \"Summary: {}\" , context . summary . unwrap_or_default ()); Ok (()) } Err ( e ) => { eprintln! ( \"Workflow failed: {}\" , e ); Err ( e ) } } } Key Concepts Demonstrated \u00b6 Node Lifecycle Preparation phase for validation and setup Execution phase with retry logic Post-processing phase for routing decisions Error Handling Custom error types Proper error propagation Retry mechanisms for transient failures Type Safety Strongly typed context Type-safe node outputs Safe error handling Best Practices Clear separation of concerns Proper logging and observability Resource cleanup Error recovery strategies Next Steps \u00b6 Explore more complex workflow patterns in the Workflow Patterns guide Learn about error handling in the Error Handling Guide Check out the Event-Driven Workflow example for handling events See the Batch Processing example for handling collections of items","title":"Basic Workflow"},{"location":"examples/basic-workflow/#basic-workflow-example","text":"This example demonstrates how to create a simple workflow using the Floxide framework, showcasing proper node lifecycle management and error handling.","title":"Basic Workflow Example"},{"location":"examples/basic-workflow/#overview","text":"In this example, we'll create a workflow that processes text data through multiple stages: Input validation and preparation Text transformation (uppercase) Analysis (character count) Summary generation The example demonstrates: - Three-phase node lifecycle (prep, exec, post) - Proper error handling and recovery - Type-safe context management - Node composition patterns","title":"Overview"},{"location":"examples/basic-workflow/#prerequisites","text":"Before running this example, make sure you have the Floxide framework installed. See the Installation Guide for details.","title":"Prerequisites"},{"location":"examples/basic-workflow/#implementation","text":"First, let's define our context and error types: use floxide_core ::{ lifecycle_node , LifecycleNode , DefaultAction , FloxideError }; use thiserror :: Error ; #[derive(Debug, Clone)] struct TextContext { input : String , uppercase : Option < String > , char_count : Option < usize > , summary : Option < String > , } impl TextContext { fn new ( input : & str ) -> Self { Self { input : input . to_string (), uppercase : None , char_count : None , summary : None , } } } #[derive(Debug, Error)] enum TextProcessingError { #[error( \"Input text is empty\" )] EmptyInput , #[error( \"Input exceeds maximum length of {0} characters\" )] InputTooLong ( usize ), #[error( \"Processing failed: {0}\" )] ProcessingError ( String ), } impl From < TextProcessingError > for FloxideError { fn from ( e : TextProcessingError ) -> Self { FloxideError :: Other ( e . to_string ()) } } Now, let's create our nodes using the three-phase lifecycle: // Node that validates and prepares text struct ValidatorNode { max_length : usize , } #[async_trait] impl LifecycleNode < TextContext , DefaultAction > for ValidatorNode { type PrepOutput = String ; type ExecOutput = String ; fn id ( & self ) -> NodeId { NodeId :: new ( \"validator\" ) } async fn prep ( & self , ctx : & mut TextContext ) -> Result < Self :: PrepOutput , FloxideError > { if ctx . input . is_empty () { return Err ( TextProcessingError :: EmptyInput . into ()); } Ok ( ctx . input . clone ()) } async fn exec ( & self , input : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > { if input . len () > self . max_length { return Err ( TextProcessingError :: InputTooLong ( self . max_length ). into ()); } Ok ( input ) } async fn post ( & self , input : Self :: ExecOutput ) -> Result < DefaultAction , FloxideError > { Ok ( DefaultAction :: Next ) } } // Node that converts text to uppercase with retry logic struct UppercaseNode { retry_count : usize , } #[async_trait] impl LifecycleNode < TextContext , DefaultAction > for UppercaseNode { type PrepOutput = String ; type ExecOutput = String ; fn id ( & self ) -> NodeId { NodeId :: new ( \"uppercase\" ) } async fn prep ( & self , ctx : & mut TextContext ) -> Result < Self :: PrepOutput , FloxideError > { Ok ( ctx . input . clone ()) } async fn exec ( & self , input : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > { let mut attempts = 0 ; loop { match try_uppercase ( & input ) { Ok ( result ) => { return Ok ( result ); } Err ( e ) if attempts < self . retry_count => { attempts += 1 ; tracing :: warn ! ( \"Uppercase conversion failed, attempt {}/{}: {}\" , attempts , self . retry_count , e ); tokio :: time :: sleep ( std :: time :: Duration :: from_millis ( 100 )). await ; } Err ( e ) => { return Err ( TextProcessingError :: ProcessingError ( format! ( \"Failed after {} retries: {}\" , self . retry_count , e ) ). into ()); } } } } async fn post ( & self , result : Self :: ExecOutput ) -> Result < DefaultAction , FloxideError > { Ok ( DefaultAction :: Next ) } } // Node that counts characters with validation struct CounterNode ; #[async_trait] impl LifecycleNode < TextContext , DefaultAction > for CounterNode { type PrepOutput = String ; type ExecOutput = usize ; fn id ( & self ) -> NodeId { NodeId :: new ( \"counter\" ) } async fn prep ( & self , ctx : & mut TextContext ) -> Result < Self :: PrepOutput , FloxideError > { ctx . uppercase . clone (). ok_or_else ( || TextProcessingError :: ProcessingError ( \"No uppercase text available\" . to_string ()). into () ) } async fn exec ( & self , input : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > { Ok ( input . chars (). count ()) } async fn post ( & self , count : Self :: ExecOutput ) -> Result < DefaultAction , FloxideError > { Ok ( DefaultAction :: Next ) } } // Node that generates a summary struct SummaryNode ; #[async_trait] impl LifecycleNode < TextContext , DefaultAction > for SummaryNode { type PrepOutput = ( String , usize ); type ExecOutput = String ; fn id ( & self ) -> NodeId { NodeId :: new ( \"summary\" ) } async fn prep ( & self , ctx : & mut TextContext ) -> Result < Self :: PrepOutput , FloxideError > { let text = ctx . uppercase . clone (). ok_or_else ( || TextProcessingError :: ProcessingError ( \"No uppercase text available\" . to_string ()) ) ? ; let count = ctx . char_count . ok_or_else ( || TextProcessingError :: ProcessingError ( \"No character count available\" . to_string ()) ) ? ; Ok (( text , count )) } async fn exec ( & self , ( text , count ): Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > { Ok ( format! ( \"Processed text with {} characters: {}\" , count , text )) } async fn post ( & self , summary : Self :: ExecOutput ) -> Result < DefaultAction , FloxideError > { Ok ( DefaultAction :: Complete ) } } // Helper function that could fail fn try_uppercase ( input : & str ) -> Result < String , TextProcessingError > { // Simulate potential failures if rand :: random :: < f32 > () < 0.2 { return Err ( TextProcessingError :: ProcessingError ( \"Random processing failure\" . to_string () )); } Ok ( input . to_uppercase ()) } Now let's create and run the workflow: use floxide_core :: Workflow ; #[tokio::main] async fn main () -> Result < (), FloxideError > { // Initialize tracing tracing_subscriber :: fmt :: init (); // Create context let mut context = TextContext :: new ( \"Hello, Floxide!\" ); // Create nodes let validator = ValidatorNode { max_length : 100 }; let uppercase = UppercaseNode { retry_count : 3 }; let counter = CounterNode ; let summary = SummaryNode ; // Create workflow let mut workflow = Workflow :: new ( validator ) . then ( uppercase ) . then ( counter ) . then ( summary ); // Run workflow match workflow . run ( & mut context ). await { Ok ( _ ) => { println! ( \"Workflow completed successfully!\" ); println! ( \"Summary: {}\" , context . summary . unwrap_or_default ()); Ok (()) } Err ( e ) => { eprintln! ( \"Workflow failed: {}\" , e ); Err ( e ) } } }","title":"Implementation"},{"location":"examples/basic-workflow/#key-concepts-demonstrated","text":"Node Lifecycle Preparation phase for validation and setup Execution phase with retry logic Post-processing phase for routing decisions Error Handling Custom error types Proper error propagation Retry mechanisms for transient failures Type Safety Strongly typed context Type-safe node outputs Safe error handling Best Practices Clear separation of concerns Proper logging and observability Resource cleanup Error recovery strategies","title":"Key Concepts Demonstrated"},{"location":"examples/basic-workflow/#next-steps","text":"Explore more complex workflow patterns in the Workflow Patterns guide Learn about error handling in the Error Handling Guide Check out the Event-Driven Workflow example for handling events See the Batch Processing example for handling collections of items","title":"Next Steps"},{"location":"examples/batch-processing/","text":"Batch Processing Example \u00b6 This document provides a complete example of using batch processing capabilities in the Floxide framework. Overview \u00b6 Batch processing allows you to efficiently process collections of items in parallel. This example demonstrates how to create and use batch nodes to process multiple items concurrently. Prerequisites \u00b6 Before running this example, ensure you have the following dependencies in your Cargo.toml : [dependencies] floxide-core = \"0.1.0\" floxide-transform = \"0.1.0\" tokio = { version = \"1.0\" , features = [ \"full\" ] } Example Implementation \u00b6 Step 1: Define State and Item Types \u00b6 First, define the state and item types for your batch processing workflow: use floxide_core :: prelude :: * ; use floxide_transform :: prelude :: * ; use std :: sync :: Arc ; // Define a state type to track processing #[derive(Clone)] struct ProcessingState { total_processed : usize , total_errors : usize , } // Define an item type to process #[derive(Clone, Debug)] struct DataItem { id : usize , value : String , } Step 2: Create a Batch Context \u00b6 Next, create a batch context that will manage the collection of items: // Create a batch context implementation struct DataBatchContext { state : ProcessingState , items : Vec < DataItem > , results : Vec < Result < DataItem , FloxideError >> , } impl DataBatchContext { fn new ( items : Vec < DataItem > ) -> Self { Self { state : ProcessingState { total_processed : 0 , total_errors : 0 , }, items , results : Vec :: new (), } } fn get_results ( & self ) -> & Vec < Result < DataItem , FloxideError >> { & self . results } } impl BatchContext < DataItem > for DataBatchContext { fn get_batch_items ( & self ) -> Result < Vec < DataItem > , FloxideError > { Ok ( self . items . clone ()) } fn create_item_context ( & self , item : DataItem ) -> Result < Self , FloxideError > { Ok ( Self { state : self . state . clone (), items : vec ! [ item ], results : Vec :: new (), }) } fn update_with_results ( & mut self , results : Vec < Result < DataItem , FloxideError >> ) -> Result < (), FloxideError > { self . results = results ; // Update state based on results for result in & self . results { match result { Ok ( _ ) => self . state . total_processed += 1 , Err ( _ ) => self . state . total_errors += 1 , } } Ok (()) } } Step 3: Implement a Batch Node \u00b6 Now, implement a batch node that will process each item: // Create a batch processing node struct DataProcessorNode ; impl Node for DataProcessorNode { type Context = DataBatchContext ; type Action = NextAction ; async fn run ( & self , ctx : & mut Self :: Context ) -> Result < Self :: Action , FloxideError > { // Process a single item (this will be called for each item in the batch) if let Some ( item ) = ctx . get_batch_items () ? . first () { println! ( \"Processing item {}: {}\" , item . id , item . value ); // Simulate processing time tokio :: time :: sleep ( tokio :: time :: Duration :: from_millis ( 100 )). await ; // Transform the item (uppercase the value) let mut processed_item = item . clone (); processed_item . value = processed_item . value . to_uppercase (); // Replace the item in the context ctx . items = vec! [ processed_item ]; } Ok ( NextAction :: Continue ) } } impl BatchNode < DataItem > for DataProcessorNode {} Step 4: Create and Execute a Batch Flow \u00b6 Finally, create and execute a batch flow: #[tokio::main] async fn main () -> Result < (), FloxideError > { // Create sample data let items = vec! [ DataItem { id : 1 , value : \"item1\" . to_string () }, DataItem { id : 2 , value : \"item2\" . to_string () }, DataItem { id : 3 , value : \"item3\" . to_string () }, DataItem { id : 4 , value : \"item4\" . to_string () }, DataItem { id : 5 , value : \"item5\" . to_string () }, ]; // Create batch context let mut context = DataBatchContext :: new ( items ); // Create batch processor node let processor_node = DataProcessorNode ; // Create batch flow with concurrency limit of 3 let batch_flow = BatchFlow :: new ( processor_node , 3 ); // Execute the batch flow let result_context = batch_flow . execute ( context ). await ? ; // Print results println! ( \"Batch processing complete!\" ); println! ( \"Total processed: {}\" , result_context . state . total_processed ); println! ( \"Total errors: {}\" , result_context . state . total_errors ); for result in result_context . get_results () { match result { Ok ( item ) => println! ( \"Processed item {}: {}\" , item . id , item . value ), Err ( e ) => println! ( \"Error processing item: {}\" , e ), } } Ok (()) } Running the Example \u00b6 To run this example: Create a new Rust project with the dependencies listed above Copy the code into your src/main.rs file Run the example with cargo run You should see output similar to: Processing item 1: item1 Processing item 2: item2 Processing item 3: item3 Processing item 4: item4 Processing item 5: item5 Batch processing complete! Total processed: 5 Total errors: 0 Processed item 1: ITEM1 Processed item 2: ITEM2 Processed item 3: ITEM3 Processed item 4: ITEM4 Processed item 5: ITEM5 Advanced Techniques \u00b6 Error Handling \u00b6 You can add error handling to your batch processing node: async fn run ( & self , ctx : & mut Self :: Context ) -> Result < Self :: Action , FloxideError > { if let Some ( item ) = ctx . get_batch_items () ? . first () { // Simulate an error for items with even IDs if item . id % 2 == 0 { return Err ( FloxideError :: ProcessingFailed ( format! ( \"Failed to process item {}\" , item . id ))); } // Process normally for odd IDs println! ( \"Processing item {}: {}\" , item . id , item . value ); // Transform the item let mut processed_item = item . clone (); processed_item . value = processed_item . value . to_uppercase (); ctx . items = vec! [ processed_item ]; } Ok ( NextAction :: Continue ) } Custom Batch Processor \u00b6 You can create a custom batch processor with specialized behavior: struct CustomBatchProcessor { concurrency_limit : usize , retry_count : usize , } impl CustomBatchProcessor { fn new ( concurrency_limit : usize , retry_count : usize ) -> Self { Self { concurrency_limit , retry_count } } async fn process_with_retry < T , F , Fut > ( & self , items : Vec < T > , processor : F ) -> Vec < Result < T , FloxideError >> where F : Fn ( T ) -> Fut + Send + Sync + Clone + ' static , Fut : Future < Output = Result < T , FloxideError >> + Send , T : Clone + Send + ' static , { // Implementation with retry logic // ... } } Conclusion \u00b6 This example demonstrates how to use batch processing in the Floxide framework to efficiently process collections of items in parallel. By leveraging the BatchContext , BatchNode , and BatchFlow abstractions, you can create powerful and flexible batch processing workflows. For more information on batch processing, refer to the Batch Processing Implementation documentation.","title":"Batch Processing"},{"location":"examples/batch-processing/#batch-processing-example","text":"This document provides a complete example of using batch processing capabilities in the Floxide framework.","title":"Batch Processing Example"},{"location":"examples/batch-processing/#overview","text":"Batch processing allows you to efficiently process collections of items in parallel. This example demonstrates how to create and use batch nodes to process multiple items concurrently.","title":"Overview"},{"location":"examples/batch-processing/#prerequisites","text":"Before running this example, ensure you have the following dependencies in your Cargo.toml : [dependencies] floxide-core = \"0.1.0\" floxide-transform = \"0.1.0\" tokio = { version = \"1.0\" , features = [ \"full\" ] }","title":"Prerequisites"},{"location":"examples/batch-processing/#example-implementation","text":"","title":"Example Implementation"},{"location":"examples/batch-processing/#step-1-define-state-and-item-types","text":"First, define the state and item types for your batch processing workflow: use floxide_core :: prelude :: * ; use floxide_transform :: prelude :: * ; use std :: sync :: Arc ; // Define a state type to track processing #[derive(Clone)] struct ProcessingState { total_processed : usize , total_errors : usize , } // Define an item type to process #[derive(Clone, Debug)] struct DataItem { id : usize , value : String , }","title":"Step 1: Define State and Item Types"},{"location":"examples/batch-processing/#step-2-create-a-batch-context","text":"Next, create a batch context that will manage the collection of items: // Create a batch context implementation struct DataBatchContext { state : ProcessingState , items : Vec < DataItem > , results : Vec < Result < DataItem , FloxideError >> , } impl DataBatchContext { fn new ( items : Vec < DataItem > ) -> Self { Self { state : ProcessingState { total_processed : 0 , total_errors : 0 , }, items , results : Vec :: new (), } } fn get_results ( & self ) -> & Vec < Result < DataItem , FloxideError >> { & self . results } } impl BatchContext < DataItem > for DataBatchContext { fn get_batch_items ( & self ) -> Result < Vec < DataItem > , FloxideError > { Ok ( self . items . clone ()) } fn create_item_context ( & self , item : DataItem ) -> Result < Self , FloxideError > { Ok ( Self { state : self . state . clone (), items : vec ! [ item ], results : Vec :: new (), }) } fn update_with_results ( & mut self , results : Vec < Result < DataItem , FloxideError >> ) -> Result < (), FloxideError > { self . results = results ; // Update state based on results for result in & self . results { match result { Ok ( _ ) => self . state . total_processed += 1 , Err ( _ ) => self . state . total_errors += 1 , } } Ok (()) } }","title":"Step 2: Create a Batch Context"},{"location":"examples/batch-processing/#step-3-implement-a-batch-node","text":"Now, implement a batch node that will process each item: // Create a batch processing node struct DataProcessorNode ; impl Node for DataProcessorNode { type Context = DataBatchContext ; type Action = NextAction ; async fn run ( & self , ctx : & mut Self :: Context ) -> Result < Self :: Action , FloxideError > { // Process a single item (this will be called for each item in the batch) if let Some ( item ) = ctx . get_batch_items () ? . first () { println! ( \"Processing item {}: {}\" , item . id , item . value ); // Simulate processing time tokio :: time :: sleep ( tokio :: time :: Duration :: from_millis ( 100 )). await ; // Transform the item (uppercase the value) let mut processed_item = item . clone (); processed_item . value = processed_item . value . to_uppercase (); // Replace the item in the context ctx . items = vec! [ processed_item ]; } Ok ( NextAction :: Continue ) } } impl BatchNode < DataItem > for DataProcessorNode {}","title":"Step 3: Implement a Batch Node"},{"location":"examples/batch-processing/#step-4-create-and-execute-a-batch-flow","text":"Finally, create and execute a batch flow: #[tokio::main] async fn main () -> Result < (), FloxideError > { // Create sample data let items = vec! [ DataItem { id : 1 , value : \"item1\" . to_string () }, DataItem { id : 2 , value : \"item2\" . to_string () }, DataItem { id : 3 , value : \"item3\" . to_string () }, DataItem { id : 4 , value : \"item4\" . to_string () }, DataItem { id : 5 , value : \"item5\" . to_string () }, ]; // Create batch context let mut context = DataBatchContext :: new ( items ); // Create batch processor node let processor_node = DataProcessorNode ; // Create batch flow with concurrency limit of 3 let batch_flow = BatchFlow :: new ( processor_node , 3 ); // Execute the batch flow let result_context = batch_flow . execute ( context ). await ? ; // Print results println! ( \"Batch processing complete!\" ); println! ( \"Total processed: {}\" , result_context . state . total_processed ); println! ( \"Total errors: {}\" , result_context . state . total_errors ); for result in result_context . get_results () { match result { Ok ( item ) => println! ( \"Processed item {}: {}\" , item . id , item . value ), Err ( e ) => println! ( \"Error processing item: {}\" , e ), } } Ok (()) }","title":"Step 4: Create and Execute a Batch Flow"},{"location":"examples/batch-processing/#running-the-example","text":"To run this example: Create a new Rust project with the dependencies listed above Copy the code into your src/main.rs file Run the example with cargo run You should see output similar to: Processing item 1: item1 Processing item 2: item2 Processing item 3: item3 Processing item 4: item4 Processing item 5: item5 Batch processing complete! Total processed: 5 Total errors: 0 Processed item 1: ITEM1 Processed item 2: ITEM2 Processed item 3: ITEM3 Processed item 4: ITEM4 Processed item 5: ITEM5","title":"Running the Example"},{"location":"examples/batch-processing/#advanced-techniques","text":"","title":"Advanced Techniques"},{"location":"examples/batch-processing/#error-handling","text":"You can add error handling to your batch processing node: async fn run ( & self , ctx : & mut Self :: Context ) -> Result < Self :: Action , FloxideError > { if let Some ( item ) = ctx . get_batch_items () ? . first () { // Simulate an error for items with even IDs if item . id % 2 == 0 { return Err ( FloxideError :: ProcessingFailed ( format! ( \"Failed to process item {}\" , item . id ))); } // Process normally for odd IDs println! ( \"Processing item {}: {}\" , item . id , item . value ); // Transform the item let mut processed_item = item . clone (); processed_item . value = processed_item . value . to_uppercase (); ctx . items = vec! [ processed_item ]; } Ok ( NextAction :: Continue ) }","title":"Error Handling"},{"location":"examples/batch-processing/#custom-batch-processor","text":"You can create a custom batch processor with specialized behavior: struct CustomBatchProcessor { concurrency_limit : usize , retry_count : usize , } impl CustomBatchProcessor { fn new ( concurrency_limit : usize , retry_count : usize ) -> Self { Self { concurrency_limit , retry_count } } async fn process_with_retry < T , F , Fut > ( & self , items : Vec < T > , processor : F ) -> Vec < Result < T , FloxideError >> where F : Fn ( T ) -> Fut + Send + Sync + Clone + ' static , Fut : Future < Output = Result < T , FloxideError >> + Send , T : Clone + Send + ' static , { // Implementation with retry logic // ... } }","title":"Custom Batch Processor"},{"location":"examples/batch-processing/#conclusion","text":"This example demonstrates how to use batch processing in the Floxide framework to efficiently process collections of items in parallel. By leveraging the BatchContext , BatchNode , and BatchFlow abstractions, you can create powerful and flexible batch processing workflows. For more information on batch processing, refer to the Batch Processing Implementation documentation.","title":"Conclusion"},{"location":"examples/event-driven-workflow/","text":"Event-Driven Workflow Example \u00b6 This document provides a complete example of using event-driven workflow capabilities in the Floxide framework. Overview \u00b6 Event-driven workflows allow you to build reactive systems that respond to external events in real-time. This example demonstrates how to create and use event-driven nodes to process events from various sources. Prerequisites \u00b6 Before running this example, ensure you have the following dependencies in your Cargo.toml : [dependencies] floxide-core = \"0.1.0\" floxide-transform = \"0.1.0\" floxide-event = \"0.1.0\" tokio = { version = \"1.0\" , features = [ \"full\" ] } chrono = { version = \"0.4\" , features = [ \"serde\" ] } Example Implementation \u00b6 Step 1: Define Event Types \u00b6 First, define the event types for your event-driven workflow: use floxide_core :: prelude :: * ; use floxide_event :: prelude :: * ; use chrono ::{ DateTime , Utc }; use std :: sync :: Arc ; // Define a sensor event type #[derive(Clone, Debug)] struct SensorEvent { id : String , value : f64 , timestamp : DateTime < Utc > , } // Implement the Event trait for the sensor event impl Event for SensorEvent {} Step 2: Create Event Sources \u00b6 Next, create event sources that will provide events to your workflow: use tokio :: sync :: mpsc ; // Create channel-based event sources let ( sensor_tx , sensor_rx ) = mpsc :: channel :: < SensorEvent > ( 100 ); let sensor_source = ChannelEventSource :: new ( sensor_rx ); // Create a shared event source that can be used by multiple nodes let shared_sensor_source = Arc :: new ( sensor_source ); Step 3: Implement Event-Driven Nodes \u00b6 Now, implement event-driven nodes that will process the events: // Create a sensor monitor node struct SensorMonitorNode { id : NodeId , event_source : Arc < dyn EventSource < SensorEvent >> , threshold : f64 , } impl SensorMonitorNode { fn new ( id_str : & str , event_source : Arc < dyn EventSource < SensorEvent >> , threshold : f64 ) -> Self { Self { id : NodeId :: from_string ( id_str ), event_source , threshold , } } } #[async_trait] impl EventDrivenNode < SensorEvent > for SensorMonitorNode { fn id ( & self ) -> NodeId { self . id } async fn wait_for_event ( & self ) -> Result < SensorEvent , FloxideError > { self . event_source . next_event (). await } async fn process_event ( & self , event : SensorEvent ) -> Result < EventAction < SensorEvent > , FloxideError > { println! ( \"Processing sensor event: id={}, value={}\" , event . id , event . value ); // Route based on the sensor value if event . value > self . threshold { println! ( \"High value detected, routing to alert handler\" ); Ok ( EventAction :: Route ( \"alert_handler\" . to_string (), event )) } else { println! ( \"Normal value, routing to standard handler\" ); Ok ( EventAction :: Route ( \"standard_handler\" . to_string (), event )) } } } // Create an alert handler node struct AlertHandlerNode { id : NodeId , } impl AlertHandlerNode { fn new ( id_str : & str ) -> Self { Self { id : NodeId :: from_string ( id_str ), } } } #[async_trait] impl EventDrivenNode < SensorEvent > for AlertHandlerNode { fn id ( & self ) -> NodeId { self . id } async fn wait_for_event ( & self ) -> Result < SensorEvent , FloxideError > { // This node doesn't wait for events directly, it receives them from the workflow // In a real implementation, you might want to add a timeout here Err ( FloxideError :: EventSourceClosed ) } async fn process_event ( & self , event : SensorEvent ) -> Result < EventAction < SensorEvent > , FloxideError > { println! ( \"ALERT: High sensor value detected: id={}, value={}\" , event . id , event . value ); // Send an alert notification (in a real system) // ... // Continue the workflow Ok ( EventAction :: Route ( \"logging\" . to_string (), event )) } } // Create a standard handler node struct StandardHandlerNode { id : NodeId , } impl StandardHandlerNode { fn new ( id_str : & str ) -> Self { Self { id : NodeId :: from_string ( id_str ), } } } #[async_trait] impl EventDrivenNode < SensorEvent > for StandardHandlerNode { fn id ( & self ) -> NodeId { self . id } async fn wait_for_event ( & self ) -> Result < SensorEvent , FloxideError > { // This node doesn't wait for events directly, it receives them from the workflow Err ( FloxideError :: EventSourceClosed ) } async fn process_event ( & self , event : SensorEvent ) -> Result < EventAction < SensorEvent > , FloxideError > { println! ( \"Standard processing for sensor: id={}, value={}\" , event . id , event . value ); // Process the event (in a real system) // ... // Continue the workflow Ok ( EventAction :: Route ( \"logging\" . to_string (), event )) } } // Create a logging node struct LoggingNode { id : NodeId , } impl LoggingNode { fn new ( id_str : & str ) -> Self { Self { id : NodeId :: from_string ( id_str ), } } } #[async_trait] impl EventDrivenNode < SensorEvent > for LoggingNode { fn id ( & self ) -> NodeId { self . id } async fn wait_for_event ( & self ) -> Result < SensorEvent , FloxideError > { // This node doesn't wait for events directly, it receives them from the workflow Err ( FloxideError :: EventSourceClosed ) } async fn process_event ( & self , event : SensorEvent ) -> Result < EventAction < SensorEvent > , FloxideError > { println! ( \"Logging event: id={}, value={}, timestamp={}\" , event . id , event . value , event . timestamp ); // In a real system, you might log to a database or file // ... // Return to the monitor node to wait for the next event Ok ( EventAction :: Route ( \"monitor\" . to_string (), event )) } } Step 4: Create and Configure the Workflow \u00b6 Now, create and configure the event-driven workflow: // Create the event-driven workflow let mut workflow = EventDrivenWorkflow :: < SensorEvent > :: new (); // Create the nodes let monitor_node = SensorMonitorNode :: new ( \"monitor\" , shared_sensor_source , 100.0 ); let alert_handler = AlertHandlerNode :: new ( \"alert_handler\" ); let standard_handler = StandardHandlerNode :: new ( \"standard_handler\" ); let logging_node = LoggingNode :: new ( \"logging\" ); // Add nodes to the workflow let monitor_id = workflow . add_node ( monitor_node ); let alert_id = workflow . add_node ( alert_handler ); let standard_id = workflow . add_node ( standard_handler ); let logging_id = workflow . add_node ( logging_node ); // Configure routes workflow . set_initial_node ( monitor_id ); workflow . set_route ( \"alert_handler\" , alert_id ); workflow . set_route ( \"standard_handler\" , standard_id ); workflow . set_route ( \"logging\" , logging_id ); workflow . set_route ( \"monitor\" , monitor_id ); // Set a timeout for the workflow (optional) workflow . set_timeout ( std :: time :: Duration :: from_secs ( 300 )); // 5 minutes Step 5: Run the Workflow and Send Events \u00b6 Finally, run the workflow and send events to it: #[tokio::main] async fn main () -> Result < (), FloxideError > { // Create and configure the workflow (as shown above) // ... // Run the workflow in a separate task let workflow_handle = tokio :: spawn ( async move { if let Err ( e ) = workflow . run (). await { eprintln! ( \"Workflow error: {}\" , e ); } }); // Send some test events for i in 1 ..= 5 { let value = if i % 2 == 0 { 120.0 } else { 80.0 }; let event = SensorEvent { id : format ! ( \"sensor-{}\" , i ), value , timestamp : Utc :: now (), }; println! ( \"Sending event: id={}, value={}\" , event . id , event . value ); sensor_tx . send ( event ). await . unwrap (); // Wait a bit between events tokio :: time :: sleep ( tokio :: time :: Duration :: from_secs ( 1 )). await ; } // Wait a bit for processing to complete tokio :: time :: sleep ( tokio :: time :: Duration :: from_secs ( 2 )). await ; // In a real application, you might wait for user input to terminate // or use a signal handler to gracefully shut down Ok (()) } Running the Example \u00b6 To run this example: Create a new Rust project with the dependencies listed above Copy the code into your src/main.rs file Run the example with cargo run You should see output similar to: Sending event: id=sensor-1, value=80.0 Processing sensor event: id=sensor-1, value=80.0 Normal value, routing to standard handler Standard processing for sensor: id=sensor-1, value=80.0 Logging event: id=sensor-1, value=80.0, timestamp=2024-02-25T12:34:56.789012Z Sending event: id=sensor-2, value=120.0 Processing sensor event: id=sensor-2, value=120.0 High value detected, routing to alert handler ALERT: High sensor value detected: id=sensor-2, value=120.0 Logging event: id=sensor-2, value=120.0, timestamp=2024-02-25T12:34:57.789012Z ... Advanced Techniques \u00b6 Custom Event Sources \u00b6 You can create custom event sources for different types of event producers: // Create a WebSocket event source struct WebSocketEventSource < E > { // WebSocket connection details // ... _phantom : PhantomData < E > , } #[async_trait] impl < E > EventSource < E > for WebSocketEventSource < E > where E : Event + Send + ' static , for <' de > E : serde :: Deserialize <' de > , { async fn next_event ( & self ) -> Result < E , FloxideError > { // Wait for and parse the next WebSocket message // ... } async fn has_more_events ( & self ) -> Result < bool , FloxideError > { // Check if the WebSocket connection is still open // ... } } Event Filtering \u00b6 You can add filtering capabilities to your event-driven nodes: struct FilteredSensorNode { id : NodeId , event_source : Arc < dyn EventSource < SensorEvent >> , filter : Box < dyn Fn ( & SensorEvent ) -> bool + Send + Sync > , } impl FilteredSensorNode { fn new ( id_str : & str , event_source : Arc < dyn EventSource < SensorEvent >> , filter : impl Fn ( & SensorEvent ) -> bool + Send + Sync + ' static , ) -> Self { Self { id : NodeId :: from_string ( id_str ), event_source , filter : Box :: new ( filter ), } } } #[async_trait] impl EventDrivenNode < SensorEvent > for FilteredSensorNode { // ... implementation with filtering logic } // Usage let temperature_filter = | event : & SensorEvent | event . id . starts_with ( \"temp-\" ); let filtered_node = FilteredSensorNode :: new ( \"temp_monitor\" , shared_source , temperature_filter ); Conclusion \u00b6 This example demonstrates how to use event-driven workflows in the Floxide framework to build reactive systems. By leveraging the EventDrivenNode , EventSource , and EventDrivenWorkflow abstractions, you can create powerful and flexible event processing pipelines. For more information on event-driven workflows, refer to the Event-Driven Workflow Pattern documentation.","title":"Event-Driven Workflow"},{"location":"examples/event-driven-workflow/#event-driven-workflow-example","text":"This document provides a complete example of using event-driven workflow capabilities in the Floxide framework.","title":"Event-Driven Workflow Example"},{"location":"examples/event-driven-workflow/#overview","text":"Event-driven workflows allow you to build reactive systems that respond to external events in real-time. This example demonstrates how to create and use event-driven nodes to process events from various sources.","title":"Overview"},{"location":"examples/event-driven-workflow/#prerequisites","text":"Before running this example, ensure you have the following dependencies in your Cargo.toml : [dependencies] floxide-core = \"0.1.0\" floxide-transform = \"0.1.0\" floxide-event = \"0.1.0\" tokio = { version = \"1.0\" , features = [ \"full\" ] } chrono = { version = \"0.4\" , features = [ \"serde\" ] }","title":"Prerequisites"},{"location":"examples/event-driven-workflow/#example-implementation","text":"","title":"Example Implementation"},{"location":"examples/event-driven-workflow/#step-1-define-event-types","text":"First, define the event types for your event-driven workflow: use floxide_core :: prelude :: * ; use floxide_event :: prelude :: * ; use chrono ::{ DateTime , Utc }; use std :: sync :: Arc ; // Define a sensor event type #[derive(Clone, Debug)] struct SensorEvent { id : String , value : f64 , timestamp : DateTime < Utc > , } // Implement the Event trait for the sensor event impl Event for SensorEvent {}","title":"Step 1: Define Event Types"},{"location":"examples/event-driven-workflow/#step-2-create-event-sources","text":"Next, create event sources that will provide events to your workflow: use tokio :: sync :: mpsc ; // Create channel-based event sources let ( sensor_tx , sensor_rx ) = mpsc :: channel :: < SensorEvent > ( 100 ); let sensor_source = ChannelEventSource :: new ( sensor_rx ); // Create a shared event source that can be used by multiple nodes let shared_sensor_source = Arc :: new ( sensor_source );","title":"Step 2: Create Event Sources"},{"location":"examples/event-driven-workflow/#step-3-implement-event-driven-nodes","text":"Now, implement event-driven nodes that will process the events: // Create a sensor monitor node struct SensorMonitorNode { id : NodeId , event_source : Arc < dyn EventSource < SensorEvent >> , threshold : f64 , } impl SensorMonitorNode { fn new ( id_str : & str , event_source : Arc < dyn EventSource < SensorEvent >> , threshold : f64 ) -> Self { Self { id : NodeId :: from_string ( id_str ), event_source , threshold , } } } #[async_trait] impl EventDrivenNode < SensorEvent > for SensorMonitorNode { fn id ( & self ) -> NodeId { self . id } async fn wait_for_event ( & self ) -> Result < SensorEvent , FloxideError > { self . event_source . next_event (). await } async fn process_event ( & self , event : SensorEvent ) -> Result < EventAction < SensorEvent > , FloxideError > { println! ( \"Processing sensor event: id={}, value={}\" , event . id , event . value ); // Route based on the sensor value if event . value > self . threshold { println! ( \"High value detected, routing to alert handler\" ); Ok ( EventAction :: Route ( \"alert_handler\" . to_string (), event )) } else { println! ( \"Normal value, routing to standard handler\" ); Ok ( EventAction :: Route ( \"standard_handler\" . to_string (), event )) } } } // Create an alert handler node struct AlertHandlerNode { id : NodeId , } impl AlertHandlerNode { fn new ( id_str : & str ) -> Self { Self { id : NodeId :: from_string ( id_str ), } } } #[async_trait] impl EventDrivenNode < SensorEvent > for AlertHandlerNode { fn id ( & self ) -> NodeId { self . id } async fn wait_for_event ( & self ) -> Result < SensorEvent , FloxideError > { // This node doesn't wait for events directly, it receives them from the workflow // In a real implementation, you might want to add a timeout here Err ( FloxideError :: EventSourceClosed ) } async fn process_event ( & self , event : SensorEvent ) -> Result < EventAction < SensorEvent > , FloxideError > { println! ( \"ALERT: High sensor value detected: id={}, value={}\" , event . id , event . value ); // Send an alert notification (in a real system) // ... // Continue the workflow Ok ( EventAction :: Route ( \"logging\" . to_string (), event )) } } // Create a standard handler node struct StandardHandlerNode { id : NodeId , } impl StandardHandlerNode { fn new ( id_str : & str ) -> Self { Self { id : NodeId :: from_string ( id_str ), } } } #[async_trait] impl EventDrivenNode < SensorEvent > for StandardHandlerNode { fn id ( & self ) -> NodeId { self . id } async fn wait_for_event ( & self ) -> Result < SensorEvent , FloxideError > { // This node doesn't wait for events directly, it receives them from the workflow Err ( FloxideError :: EventSourceClosed ) } async fn process_event ( & self , event : SensorEvent ) -> Result < EventAction < SensorEvent > , FloxideError > { println! ( \"Standard processing for sensor: id={}, value={}\" , event . id , event . value ); // Process the event (in a real system) // ... // Continue the workflow Ok ( EventAction :: Route ( \"logging\" . to_string (), event )) } } // Create a logging node struct LoggingNode { id : NodeId , } impl LoggingNode { fn new ( id_str : & str ) -> Self { Self { id : NodeId :: from_string ( id_str ), } } } #[async_trait] impl EventDrivenNode < SensorEvent > for LoggingNode { fn id ( & self ) -> NodeId { self . id } async fn wait_for_event ( & self ) -> Result < SensorEvent , FloxideError > { // This node doesn't wait for events directly, it receives them from the workflow Err ( FloxideError :: EventSourceClosed ) } async fn process_event ( & self , event : SensorEvent ) -> Result < EventAction < SensorEvent > , FloxideError > { println! ( \"Logging event: id={}, value={}, timestamp={}\" , event . id , event . value , event . timestamp ); // In a real system, you might log to a database or file // ... // Return to the monitor node to wait for the next event Ok ( EventAction :: Route ( \"monitor\" . to_string (), event )) } }","title":"Step 3: Implement Event-Driven Nodes"},{"location":"examples/event-driven-workflow/#step-4-create-and-configure-the-workflow","text":"Now, create and configure the event-driven workflow: // Create the event-driven workflow let mut workflow = EventDrivenWorkflow :: < SensorEvent > :: new (); // Create the nodes let monitor_node = SensorMonitorNode :: new ( \"monitor\" , shared_sensor_source , 100.0 ); let alert_handler = AlertHandlerNode :: new ( \"alert_handler\" ); let standard_handler = StandardHandlerNode :: new ( \"standard_handler\" ); let logging_node = LoggingNode :: new ( \"logging\" ); // Add nodes to the workflow let monitor_id = workflow . add_node ( monitor_node ); let alert_id = workflow . add_node ( alert_handler ); let standard_id = workflow . add_node ( standard_handler ); let logging_id = workflow . add_node ( logging_node ); // Configure routes workflow . set_initial_node ( monitor_id ); workflow . set_route ( \"alert_handler\" , alert_id ); workflow . set_route ( \"standard_handler\" , standard_id ); workflow . set_route ( \"logging\" , logging_id ); workflow . set_route ( \"monitor\" , monitor_id ); // Set a timeout for the workflow (optional) workflow . set_timeout ( std :: time :: Duration :: from_secs ( 300 )); // 5 minutes","title":"Step 4: Create and Configure the Workflow"},{"location":"examples/event-driven-workflow/#step-5-run-the-workflow-and-send-events","text":"Finally, run the workflow and send events to it: #[tokio::main] async fn main () -> Result < (), FloxideError > { // Create and configure the workflow (as shown above) // ... // Run the workflow in a separate task let workflow_handle = tokio :: spawn ( async move { if let Err ( e ) = workflow . run (). await { eprintln! ( \"Workflow error: {}\" , e ); } }); // Send some test events for i in 1 ..= 5 { let value = if i % 2 == 0 { 120.0 } else { 80.0 }; let event = SensorEvent { id : format ! ( \"sensor-{}\" , i ), value , timestamp : Utc :: now (), }; println! ( \"Sending event: id={}, value={}\" , event . id , event . value ); sensor_tx . send ( event ). await . unwrap (); // Wait a bit between events tokio :: time :: sleep ( tokio :: time :: Duration :: from_secs ( 1 )). await ; } // Wait a bit for processing to complete tokio :: time :: sleep ( tokio :: time :: Duration :: from_secs ( 2 )). await ; // In a real application, you might wait for user input to terminate // or use a signal handler to gracefully shut down Ok (()) }","title":"Step 5: Run the Workflow and Send Events"},{"location":"examples/event-driven-workflow/#running-the-example","text":"To run this example: Create a new Rust project with the dependencies listed above Copy the code into your src/main.rs file Run the example with cargo run You should see output similar to: Sending event: id=sensor-1, value=80.0 Processing sensor event: id=sensor-1, value=80.0 Normal value, routing to standard handler Standard processing for sensor: id=sensor-1, value=80.0 Logging event: id=sensor-1, value=80.0, timestamp=2024-02-25T12:34:56.789012Z Sending event: id=sensor-2, value=120.0 Processing sensor event: id=sensor-2, value=120.0 High value detected, routing to alert handler ALERT: High sensor value detected: id=sensor-2, value=120.0 Logging event: id=sensor-2, value=120.0, timestamp=2024-02-25T12:34:57.789012Z ...","title":"Running the Example"},{"location":"examples/event-driven-workflow/#advanced-techniques","text":"","title":"Advanced Techniques"},{"location":"examples/event-driven-workflow/#custom-event-sources","text":"You can create custom event sources for different types of event producers: // Create a WebSocket event source struct WebSocketEventSource < E > { // WebSocket connection details // ... _phantom : PhantomData < E > , } #[async_trait] impl < E > EventSource < E > for WebSocketEventSource < E > where E : Event + Send + ' static , for <' de > E : serde :: Deserialize <' de > , { async fn next_event ( & self ) -> Result < E , FloxideError > { // Wait for and parse the next WebSocket message // ... } async fn has_more_events ( & self ) -> Result < bool , FloxideError > { // Check if the WebSocket connection is still open // ... } }","title":"Custom Event Sources"},{"location":"examples/event-driven-workflow/#event-filtering","text":"You can add filtering capabilities to your event-driven nodes: struct FilteredSensorNode { id : NodeId , event_source : Arc < dyn EventSource < SensorEvent >> , filter : Box < dyn Fn ( & SensorEvent ) -> bool + Send + Sync > , } impl FilteredSensorNode { fn new ( id_str : & str , event_source : Arc < dyn EventSource < SensorEvent >> , filter : impl Fn ( & SensorEvent ) -> bool + Send + Sync + ' static , ) -> Self { Self { id : NodeId :: from_string ( id_str ), event_source , filter : Box :: new ( filter ), } } } #[async_trait] impl EventDrivenNode < SensorEvent > for FilteredSensorNode { // ... implementation with filtering logic } // Usage let temperature_filter = | event : & SensorEvent | event . id . starts_with ( \"temp-\" ); let filtered_node = FilteredSensorNode :: new ( \"temp_monitor\" , shared_source , temperature_filter );","title":"Event Filtering"},{"location":"examples/event-driven-workflow/#conclusion","text":"This example demonstrates how to use event-driven workflows in the Floxide framework to build reactive systems. By leveraging the EventDrivenNode , EventSource , and EventDrivenWorkflow abstractions, you can create powerful and flexible event processing pipelines. For more information on event-driven workflows, refer to the Event-Driven Workflow Pattern documentation.","title":"Conclusion"},{"location":"examples/longrunning-node/","text":"Long-Running Node Example \u00b6 This example demonstrates how to use long-running nodes in the Floxide framework for handling operations that take significant time to complete. Overview \u00b6 Long-running nodes are useful for: - Batch processing large datasets - External API calls with unknown duration - Complex computations - Resource-intensive operations Implementation \u00b6 Let's create a long-running node that processes a large dataset: use floxide_core ::{ lifecycle_node , LifecycleNode , DefaultAction }; use floxide_longrunning ::{ LongRunningNode , Progress , LongRunningStatus }; // Define our context #[derive(Debug, Clone)] struct ProcessingContext { input_size : usize , processed_items : usize , results : Vec < String > , } // Create a data processing node struct DataProcessor { chunk_size : usize , } impl LongRunningNode < ProcessingContext , DefaultAction > for DataProcessor { async fn start ( & self , context : & mut ProcessingContext ) -> Result < (), FloxideError > { // Initialize processing context . processed_items = 0 ; context . results . clear (); Ok (()) } async fn check_status ( & self , context : & mut ProcessingContext ) -> Result < LongRunningStatus , FloxideError > { if context . processed_items >= context . input_size { Ok ( LongRunningStatus :: Complete ) } else { // Process next chunk let start = context . processed_items ; let end = ( start + self . chunk_size ). min ( context . input_size ); for i in start .. end { context . results . push ( format! ( \"Processed item {}\" , i )); context . processed_items += 1 ; } let progress = Progress { percent : ( context . processed_items as f64 / context . input_size as f64 ) * 100.0 , message : format ! ( \"Processed {}/{} items\" , context . processed_items , context . input_size ), }; Ok ( LongRunningStatus :: Running ( progress )) } } async fn cleanup ( & self , context : & mut ProcessingContext ) -> Result < DefaultAction , FloxideError > { println! ( \"Processing complete: {} items processed\" , context . processed_items ); Ok ( DefaultAction :: Next ) } } Running the Example \u00b6 Here's how to use the long-running node: #[tokio::main] async fn main () { // Create initial context let context = ProcessingContext { input_size : 1000 , processed_items : 0 , results : Vec :: new (), }; // Create the processor node let processor = DataProcessor { chunk_size : 100 , }; // Create and run the workflow let mut workflow = Workflow :: new ( processor ); match workflow . run ( context ). await { Ok ( final_context ) => { println! ( \"Processing complete!\" ); println! ( \"Results: {} items\" , final_context . results . len ()); } Err ( e ) => eprintln! ( \"Processing failed: {}\" , e ), } } Advanced Usage \u00b6 Progress Monitoring \u00b6 // Create a progress monitoring wrapper struct ProgressMonitor < N > { inner : N , } impl < N : LongRunningNode < C , A > , C , A > LongRunningNode < C , A > for ProgressMonitor < N > { async fn check_status ( & self , context : & mut C ) -> Result < LongRunningStatus , FloxideError > { let status = self . inner . check_status ( context ). await ? ; if let LongRunningStatus :: Running ( progress ) = & status { println! ( \"Progress: {:.1}% - {}\" , progress . percent , progress . message ); } Ok ( status ) } // ... delegate other methods to inner } Cancellation Support \u00b6 // Add cancellation support to a long-running node impl LongRunningNode < ProcessingContext , DefaultAction > for DataProcessor { async fn check_status ( & self , context : & mut ProcessingContext ) -> Result < LongRunningStatus , FloxideError > { if context . should_cancel { return Ok ( LongRunningStatus :: Failed ( FloxideError :: new ( \"Processing cancelled\" ) )); } // ... normal processing ... } } Resource Management \u00b6 // Implement proper resource cleanup impl LongRunningNode < ProcessingContext , DefaultAction > for DataProcessor { async fn cleanup ( & self , context : & mut ProcessingContext ) -> Result < DefaultAction , FloxideError > { // Clean up any temporary files if let Err ( e ) = cleanup_temp_files (). await { eprintln! ( \"Warning: cleanup failed: {}\" , e ); } // Release any held resources context . results . shrink_to_fit (); Ok ( DefaultAction :: Next ) } } Best Practices \u00b6 Resource Management Clean up resources in the cleanup phase Handle cancellation gracefully Monitor memory usage Progress Reporting Provide meaningful progress updates Include both percentage and descriptive messages Update progress at reasonable intervals Error Handling Handle transient failures Provide clear error messages Clean up resources on failure Testing Test cancellation scenarios Verify resource cleanup Test progress reporting Simulate various failure modes See Also \u00b6 Long-Running Node Implementation ADR Node Lifecycle Methods Batch Processing","title":"Long-Running Node"},{"location":"examples/longrunning-node/#long-running-node-example","text":"This example demonstrates how to use long-running nodes in the Floxide framework for handling operations that take significant time to complete.","title":"Long-Running Node Example"},{"location":"examples/longrunning-node/#overview","text":"Long-running nodes are useful for: - Batch processing large datasets - External API calls with unknown duration - Complex computations - Resource-intensive operations","title":"Overview"},{"location":"examples/longrunning-node/#implementation","text":"Let's create a long-running node that processes a large dataset: use floxide_core ::{ lifecycle_node , LifecycleNode , DefaultAction }; use floxide_longrunning ::{ LongRunningNode , Progress , LongRunningStatus }; // Define our context #[derive(Debug, Clone)] struct ProcessingContext { input_size : usize , processed_items : usize , results : Vec < String > , } // Create a data processing node struct DataProcessor { chunk_size : usize , } impl LongRunningNode < ProcessingContext , DefaultAction > for DataProcessor { async fn start ( & self , context : & mut ProcessingContext ) -> Result < (), FloxideError > { // Initialize processing context . processed_items = 0 ; context . results . clear (); Ok (()) } async fn check_status ( & self , context : & mut ProcessingContext ) -> Result < LongRunningStatus , FloxideError > { if context . processed_items >= context . input_size { Ok ( LongRunningStatus :: Complete ) } else { // Process next chunk let start = context . processed_items ; let end = ( start + self . chunk_size ). min ( context . input_size ); for i in start .. end { context . results . push ( format! ( \"Processed item {}\" , i )); context . processed_items += 1 ; } let progress = Progress { percent : ( context . processed_items as f64 / context . input_size as f64 ) * 100.0 , message : format ! ( \"Processed {}/{} items\" , context . processed_items , context . input_size ), }; Ok ( LongRunningStatus :: Running ( progress )) } } async fn cleanup ( & self , context : & mut ProcessingContext ) -> Result < DefaultAction , FloxideError > { println! ( \"Processing complete: {} items processed\" , context . processed_items ); Ok ( DefaultAction :: Next ) } }","title":"Implementation"},{"location":"examples/longrunning-node/#running-the-example","text":"Here's how to use the long-running node: #[tokio::main] async fn main () { // Create initial context let context = ProcessingContext { input_size : 1000 , processed_items : 0 , results : Vec :: new (), }; // Create the processor node let processor = DataProcessor { chunk_size : 100 , }; // Create and run the workflow let mut workflow = Workflow :: new ( processor ); match workflow . run ( context ). await { Ok ( final_context ) => { println! ( \"Processing complete!\" ); println! ( \"Results: {} items\" , final_context . results . len ()); } Err ( e ) => eprintln! ( \"Processing failed: {}\" , e ), } }","title":"Running the Example"},{"location":"examples/longrunning-node/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"examples/longrunning-node/#progress-monitoring","text":"// Create a progress monitoring wrapper struct ProgressMonitor < N > { inner : N , } impl < N : LongRunningNode < C , A > , C , A > LongRunningNode < C , A > for ProgressMonitor < N > { async fn check_status ( & self , context : & mut C ) -> Result < LongRunningStatus , FloxideError > { let status = self . inner . check_status ( context ). await ? ; if let LongRunningStatus :: Running ( progress ) = & status { println! ( \"Progress: {:.1}% - {}\" , progress . percent , progress . message ); } Ok ( status ) } // ... delegate other methods to inner }","title":"Progress Monitoring"},{"location":"examples/longrunning-node/#cancellation-support","text":"// Add cancellation support to a long-running node impl LongRunningNode < ProcessingContext , DefaultAction > for DataProcessor { async fn check_status ( & self , context : & mut ProcessingContext ) -> Result < LongRunningStatus , FloxideError > { if context . should_cancel { return Ok ( LongRunningStatus :: Failed ( FloxideError :: new ( \"Processing cancelled\" ) )); } // ... normal processing ... } }","title":"Cancellation Support"},{"location":"examples/longrunning-node/#resource-management","text":"// Implement proper resource cleanup impl LongRunningNode < ProcessingContext , DefaultAction > for DataProcessor { async fn cleanup ( & self , context : & mut ProcessingContext ) -> Result < DefaultAction , FloxideError > { // Clean up any temporary files if let Err ( e ) = cleanup_temp_files (). await { eprintln! ( \"Warning: cleanup failed: {}\" , e ); } // Release any held resources context . results . shrink_to_fit (); Ok ( DefaultAction :: Next ) } }","title":"Resource Management"},{"location":"examples/longrunning-node/#best-practices","text":"Resource Management Clean up resources in the cleanup phase Handle cancellation gracefully Monitor memory usage Progress Reporting Provide meaningful progress updates Include both percentage and descriptive messages Update progress at reasonable intervals Error Handling Handle transient failures Provide clear error messages Clean up resources on failure Testing Test cancellation scenarios Verify resource cleanup Test progress reporting Simulate various failure modes","title":"Best Practices"},{"location":"examples/longrunning-node/#see-also","text":"Long-Running Node Implementation ADR Node Lifecycle Methods Batch Processing","title":"See Also"},{"location":"examples/reactive-node/","text":"Reactive Node Example \u00b6 This example demonstrates how to use reactive nodes in the Floxide framework for building event-driven, reactive workflows. Overview \u00b6 Reactive nodes enable: - Stream processing - Event-driven workflows - Real-time data processing - Backpressure handling Implementation \u00b6 Let's create a reactive workflow that processes a stream of temperature readings: use floxide_core ::{ ActionType , DefaultAction , FloxideError , NodeId }; use floxide_reactive ::{ CustomReactiveNode , ReactiveNode }; use futures :: stream ::{ self , Stream }; use std :: time ::{ Duration , Instant }; use tokio :: time ; // Define our data types #[derive(Debug, Clone)] struct Temperature { celsius : f64 , timestamp : Instant , } #[derive(Debug, Clone)] struct Alert { message : String , level : AlertLevel , timestamp : Instant , } #[derive(Debug, Clone)] enum AlertLevel { Info , Warning , Critical , } // Define our context #[derive(Debug, Clone)] struct MonitoringContext { temperatures : Vec < Temperature > , alerts : Vec < Alert > , } // Create a temperature monitoring node fn create_temperature_monitor () -> impl ReactiveNode < Temperature , MonitoringContext , DefaultAction > { CustomReactiveNode :: new ( // Watch function that creates a stream of temperature readings || { let stream = stream :: unfold ( 0 , | state | async move { time :: sleep ( Duration :: from_secs ( 1 )). await ; let temp = Temperature { celsius : 20.0 + ( state as f64 / 10.0 ), timestamp : Instant :: now (), }; Some (( temp , state + 1 )) }); Ok ( Box :: new ( stream )) }, // React function that processes each temperature reading | temp : Temperature , ctx : & mut MonitoringContext | { ctx . temperatures . push ( temp . clone ()); let alert = match temp . celsius { t if t > 25.0 => Some ( Alert { message : format ! ( \"Critical temperature: {:.1}\u00b0C\" , t ), level : AlertLevel :: Critical , timestamp : temp . timestamp , }), t if t > 22.0 => Some ( Alert { message : format ! ( \"Warning temperature: {:.1}\u00b0C\" , t ), level : AlertLevel :: Warning , timestamp : temp . timestamp , }), _ => None , }; if let Some ( alert ) = alert { ctx . alerts . push ( alert ); Ok ( DefaultAction :: change_detected ()) } else { Ok ( DefaultAction :: no_change ()) } }, ) } // Example usage #[tokio::main] async fn main () -> Result < (), FloxideError > { let mut ctx = MonitoringContext { temperatures : Vec :: new (), alerts : Vec :: new (), }; let monitor = create_temperature_monitor (); // Set up the watch stream with backpressure handling let mut stream = monitor . watch (). await ? ; // Process temperature readings with proper error handling while let Some ( temp ) = stream . next (). await { match monitor . react_to_change ( temp , & mut ctx ). await { Ok ( action ) if action . is_change_detected () => { println! ( \"Alert triggered! Total alerts: {}\" , ctx . alerts . len ()); } Ok ( _ ) => { println! ( \"Temperature normal: {:.1}\u00b0C\" , ctx . temperatures . last (). unwrap (). celsius ); } Err ( e ) => { eprintln! ( \"Error processing temperature: {}\" , e ); // Implement your error recovery strategy here } } } Ok (()) } Running the Example \u00b6 Here's how to run the reactive nodes: #[tokio::main] async fn main () -> Result < (), FloxideError > { // Create the reactive stream let ( temperature_stream , temperature_sender ) = ReactiveStream :: new (); // Create the nodes let monitor = create_temperature_monitor (); let processor = create_alert_processor (); // Create the workflow let workflow = temperature_stream . through ( monitor ) . through ( processor ) . for_each ( | message | println! ( \"{}\" , message )); // Send some test data temperature_sender . send ( Temperature { celsius : 24.0 , timestamp : Instant :: now (), }). await ; temperature_sender . send ( Temperature { celsius : 26.0 , timestamp : Instant :: now (), }). await ; temperature_sender . send ( Temperature { celsius : 31.0 , timestamp : Instant :: now (), }). await ; } Advanced Usage \u00b6 Backpressure Handling \u00b6 // Create a node with backpressure handling fn create_throttled_monitor () -> impl ReactiveNode < Temperature , Alert > { reactive_node ( | temp : Temperature | async move { // ... processing logic ... }) . with_buffer ( 100 ) // Buffer up to 100 items . with_backpressure ( BackpressureStrategy :: DropOldest ) } Error Handling \u00b6 // Create a node with error recovery fn create_robust_monitor () -> impl ReactiveNode < Temperature , Alert > { reactive_node ( | temp : Temperature | async move { if temp . celsius < - 273.15 { return Err ( FloxideError :: new ( \"Invalid temperature\" )); } // ... normal processing ... }) . with_retry ( 3 ) . with_error_handler ( | e | { println! ( \"Error processing temperature: {}\" , e ); None // Skip invalid temperatures }) } Combining Streams \u00b6 // Combine multiple temperature streams let combined = ReactiveStream :: combine ( vec! [ temp_stream1 , temp_stream2 , temp_stream3 ], | temps : Vec < Temperature >| { // Process multiple temperature readings let avg = temps . iter (). map ( | t | t . celsius ). sum :: < f64 > () / temps . len () as f64 ; Temperature { celsius : avg , timestamp : Instant :: now (), } } ); Best Practices \u00b6 Stream Management Handle backpressure appropriately Clean up resources when streams are dropped Use appropriate buffer sizes Error Handling Implement proper error recovery Handle stream completion Log errors for debugging Testing Test stream processing logic Verify backpressure handling Test error scenarios See Also \u00b6 Reactive Node Implementation ADR Event-Driven Architecture Node Lifecycle Methods","title":"Reactive Node"},{"location":"examples/reactive-node/#reactive-node-example","text":"This example demonstrates how to use reactive nodes in the Floxide framework for building event-driven, reactive workflows.","title":"Reactive Node Example"},{"location":"examples/reactive-node/#overview","text":"Reactive nodes enable: - Stream processing - Event-driven workflows - Real-time data processing - Backpressure handling","title":"Overview"},{"location":"examples/reactive-node/#implementation","text":"Let's create a reactive workflow that processes a stream of temperature readings: use floxide_core ::{ ActionType , DefaultAction , FloxideError , NodeId }; use floxide_reactive ::{ CustomReactiveNode , ReactiveNode }; use futures :: stream ::{ self , Stream }; use std :: time ::{ Duration , Instant }; use tokio :: time ; // Define our data types #[derive(Debug, Clone)] struct Temperature { celsius : f64 , timestamp : Instant , } #[derive(Debug, Clone)] struct Alert { message : String , level : AlertLevel , timestamp : Instant , } #[derive(Debug, Clone)] enum AlertLevel { Info , Warning , Critical , } // Define our context #[derive(Debug, Clone)] struct MonitoringContext { temperatures : Vec < Temperature > , alerts : Vec < Alert > , } // Create a temperature monitoring node fn create_temperature_monitor () -> impl ReactiveNode < Temperature , MonitoringContext , DefaultAction > { CustomReactiveNode :: new ( // Watch function that creates a stream of temperature readings || { let stream = stream :: unfold ( 0 , | state | async move { time :: sleep ( Duration :: from_secs ( 1 )). await ; let temp = Temperature { celsius : 20.0 + ( state as f64 / 10.0 ), timestamp : Instant :: now (), }; Some (( temp , state + 1 )) }); Ok ( Box :: new ( stream )) }, // React function that processes each temperature reading | temp : Temperature , ctx : & mut MonitoringContext | { ctx . temperatures . push ( temp . clone ()); let alert = match temp . celsius { t if t > 25.0 => Some ( Alert { message : format ! ( \"Critical temperature: {:.1}\u00b0C\" , t ), level : AlertLevel :: Critical , timestamp : temp . timestamp , }), t if t > 22.0 => Some ( Alert { message : format ! ( \"Warning temperature: {:.1}\u00b0C\" , t ), level : AlertLevel :: Warning , timestamp : temp . timestamp , }), _ => None , }; if let Some ( alert ) = alert { ctx . alerts . push ( alert ); Ok ( DefaultAction :: change_detected ()) } else { Ok ( DefaultAction :: no_change ()) } }, ) } // Example usage #[tokio::main] async fn main () -> Result < (), FloxideError > { let mut ctx = MonitoringContext { temperatures : Vec :: new (), alerts : Vec :: new (), }; let monitor = create_temperature_monitor (); // Set up the watch stream with backpressure handling let mut stream = monitor . watch (). await ? ; // Process temperature readings with proper error handling while let Some ( temp ) = stream . next (). await { match monitor . react_to_change ( temp , & mut ctx ). await { Ok ( action ) if action . is_change_detected () => { println! ( \"Alert triggered! Total alerts: {}\" , ctx . alerts . len ()); } Ok ( _ ) => { println! ( \"Temperature normal: {:.1}\u00b0C\" , ctx . temperatures . last (). unwrap (). celsius ); } Err ( e ) => { eprintln! ( \"Error processing temperature: {}\" , e ); // Implement your error recovery strategy here } } } Ok (()) }","title":"Implementation"},{"location":"examples/reactive-node/#running-the-example","text":"Here's how to run the reactive nodes: #[tokio::main] async fn main () -> Result < (), FloxideError > { // Create the reactive stream let ( temperature_stream , temperature_sender ) = ReactiveStream :: new (); // Create the nodes let monitor = create_temperature_monitor (); let processor = create_alert_processor (); // Create the workflow let workflow = temperature_stream . through ( monitor ) . through ( processor ) . for_each ( | message | println! ( \"{}\" , message )); // Send some test data temperature_sender . send ( Temperature { celsius : 24.0 , timestamp : Instant :: now (), }). await ; temperature_sender . send ( Temperature { celsius : 26.0 , timestamp : Instant :: now (), }). await ; temperature_sender . send ( Temperature { celsius : 31.0 , timestamp : Instant :: now (), }). await ; }","title":"Running the Example"},{"location":"examples/reactive-node/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"examples/reactive-node/#backpressure-handling","text":"// Create a node with backpressure handling fn create_throttled_monitor () -> impl ReactiveNode < Temperature , Alert > { reactive_node ( | temp : Temperature | async move { // ... processing logic ... }) . with_buffer ( 100 ) // Buffer up to 100 items . with_backpressure ( BackpressureStrategy :: DropOldest ) }","title":"Backpressure Handling"},{"location":"examples/reactive-node/#error-handling","text":"// Create a node with error recovery fn create_robust_monitor () -> impl ReactiveNode < Temperature , Alert > { reactive_node ( | temp : Temperature | async move { if temp . celsius < - 273.15 { return Err ( FloxideError :: new ( \"Invalid temperature\" )); } // ... normal processing ... }) . with_retry ( 3 ) . with_error_handler ( | e | { println! ( \"Error processing temperature: {}\" , e ); None // Skip invalid temperatures }) }","title":"Error Handling"},{"location":"examples/reactive-node/#combining-streams","text":"// Combine multiple temperature streams let combined = ReactiveStream :: combine ( vec! [ temp_stream1 , temp_stream2 , temp_stream3 ], | temps : Vec < Temperature >| { // Process multiple temperature readings let avg = temps . iter (). map ( | t | t . celsius ). sum :: < f64 > () / temps . len () as f64 ; Temperature { celsius : avg , timestamp : Instant :: now (), } } );","title":"Combining Streams"},{"location":"examples/reactive-node/#best-practices","text":"Stream Management Handle backpressure appropriately Clean up resources when streams are dropped Use appropriate buffer sizes Error Handling Implement proper error recovery Handle stream completion Log errors for debugging Testing Test stream processing logic Verify backpressure handling Test error scenarios","title":"Best Practices"},{"location":"examples/reactive-node/#see-also","text":"Reactive Node Implementation ADR Event-Driven Architecture Node Lifecycle Methods","title":"See Also"},{"location":"examples/timer-node/","text":"Timer Node Example \u00b6 This example demonstrates how to use timer nodes in the Floxide framework for scheduling and periodic execution. Overview \u00b6 Timer nodes allow you to: - Execute workflow nodes at specific times - Run periodic tasks - Implement delays and timeouts - Handle scheduled workflows Implementation \u00b6 First, let's create a simple timer-based workflow: use floxide_core ::{ DefaultAction , FloxideError , NodeId }; use floxide_timer ::{ Schedule , SimpleTimer , TimerNode , TimerWorkflow }; use chrono ::{ DateTime , Duration as ChronoDuration , Utc }; use std :: sync :: Arc ; // Define our context #[derive(Debug, Clone)] struct BackupContext { last_backup : Option < DateTime < Utc >> , backup_count : usize , status : String , } // Create different types of timer nodes fn create_backup_workflow () -> TimerWorkflow < BackupContext , DefaultAction > { // Daily backup at 2 AM let daily_backup = Arc :: new ( SimpleTimer :: with_id ( \"daily_backup\" , Schedule :: Cron ( \"0 2 * * *\" . to_string ()), | ctx : & mut BackupContext | { ctx . backup_count += 1 ; ctx . last_backup = Some ( Utc :: now ()); ctx . status = format! ( \"Daily backup completed ({})\" , ctx . backup_count ); Ok ( DefaultAction :: Next ) }, )); // Hourly health check let health_check = Arc :: new ( SimpleTimer :: with_id ( \"health_check\" , Schedule :: Periodic ( ChronoDuration :: hours ( 1 )), | ctx : & mut BackupContext | { ctx . status = \"Health check passed\" . to_string (); Ok ( DefaultAction :: Next ) }, )); // One-time initialization let init = Arc :: new ( SimpleTimer :: with_id ( \"init\" , Schedule :: Once ( Utc :: now ()), | ctx : & mut BackupContext | { ctx . status = \"Backup system initialized\" . to_string (); Ok ( DefaultAction :: Next ) }, )); // Create the workflow let mut workflow = TimerWorkflow :: new ( init , DefaultAction :: complete (), // Action to terminate the workflow ); // Add nodes and set up routes workflow . add_node ( daily_backup . clone ()); workflow . add_node ( health_check . clone ()); // Set up routing workflow . set_route ( & init . id (), DefaultAction :: Next , & daily_backup . id ()); workflow . set_route ( & daily_backup . id (), DefaultAction :: Next , & health_check . id ()); workflow . set_route ( & health_check . id (), DefaultAction :: Next , & daily_backup . id ()); workflow } // Example usage #[tokio::main] async fn main () -> Result < (), FloxideError > { let mut ctx = BackupContext { last_backup : None , backup_count : 0 , status : String :: new (), }; let workflow = create_backup_workflow (); // Execute the workflow with proper error handling match workflow . execute ( & mut ctx ). await { Ok (()) => { println! ( \"Workflow completed successfully\" ); println! ( \"Final status: {}\" , ctx . status ); println! ( \"Total backups: {}\" , ctx . backup_count ); } Err ( e ) => { eprintln! ( \"Workflow error: {}\" , e ); // Implement your error recovery strategy here } } Ok (()) } Running the Example \u00b6 To run the timer nodes, execute the main function. Advanced Usage \u00b6 Combining with Other Nodes \u00b6 Timer nodes can be combined with other node types: // Create a workflow that processes data periodically let mut workflow = TimerWorkflow :: new ( create_periodic_node ()) . then ( create_processor_node ()) . then ( create_cleanup_node ()); Error Handling \u00b6 Timer nodes include built-in error handling: fn create_robust_timer () -> impl TimerNode < BackupContext , DefaultAction > { SimpleTimer :: with_id ( \"robust_timer\" , Schedule :: Periodic ( ChronoDuration :: hours ( 1 )), | ctx : & mut BackupContext | { if let Err ( e ) = process_data (). await { ctx . status = format! ( \"Error: {}\" , e ); return Ok ( DefaultAction :: Stop ); } Ok ( DefaultAction :: Next ) }, ) . with_retry ( 3 ) // Retry up to 3 times on failure . with_backoff ( ChronoDuration :: seconds ( 1 )) // Wait 1 second between retries } Cancellation \u00b6 Timer nodes support graceful cancellation: fn create_cancellable_timer () -> impl TimerNode < BackupContext , DefaultAction > { SimpleTimer :: with_id ( \"cancellable_timer\" , Schedule :: Periodic ( ChronoDuration :: hours ( 1 )), | ctx : & mut BackupContext | { if ctx . backup_count >= 10 { return Ok ( DefaultAction :: Stop ); } Ok ( DefaultAction :: Next ) }, ) . with_cancellation () } Best Practices \u00b6 Resource Management Always implement proper cleanup for timer resources Use appropriate timeout values Consider system resource limitations Error Handling Implement retry logic for transient failures Log timer execution errors Handle edge cases (e.g., system time changes) Testing Use shorter durations in tests Mock time-dependent operations Test cancellation scenarios See Also \u00b6 Timer Node Implementation ADR Node Lifecycle Methods Event-Driven Architecture","title":"Timer Node"},{"location":"examples/timer-node/#timer-node-example","text":"This example demonstrates how to use timer nodes in the Floxide framework for scheduling and periodic execution.","title":"Timer Node Example"},{"location":"examples/timer-node/#overview","text":"Timer nodes allow you to: - Execute workflow nodes at specific times - Run periodic tasks - Implement delays and timeouts - Handle scheduled workflows","title":"Overview"},{"location":"examples/timer-node/#implementation","text":"First, let's create a simple timer-based workflow: use floxide_core ::{ DefaultAction , FloxideError , NodeId }; use floxide_timer ::{ Schedule , SimpleTimer , TimerNode , TimerWorkflow }; use chrono ::{ DateTime , Duration as ChronoDuration , Utc }; use std :: sync :: Arc ; // Define our context #[derive(Debug, Clone)] struct BackupContext { last_backup : Option < DateTime < Utc >> , backup_count : usize , status : String , } // Create different types of timer nodes fn create_backup_workflow () -> TimerWorkflow < BackupContext , DefaultAction > { // Daily backup at 2 AM let daily_backup = Arc :: new ( SimpleTimer :: with_id ( \"daily_backup\" , Schedule :: Cron ( \"0 2 * * *\" . to_string ()), | ctx : & mut BackupContext | { ctx . backup_count += 1 ; ctx . last_backup = Some ( Utc :: now ()); ctx . status = format! ( \"Daily backup completed ({})\" , ctx . backup_count ); Ok ( DefaultAction :: Next ) }, )); // Hourly health check let health_check = Arc :: new ( SimpleTimer :: with_id ( \"health_check\" , Schedule :: Periodic ( ChronoDuration :: hours ( 1 )), | ctx : & mut BackupContext | { ctx . status = \"Health check passed\" . to_string (); Ok ( DefaultAction :: Next ) }, )); // One-time initialization let init = Arc :: new ( SimpleTimer :: with_id ( \"init\" , Schedule :: Once ( Utc :: now ()), | ctx : & mut BackupContext | { ctx . status = \"Backup system initialized\" . to_string (); Ok ( DefaultAction :: Next ) }, )); // Create the workflow let mut workflow = TimerWorkflow :: new ( init , DefaultAction :: complete (), // Action to terminate the workflow ); // Add nodes and set up routes workflow . add_node ( daily_backup . clone ()); workflow . add_node ( health_check . clone ()); // Set up routing workflow . set_route ( & init . id (), DefaultAction :: Next , & daily_backup . id ()); workflow . set_route ( & daily_backup . id (), DefaultAction :: Next , & health_check . id ()); workflow . set_route ( & health_check . id (), DefaultAction :: Next , & daily_backup . id ()); workflow } // Example usage #[tokio::main] async fn main () -> Result < (), FloxideError > { let mut ctx = BackupContext { last_backup : None , backup_count : 0 , status : String :: new (), }; let workflow = create_backup_workflow (); // Execute the workflow with proper error handling match workflow . execute ( & mut ctx ). await { Ok (()) => { println! ( \"Workflow completed successfully\" ); println! ( \"Final status: {}\" , ctx . status ); println! ( \"Total backups: {}\" , ctx . backup_count ); } Err ( e ) => { eprintln! ( \"Workflow error: {}\" , e ); // Implement your error recovery strategy here } } Ok (()) }","title":"Implementation"},{"location":"examples/timer-node/#running-the-example","text":"To run the timer nodes, execute the main function.","title":"Running the Example"},{"location":"examples/timer-node/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"examples/timer-node/#combining-with-other-nodes","text":"Timer nodes can be combined with other node types: // Create a workflow that processes data periodically let mut workflow = TimerWorkflow :: new ( create_periodic_node ()) . then ( create_processor_node ()) . then ( create_cleanup_node ());","title":"Combining with Other Nodes"},{"location":"examples/timer-node/#error-handling","text":"Timer nodes include built-in error handling: fn create_robust_timer () -> impl TimerNode < BackupContext , DefaultAction > { SimpleTimer :: with_id ( \"robust_timer\" , Schedule :: Periodic ( ChronoDuration :: hours ( 1 )), | ctx : & mut BackupContext | { if let Err ( e ) = process_data (). await { ctx . status = format! ( \"Error: {}\" , e ); return Ok ( DefaultAction :: Stop ); } Ok ( DefaultAction :: Next ) }, ) . with_retry ( 3 ) // Retry up to 3 times on failure . with_backoff ( ChronoDuration :: seconds ( 1 )) // Wait 1 second between retries }","title":"Error Handling"},{"location":"examples/timer-node/#cancellation","text":"Timer nodes support graceful cancellation: fn create_cancellable_timer () -> impl TimerNode < BackupContext , DefaultAction > { SimpleTimer :: with_id ( \"cancellable_timer\" , Schedule :: Periodic ( ChronoDuration :: hours ( 1 )), | ctx : & mut BackupContext | { if ctx . backup_count >= 10 { return Ok ( DefaultAction :: Stop ); } Ok ( DefaultAction :: Next ) }, ) . with_cancellation () }","title":"Cancellation"},{"location":"examples/timer-node/#best-practices","text":"Resource Management Always implement proper cleanup for timer resources Use appropriate timeout values Consider system resource limitations Error Handling Implement retry logic for transient failures Log timer execution errors Handle edge cases (e.g., system time changes) Testing Use shorter durations in tests Mock time-dependent operations Test cancellation scenarios","title":"Best Practices"},{"location":"examples/timer-node/#see-also","text":"Timer Node Implementation ADR Node Lifecycle Methods Event-Driven Architecture","title":"See Also"},{"location":"examples/transform-node/","text":"Transform Node Example \u00b6 This example demonstrates how to use transform nodes in the Floxide framework for functional data transformations with explicit input and output types. Overview \u00b6 Transform nodes enable: - Functional programming style with explicit input/output types - Direct error types specific to the node - Three-phase transformation lifecycle (prep, exec, post) - Easy composition of transformations Implementation \u00b6 Let's create a data processing pipeline that validates, transforms, and enriches JSON data: use async_trait :: async_trait ; use floxide_core ::{ DefaultAction , FloxideError }; use floxide_transform ::{ TransformNode , TransformContext , to_lifecycle_node }; use serde_json ::{ Value as JsonValue , json }; use thiserror :: Error ; // Custom error type #[derive(Debug, Error)] enum DataTransformError { #[error( \"Validation failed: {0}\" )] ValidationError ( String ), #[error( \"Transform failed: {0}\" )] TransformError ( String ), #[error( \"JSON error: {0}\" )] JsonError ( #[from] serde_json :: Error ), } // A transform node that validates and enriches user data struct UserDataTransformer ; #[async_trait] impl TransformNode < JsonValue , JsonValue , DataTransformError > for UserDataTransformer { async fn prep ( & self , input : JsonValue ) -> Result < JsonValue , DataTransformError > { // Validate required fields if ! input . is_object () { return Err ( DataTransformError :: ValidationError ( \"Input must be an object\" . into ())); } let obj = input . as_object (). unwrap (); if ! obj . contains_key ( \"name\" ) || ! obj . contains_key ( \"email\" ) { return Err ( DataTransformError :: ValidationError ( \"Missing required fields\" . into ())); } Ok ( input ) } async fn exec ( & self , input : JsonValue ) -> Result < JsonValue , DataTransformError > { let mut obj = input . as_object (). unwrap (). clone (); // Transform name to uppercase if let Some ( name ) = obj . get ( \"name\" ) { let uppercase_name = name . as_str () . ok_or_else ( || DataTransformError :: TransformError ( \"Invalid name format\" . into ())) ? . to_uppercase (); obj . insert ( \"name\" . into (), json ! ( uppercase_name )); } // Add metadata obj . insert ( \"processed_at\" . into (), json ! ( chrono :: Utc :: now (). to_rfc3339 ())); obj . insert ( \"version\" . into (), json ! ( \"1.0.0\" )); Ok ( JsonValue :: Object ( obj )) } async fn post ( & self , output : JsonValue ) -> Result < JsonValue , DataTransformError > { // Add a summary field let mut obj = output . as_object (). unwrap (). clone (); let summary = format! ( \"Processed user data for: {}\" , obj . get ( \"name\" ). and_then ( | n | n . as_str ()). unwrap_or ( \"unknown\" ) ); obj . insert ( \"summary\" . into (), json ! ( summary )); Ok ( JsonValue :: Object ( obj )) } } // Example usage #[tokio::main] async fn main () -> Result < (), FloxideError > { // Create input data let input_data = json ! ({ \"name\" : \"John Doe\" , \"email\" : \"john@example.com\" , \"age\" : 30 }); // Create transform node and convert to lifecycle node let transformer = UserDataTransformer ; let lifecycle_node = to_lifecycle_node ( transformer ); // Create context with input data let mut ctx = TransformContext :: new ( input_data ); // Process the data through all phases let prep_result = lifecycle_node . prep ( & mut ctx ) ? ; let exec_result = lifecycle_node . exec ( prep_result ) ? ; let _action = lifecycle_node . post ( prep_result , exec_result , & mut ctx ) ? ; // Access the transformed data from the context println! ( \"Transformed data: {}\" , serde_json :: to_string_pretty ( & ctx . input ) ? ); Ok (()) } Advanced Usage \u00b6 Composing Transform Nodes \u00b6 Transform nodes can be easily composed into pipelines: // Create a pipeline of transform nodes let pipeline = to_lifecycle_node ( ValidatorNode . and_then ( TransformerNode ) . and_then ( EnricherNode ) ); Using Helper Functions \u00b6 The framework provides helper functions for simpler transformations: use floxide_transform :: transform_node ; // Create a simple transform node using closures let simple_transformer = transform_node ( // Prep function | input : String | async move { if input . is_empty () { Err ( DataTransformError :: ValidationError ( \"Empty input\" . into ())) } else { Ok ( input ) } }, // Exec function | input : String | async move { Ok ( input . to_uppercase ()) }, // Post function | output : String | async move { Ok ( format! ( \"Processed: {}\" , output )) }, ); Error Handling \u00b6 Transform nodes support custom error types for better error handling: // Create a transform node with robust error handling let robust_transformer = transform_node ( | input : JsonValue | async move { match validate_input ( & input ) { Ok ( valid_input ) => Ok ( valid_input ), Err ( e ) => Err ( DataTransformError :: ValidationError ( e . to_string ())), } }, | input : JsonValue | async move { transform_data ( & input ) . map_err ( | e | DataTransformError :: TransformError ( e . to_string ())) }, | output : JsonValue | async move { enrich_data ( & output ) . map_err ( | e | DataTransformError :: TransformError ( e . to_string ())) }, ); Best Practices \u00b6 Use Custom Error Types : Define specific error types for your transformations to provide clear error handling. Validate Early : Use the prep phase to validate input data before processing. Keep Transformations Pure : Avoid side effects in transform nodes when possible. Compose Nodes : Break complex transformations into smaller, composable nodes. Add Context : Use the post phase to add metadata about the transformation process.","title":"Transform Node"},{"location":"examples/transform-node/#transform-node-example","text":"This example demonstrates how to use transform nodes in the Floxide framework for functional data transformations with explicit input and output types.","title":"Transform Node Example"},{"location":"examples/transform-node/#overview","text":"Transform nodes enable: - Functional programming style with explicit input/output types - Direct error types specific to the node - Three-phase transformation lifecycle (prep, exec, post) - Easy composition of transformations","title":"Overview"},{"location":"examples/transform-node/#implementation","text":"Let's create a data processing pipeline that validates, transforms, and enriches JSON data: use async_trait :: async_trait ; use floxide_core ::{ DefaultAction , FloxideError }; use floxide_transform ::{ TransformNode , TransformContext , to_lifecycle_node }; use serde_json ::{ Value as JsonValue , json }; use thiserror :: Error ; // Custom error type #[derive(Debug, Error)] enum DataTransformError { #[error( \"Validation failed: {0}\" )] ValidationError ( String ), #[error( \"Transform failed: {0}\" )] TransformError ( String ), #[error( \"JSON error: {0}\" )] JsonError ( #[from] serde_json :: Error ), } // A transform node that validates and enriches user data struct UserDataTransformer ; #[async_trait] impl TransformNode < JsonValue , JsonValue , DataTransformError > for UserDataTransformer { async fn prep ( & self , input : JsonValue ) -> Result < JsonValue , DataTransformError > { // Validate required fields if ! input . is_object () { return Err ( DataTransformError :: ValidationError ( \"Input must be an object\" . into ())); } let obj = input . as_object (). unwrap (); if ! obj . contains_key ( \"name\" ) || ! obj . contains_key ( \"email\" ) { return Err ( DataTransformError :: ValidationError ( \"Missing required fields\" . into ())); } Ok ( input ) } async fn exec ( & self , input : JsonValue ) -> Result < JsonValue , DataTransformError > { let mut obj = input . as_object (). unwrap (). clone (); // Transform name to uppercase if let Some ( name ) = obj . get ( \"name\" ) { let uppercase_name = name . as_str () . ok_or_else ( || DataTransformError :: TransformError ( \"Invalid name format\" . into ())) ? . to_uppercase (); obj . insert ( \"name\" . into (), json ! ( uppercase_name )); } // Add metadata obj . insert ( \"processed_at\" . into (), json ! ( chrono :: Utc :: now (). to_rfc3339 ())); obj . insert ( \"version\" . into (), json ! ( \"1.0.0\" )); Ok ( JsonValue :: Object ( obj )) } async fn post ( & self , output : JsonValue ) -> Result < JsonValue , DataTransformError > { // Add a summary field let mut obj = output . as_object (). unwrap (). clone (); let summary = format! ( \"Processed user data for: {}\" , obj . get ( \"name\" ). and_then ( | n | n . as_str ()). unwrap_or ( \"unknown\" ) ); obj . insert ( \"summary\" . into (), json ! ( summary )); Ok ( JsonValue :: Object ( obj )) } } // Example usage #[tokio::main] async fn main () -> Result < (), FloxideError > { // Create input data let input_data = json ! ({ \"name\" : \"John Doe\" , \"email\" : \"john@example.com\" , \"age\" : 30 }); // Create transform node and convert to lifecycle node let transformer = UserDataTransformer ; let lifecycle_node = to_lifecycle_node ( transformer ); // Create context with input data let mut ctx = TransformContext :: new ( input_data ); // Process the data through all phases let prep_result = lifecycle_node . prep ( & mut ctx ) ? ; let exec_result = lifecycle_node . exec ( prep_result ) ? ; let _action = lifecycle_node . post ( prep_result , exec_result , & mut ctx ) ? ; // Access the transformed data from the context println! ( \"Transformed data: {}\" , serde_json :: to_string_pretty ( & ctx . input ) ? ); Ok (()) }","title":"Implementation"},{"location":"examples/transform-node/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"examples/transform-node/#composing-transform-nodes","text":"Transform nodes can be easily composed into pipelines: // Create a pipeline of transform nodes let pipeline = to_lifecycle_node ( ValidatorNode . and_then ( TransformerNode ) . and_then ( EnricherNode ) );","title":"Composing Transform Nodes"},{"location":"examples/transform-node/#using-helper-functions","text":"The framework provides helper functions for simpler transformations: use floxide_transform :: transform_node ; // Create a simple transform node using closures let simple_transformer = transform_node ( // Prep function | input : String | async move { if input . is_empty () { Err ( DataTransformError :: ValidationError ( \"Empty input\" . into ())) } else { Ok ( input ) } }, // Exec function | input : String | async move { Ok ( input . to_uppercase ()) }, // Post function | output : String | async move { Ok ( format! ( \"Processed: {}\" , output )) }, );","title":"Using Helper Functions"},{"location":"examples/transform-node/#error-handling","text":"Transform nodes support custom error types for better error handling: // Create a transform node with robust error handling let robust_transformer = transform_node ( | input : JsonValue | async move { match validate_input ( & input ) { Ok ( valid_input ) => Ok ( valid_input ), Err ( e ) => Err ( DataTransformError :: ValidationError ( e . to_string ())), } }, | input : JsonValue | async move { transform_data ( & input ) . map_err ( | e | DataTransformError :: TransformError ( e . to_string ())) }, | output : JsonValue | async move { enrich_data ( & output ) . map_err ( | e | DataTransformError :: TransformError ( e . to_string ())) }, );","title":"Error Handling"},{"location":"examples/transform-node/#best-practices","text":"Use Custom Error Types : Define specific error types for your transformations to provide clear error handling. Validate Early : Use the prep phase to validate input data before processing. Keep Transformations Pure : Avoid side effects in transform nodes when possible. Compose Nodes : Break complex transformations into smaller, composable nodes. Add Context : Use the post phase to add metadata about the transformation process.","title":"Best Practices"},{"location":"getting-started/error-handling/","text":"Error Handling in Floxide \u00b6 This guide explains how to handle errors effectively in the Floxide framework. Overview \u00b6 Floxide provides a comprehensive error handling system that allows you to: - Define custom error types for your nodes - Handle errors at different lifecycle phases - Implement recovery strategies - Maintain type safety Error Types \u00b6 FloxideError \u00b6 The core error type in Floxide is FloxideError : pub enum FloxideError { NodeError ( String ), WorkflowError ( String ), StateError ( String ), Other ( String ), } Custom Error Types \u00b6 You can define custom error types for your nodes: use thiserror :: Error ; #[derive(Debug, Error)] pub enum MyNodeError { #[error( \"Validation failed: {0}\" )] ValidationError ( String ), #[error( \"Processing failed: {0}\" )] ProcessingError ( String ), } impl From < MyNodeError > for FloxideError { fn from ( err : MyNodeError ) -> Self { FloxideError :: NodeError ( err . to_string ()) } } Error Handling in Nodes \u00b6 Lifecycle Node Error Handling \u00b6 #[async_trait] impl LifecycleNode < MyContext , DefaultAction > for MyNode { async fn prep ( & self , ctx : & mut MyContext ) -> Result < Self :: PrepOutput , FloxideError > { match validate_input ( ctx ) { Ok ( input ) => Ok ( input ), Err ( e ) => Err ( MyNodeError :: ValidationError ( e . to_string ()). into ()), } } async fn exec ( & self , input : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > { process_data ( input ). map_err ( | e | MyNodeError :: ProcessingError ( e . to_string ()). into ()) } async fn post ( & self , output : Self :: ExecOutput ) -> Result < DefaultAction , FloxideError > { Ok ( DefaultAction :: Next ) } } Transform Node Error Handling \u00b6 #[async_trait] impl TransformNode < Input , Output , MyNodeError > for MyTransformNode { async fn prep ( & self , input : Input ) -> Result < Input , MyNodeError > { if ! is_valid ( & input ) { return Err ( MyNodeError :: ValidationError ( \"Invalid input\" . into ())); } Ok ( input ) } async fn exec ( & self , input : Input ) -> Result < Output , MyNodeError > { transform_data ( input ) . map_err ( | e | MyNodeError :: ProcessingError ( e . to_string ())) } async fn post ( & self , output : Output ) -> Result < Output , MyNodeError > { Ok ( output ) } } Error Recovery Strategies \u00b6 Retry Logic \u00b6 impl MyNode { async fn with_retry < T , F > ( & self , f : F ) -> Result < T , FloxideError > where F : Fn () -> Future < Output = Result < T , FloxideError >> , { let mut attempts = 0 ; while attempts < 3 { match f (). await { Ok ( result ) => return Ok ( result ), Err ( e ) => { attempts += 1 ; if attempts == 3 { return Err ( e ); } tokio :: time :: sleep ( Duration :: from_secs ( 1 )). await ; } } } unreachable! () } } Fallback Values \u00b6 impl MyNode { async fn with_fallback < T > ( & self , f : impl Fn () -> T ) -> Result < T , FloxideError > { match self . process_data (). await { Ok ( result ) => Ok ( result ), Err ( _ ) => Ok ( f ()), } } } Error Propagation \u00b6 In Workflows \u00b6 let workflow = Workflow :: new ( node1 ) . then ( node2 ) . on_error ( | e | { eprintln! ( \"Workflow error: {}\" , e ); DefaultAction :: Stop }); With Context \u00b6 impl MyContext { fn record_error ( & mut self , error : & FloxideError ) { self . errors . push ( ErrorRecord { timestamp : Utc :: now (), message : error . to_string (), }); } } Best Practices \u00b6 Custom Error Types Define specific error types for your nodes Use thiserror for error definitions Implement From for FloxideError Error Context Include relevant context in errors Use structured error types Maintain error chains Recovery Strategies Implement appropriate retry logic Use fallback values when suitable Clean up resources on error Testing Test error conditions Verify error recovery Check error propagation Related Topics \u00b6 Node Lifecycle Methods Transform Node Implementation ADR-0005: State Serialization/Deserialization","title":"Error Handling"},{"location":"getting-started/error-handling/#error-handling-in-floxide","text":"This guide explains how to handle errors effectively in the Floxide framework.","title":"Error Handling in Floxide"},{"location":"getting-started/error-handling/#overview","text":"Floxide provides a comprehensive error handling system that allows you to: - Define custom error types for your nodes - Handle errors at different lifecycle phases - Implement recovery strategies - Maintain type safety","title":"Overview"},{"location":"getting-started/error-handling/#error-types","text":"","title":"Error Types"},{"location":"getting-started/error-handling/#floxideerror","text":"The core error type in Floxide is FloxideError : pub enum FloxideError { NodeError ( String ), WorkflowError ( String ), StateError ( String ), Other ( String ), }","title":"FloxideError"},{"location":"getting-started/error-handling/#custom-error-types","text":"You can define custom error types for your nodes: use thiserror :: Error ; #[derive(Debug, Error)] pub enum MyNodeError { #[error( \"Validation failed: {0}\" )] ValidationError ( String ), #[error( \"Processing failed: {0}\" )] ProcessingError ( String ), } impl From < MyNodeError > for FloxideError { fn from ( err : MyNodeError ) -> Self { FloxideError :: NodeError ( err . to_string ()) } }","title":"Custom Error Types"},{"location":"getting-started/error-handling/#error-handling-in-nodes","text":"","title":"Error Handling in Nodes"},{"location":"getting-started/error-handling/#lifecycle-node-error-handling","text":"#[async_trait] impl LifecycleNode < MyContext , DefaultAction > for MyNode { async fn prep ( & self , ctx : & mut MyContext ) -> Result < Self :: PrepOutput , FloxideError > { match validate_input ( ctx ) { Ok ( input ) => Ok ( input ), Err ( e ) => Err ( MyNodeError :: ValidationError ( e . to_string ()). into ()), } } async fn exec ( & self , input : Self :: PrepOutput ) -> Result < Self :: ExecOutput , FloxideError > { process_data ( input ). map_err ( | e | MyNodeError :: ProcessingError ( e . to_string ()). into ()) } async fn post ( & self , output : Self :: ExecOutput ) -> Result < DefaultAction , FloxideError > { Ok ( DefaultAction :: Next ) } }","title":"Lifecycle Node Error Handling"},{"location":"getting-started/error-handling/#transform-node-error-handling","text":"#[async_trait] impl TransformNode < Input , Output , MyNodeError > for MyTransformNode { async fn prep ( & self , input : Input ) -> Result < Input , MyNodeError > { if ! is_valid ( & input ) { return Err ( MyNodeError :: ValidationError ( \"Invalid input\" . into ())); } Ok ( input ) } async fn exec ( & self , input : Input ) -> Result < Output , MyNodeError > { transform_data ( input ) . map_err ( | e | MyNodeError :: ProcessingError ( e . to_string ())) } async fn post ( & self , output : Output ) -> Result < Output , MyNodeError > { Ok ( output ) } }","title":"Transform Node Error Handling"},{"location":"getting-started/error-handling/#error-recovery-strategies","text":"","title":"Error Recovery Strategies"},{"location":"getting-started/error-handling/#retry-logic","text":"impl MyNode { async fn with_retry < T , F > ( & self , f : F ) -> Result < T , FloxideError > where F : Fn () -> Future < Output = Result < T , FloxideError >> , { let mut attempts = 0 ; while attempts < 3 { match f (). await { Ok ( result ) => return Ok ( result ), Err ( e ) => { attempts += 1 ; if attempts == 3 { return Err ( e ); } tokio :: time :: sleep ( Duration :: from_secs ( 1 )). await ; } } } unreachable! () } }","title":"Retry Logic"},{"location":"getting-started/error-handling/#fallback-values","text":"impl MyNode { async fn with_fallback < T > ( & self , f : impl Fn () -> T ) -> Result < T , FloxideError > { match self . process_data (). await { Ok ( result ) => Ok ( result ), Err ( _ ) => Ok ( f ()), } } }","title":"Fallback Values"},{"location":"getting-started/error-handling/#error-propagation","text":"","title":"Error Propagation"},{"location":"getting-started/error-handling/#in-workflows","text":"let workflow = Workflow :: new ( node1 ) . then ( node2 ) . on_error ( | e | { eprintln! ( \"Workflow error: {}\" , e ); DefaultAction :: Stop });","title":"In Workflows"},{"location":"getting-started/error-handling/#with-context","text":"impl MyContext { fn record_error ( & mut self , error : & FloxideError ) { self . errors . push ( ErrorRecord { timestamp : Utc :: now (), message : error . to_string (), }); } }","title":"With Context"},{"location":"getting-started/error-handling/#best-practices","text":"Custom Error Types Define specific error types for your nodes Use thiserror for error definitions Implement From for FloxideError Error Context Include relevant context in errors Use structured error types Maintain error chains Recovery Strategies Implement appropriate retry logic Use fallback values when suitable Clean up resources on error Testing Test error conditions Verify error recovery Check error propagation","title":"Best Practices"},{"location":"getting-started/error-handling/#related-topics","text":"Node Lifecycle Methods Transform Node Implementation ADR-0005: State Serialization/Deserialization","title":"Related Topics"},{"location":"getting-started/installation/","text":"Installation \u00b6 Getting started with Floxide is straightforward. This guide will walk you through the installation process and help you set up your first Floxide project. Prerequisites \u00b6 Before installing Floxide, ensure you have the following prerequisites: Rust and Cargo : Floxide is a Rust library, so you'll need Rust and Cargo installed. If you don't have them installed, you can get them from rustup.rs . Rust version : Floxide requires Rust 1.65 or later due to its use of async traits. Adding Floxide to Your Project \u00b6 Creating a New Project \u00b6 If you're starting a new project, create a new Rust project using Cargo: cargo new my_floxide_project cd my_floxide_project Adding Dependencies \u00b6 Add Floxide to your project by adding the following to your Cargo.toml file: [dependencies] floxide-core = \"0.1.0\" tokio = { version = \"1.28\" , features = [ \"full\" ] } async-trait = \"0.1.68\" floxide-core : The core library containing the fundamental abstractions and workflow engine. tokio : The async runtime used by Floxide. async-trait : Required for using async functions in traits. Optional Crates \u00b6 Depending on your needs, you might want to add additional Floxide crates: [dependencies] # For transform-based workflows floxide-transform = \"0.1.0\" # For batch processing floxide-batch = \"0.1.0\" # For event-driven workflows floxide-event = \"0.1.0\" # For time-based workflows floxide-timer = \"0.1.0\" # For reactive workflows floxide-reactive = \"0.1.0\" # For long-running workflows floxide-longrunning = \"0.1.0\" Verifying Installation \u00b6 To verify that Floxide is correctly installed, create a simple program that uses Floxide: use floxide_core ::{ lifecycle_node , LifecycleNode , Workflow , DefaultAction }; use std :: sync :: Arc ; #[derive(Debug, Clone)] struct SimpleContext { message : String , } fn main () { println! ( \"Floxide is installed correctly!\" ); } Build your project to ensure all dependencies are resolved: cargo build If the build succeeds, you've successfully installed Floxide! Next Steps \u00b6 Now that you have Floxide installed, you can: Continue to the Quick Start Guide to create your first workflow Explore the Core Concepts to understand the fundamental abstractions Check out the Examples to see Floxide in action","title":"Installation"},{"location":"getting-started/installation/#installation","text":"Getting started with Floxide is straightforward. This guide will walk you through the installation process and help you set up your first Floxide project.","title":"Installation"},{"location":"getting-started/installation/#prerequisites","text":"Before installing Floxide, ensure you have the following prerequisites: Rust and Cargo : Floxide is a Rust library, so you'll need Rust and Cargo installed. If you don't have them installed, you can get them from rustup.rs . Rust version : Floxide requires Rust 1.65 or later due to its use of async traits.","title":"Prerequisites"},{"location":"getting-started/installation/#adding-floxide-to-your-project","text":"","title":"Adding Floxide to Your Project"},{"location":"getting-started/installation/#creating-a-new-project","text":"If you're starting a new project, create a new Rust project using Cargo: cargo new my_floxide_project cd my_floxide_project","title":"Creating a New Project"},{"location":"getting-started/installation/#adding-dependencies","text":"Add Floxide to your project by adding the following to your Cargo.toml file: [dependencies] floxide-core = \"0.1.0\" tokio = { version = \"1.28\" , features = [ \"full\" ] } async-trait = \"0.1.68\" floxide-core : The core library containing the fundamental abstractions and workflow engine. tokio : The async runtime used by Floxide. async-trait : Required for using async functions in traits.","title":"Adding Dependencies"},{"location":"getting-started/installation/#optional-crates","text":"Depending on your needs, you might want to add additional Floxide crates: [dependencies] # For transform-based workflows floxide-transform = \"0.1.0\" # For batch processing floxide-batch = \"0.1.0\" # For event-driven workflows floxide-event = \"0.1.0\" # For time-based workflows floxide-timer = \"0.1.0\" # For reactive workflows floxide-reactive = \"0.1.0\" # For long-running workflows floxide-longrunning = \"0.1.0\"","title":"Optional Crates"},{"location":"getting-started/installation/#verifying-installation","text":"To verify that Floxide is correctly installed, create a simple program that uses Floxide: use floxide_core ::{ lifecycle_node , LifecycleNode , Workflow , DefaultAction }; use std :: sync :: Arc ; #[derive(Debug, Clone)] struct SimpleContext { message : String , } fn main () { println! ( \"Floxide is installed correctly!\" ); } Build your project to ensure all dependencies are resolved: cargo build If the build succeeds, you've successfully installed Floxide!","title":"Verifying Installation"},{"location":"getting-started/installation/#next-steps","text":"Now that you have Floxide installed, you can: Continue to the Quick Start Guide to create your first workflow Explore the Core Concepts to understand the fundamental abstractions Check out the Examples to see Floxide in action","title":"Next Steps"},{"location":"getting-started/quick-start/","text":"Quick Start Guide \u00b6 This guide will help you create your first Floxide workflow. We'll build a simple workflow that processes a message and returns a result. Prerequisites \u00b6 Before starting, make sure you have: Installed Rust and Cargo (see Installation ) Created a new Rust project or added Floxide to an existing project Step 1: Set Up Your Project \u00b6 First, let's set up a new Rust project and add the necessary dependencies: cargo new floxide_quickstart cd floxide_quickstart Edit your Cargo.toml file to include the required dependencies: [dependencies] floxide-core = \"0.1.0\" tokio = { version = \"1.28\" , features = [ \"full\" ] } async-trait = \"0.1.68\" Step 2: Create Your First Workflow \u00b6 Now, let's create a simple workflow that processes a message. Replace the contents of src/main.rs with the following code: use floxide_core ::{ lifecycle_node , LifecycleNode , Workflow , DefaultAction }; use async_trait :: async_trait ; use std :: sync :: Arc ; // Define your context type #[derive(Debug, Clone)] struct MessageContext { input : String , result : Option < String > , } // Create a node using the convenience function fn create_processor_node () -> impl LifecycleNode < MessageContext , DefaultAction > { lifecycle_node ( Some ( \"processor\" ), // Node ID | ctx : & mut MessageContext | async move { // Preparation phase let input = ctx . input . clone (); Ok ( input ) }, | input : String | async move { // Execution phase let processed = format! ( \"Processed: {}\" , input . to_uppercase ()); Ok ( processed ) }, | ctx : & mut MessageContext , result : String | async move { // Post-processing phase ctx . result = Some ( result ); Ok ( DefaultAction :: Next ) }, ) } #[tokio::main] async fn main () { // Create the initial context let context = MessageContext { input : \"hello world\" . to_string (), result : None , }; // Create and run the workflow let node = create_processor_node (); let mut workflow = Workflow :: new ( node ); let result = workflow . run ( context ). await . unwrap (); println! ( \"Result: {:?}\" , result . result ); } Step 3: Add Error Handling \u00b6 Let's enhance our workflow with error handling: use std :: error :: Error ; #[derive(Debug)] struct ProcessingError ( String ); impl std :: fmt :: Display for ProcessingError { fn fmt ( & self , f : & mut std :: fmt :: Formatter <' _ > ) -> std :: fmt :: Result { write! ( f , \"Processing error: {}\" , self . 0 ) } } impl Error for ProcessingError {} fn create_error_handling_node () -> impl LifecycleNode < MessageContext , DefaultAction > { lifecycle_node ( Some ( \"error_handler\" ), | ctx : & MessageContext | async move { let input = ctx . input . clone (); if input . is_empty () { return Err ( Box :: new ( ProcessingError ( \"Empty input\" . to_string ()))); } Ok ( input ) }, | input : String | async move { Ok ( input . to_uppercase ()) }, | ctx : & mut MessageContext , result : String | async move { ctx . result = Some ( result ); Ok ( DefaultAction :: Next ) }, ) } Step 4: Create a Multi-Node Workflow \u00b6 Now let's create a more complex workflow with multiple nodes: #[derive(Debug, Clone)] enum CustomAction { Success , Error , } // Validator node fn create_validator_node () -> impl LifecycleNode < MessageContext , CustomAction > { lifecycle_node ( Some ( \"validator\" ), | ctx : & MessageContext | async move { Ok ( ctx . input . clone ()) }, | input : String | async move { if input . len () < 3 { Ok ( CustomAction :: Error ) } else { Ok ( CustomAction :: Success ) } }, | ctx : & mut MessageContext , action : CustomAction | async move { Ok ( action ) }, ) } // Success handler node fn create_success_node () -> impl LifecycleNode < MessageContext , DefaultAction > { lifecycle_node ( Some ( \"success_handler\" ), | ctx : & MessageContext | async move { Ok ( ctx . input . clone ()) }, | input : String | async move { Ok ( format! ( \"SUCCESS: {}\" , input . to_uppercase ())) }, | ctx : & mut MessageContext , result : String | async move { ctx . result = Some ( result ); Ok ( DefaultAction :: Next ) }, ) } // Error handler node fn create_error_node () -> impl LifecycleNode < MessageContext , DefaultAction > { lifecycle_node ( Some ( \"error_handler\" ), | ctx : & MessageContext | async move { Ok ( ctx . input . clone ()) }, | input : String | async move { Ok ( format! ( \"ERROR: Input '{}' is too short\" , input )) }, | ctx : & mut MessageContext , result : String | async move { ctx . result = Some ( result ); Ok ( DefaultAction :: Next ) }, ) } #[tokio::main] async fn main () { // Create nodes let validator = create_validator_node (); let success_handler = create_success_node (); let error_handler = create_error_node (); // Create workflow with conditional branching let mut workflow = Workflow :: new ( validator ) . on ( CustomAction :: Success , success_handler ) . on ( CustomAction :: Error , error_handler ); // Test with valid input let context = MessageContext { input : \"hello world\" . to_string (), result : None , }; let result = workflow . run ( context ). await . unwrap (); println! ( \"Valid input result: {:?}\" , result . result ); // Test with invalid input let context = MessageContext { input : \"hi\" . to_string (), result : None , }; let result = workflow . run ( context ). await . unwrap (); println! ( \"Invalid input result: {:?}\" , result . result ); } Next Steps \u00b6 Now that you've created your first workflow, you can: Learn about Core Concepts in depth Explore Event-Driven Architecture Check out more Examples Read about Actions and flow control Common Patterns \u00b6 Here are some common patterns you might want to try: Parallel Processing : Use multiple nodes that process data concurrently Aggregate results in a final node Error Recovery : Implement retry logic Use fallback nodes for error cases State Management : Share state between nodes Persist workflow state Monitoring : Add logging nodes Track workflow metrics For more detailed examples and patterns, check out the Examples section.","title":"Quick Start"},{"location":"getting-started/quick-start/#quick-start-guide","text":"This guide will help you create your first Floxide workflow. We'll build a simple workflow that processes a message and returns a result.","title":"Quick Start Guide"},{"location":"getting-started/quick-start/#prerequisites","text":"Before starting, make sure you have: Installed Rust and Cargo (see Installation ) Created a new Rust project or added Floxide to an existing project","title":"Prerequisites"},{"location":"getting-started/quick-start/#step-1-set-up-your-project","text":"First, let's set up a new Rust project and add the necessary dependencies: cargo new floxide_quickstart cd floxide_quickstart Edit your Cargo.toml file to include the required dependencies: [dependencies] floxide-core = \"0.1.0\" tokio = { version = \"1.28\" , features = [ \"full\" ] } async-trait = \"0.1.68\"","title":"Step 1: Set Up Your Project"},{"location":"getting-started/quick-start/#step-2-create-your-first-workflow","text":"Now, let's create a simple workflow that processes a message. Replace the contents of src/main.rs with the following code: use floxide_core ::{ lifecycle_node , LifecycleNode , Workflow , DefaultAction }; use async_trait :: async_trait ; use std :: sync :: Arc ; // Define your context type #[derive(Debug, Clone)] struct MessageContext { input : String , result : Option < String > , } // Create a node using the convenience function fn create_processor_node () -> impl LifecycleNode < MessageContext , DefaultAction > { lifecycle_node ( Some ( \"processor\" ), // Node ID | ctx : & mut MessageContext | async move { // Preparation phase let input = ctx . input . clone (); Ok ( input ) }, | input : String | async move { // Execution phase let processed = format! ( \"Processed: {}\" , input . to_uppercase ()); Ok ( processed ) }, | ctx : & mut MessageContext , result : String | async move { // Post-processing phase ctx . result = Some ( result ); Ok ( DefaultAction :: Next ) }, ) } #[tokio::main] async fn main () { // Create the initial context let context = MessageContext { input : \"hello world\" . to_string (), result : None , }; // Create and run the workflow let node = create_processor_node (); let mut workflow = Workflow :: new ( node ); let result = workflow . run ( context ). await . unwrap (); println! ( \"Result: {:?}\" , result . result ); }","title":"Step 2: Create Your First Workflow"},{"location":"getting-started/quick-start/#step-3-add-error-handling","text":"Let's enhance our workflow with error handling: use std :: error :: Error ; #[derive(Debug)] struct ProcessingError ( String ); impl std :: fmt :: Display for ProcessingError { fn fmt ( & self , f : & mut std :: fmt :: Formatter <' _ > ) -> std :: fmt :: Result { write! ( f , \"Processing error: {}\" , self . 0 ) } } impl Error for ProcessingError {} fn create_error_handling_node () -> impl LifecycleNode < MessageContext , DefaultAction > { lifecycle_node ( Some ( \"error_handler\" ), | ctx : & MessageContext | async move { let input = ctx . input . clone (); if input . is_empty () { return Err ( Box :: new ( ProcessingError ( \"Empty input\" . to_string ()))); } Ok ( input ) }, | input : String | async move { Ok ( input . to_uppercase ()) }, | ctx : & mut MessageContext , result : String | async move { ctx . result = Some ( result ); Ok ( DefaultAction :: Next ) }, ) }","title":"Step 3: Add Error Handling"},{"location":"getting-started/quick-start/#step-4-create-a-multi-node-workflow","text":"Now let's create a more complex workflow with multiple nodes: #[derive(Debug, Clone)] enum CustomAction { Success , Error , } // Validator node fn create_validator_node () -> impl LifecycleNode < MessageContext , CustomAction > { lifecycle_node ( Some ( \"validator\" ), | ctx : & MessageContext | async move { Ok ( ctx . input . clone ()) }, | input : String | async move { if input . len () < 3 { Ok ( CustomAction :: Error ) } else { Ok ( CustomAction :: Success ) } }, | ctx : & mut MessageContext , action : CustomAction | async move { Ok ( action ) }, ) } // Success handler node fn create_success_node () -> impl LifecycleNode < MessageContext , DefaultAction > { lifecycle_node ( Some ( \"success_handler\" ), | ctx : & MessageContext | async move { Ok ( ctx . input . clone ()) }, | input : String | async move { Ok ( format! ( \"SUCCESS: {}\" , input . to_uppercase ())) }, | ctx : & mut MessageContext , result : String | async move { ctx . result = Some ( result ); Ok ( DefaultAction :: Next ) }, ) } // Error handler node fn create_error_node () -> impl LifecycleNode < MessageContext , DefaultAction > { lifecycle_node ( Some ( \"error_handler\" ), | ctx : & MessageContext | async move { Ok ( ctx . input . clone ()) }, | input : String | async move { Ok ( format! ( \"ERROR: Input '{}' is too short\" , input )) }, | ctx : & mut MessageContext , result : String | async move { ctx . result = Some ( result ); Ok ( DefaultAction :: Next ) }, ) } #[tokio::main] async fn main () { // Create nodes let validator = create_validator_node (); let success_handler = create_success_node (); let error_handler = create_error_node (); // Create workflow with conditional branching let mut workflow = Workflow :: new ( validator ) . on ( CustomAction :: Success , success_handler ) . on ( CustomAction :: Error , error_handler ); // Test with valid input let context = MessageContext { input : \"hello world\" . to_string (), result : None , }; let result = workflow . run ( context ). await . unwrap (); println! ( \"Valid input result: {:?}\" , result . result ); // Test with invalid input let context = MessageContext { input : \"hi\" . to_string (), result : None , }; let result = workflow . run ( context ). await . unwrap (); println! ( \"Invalid input result: {:?}\" , result . result ); }","title":"Step 4: Create a Multi-Node Workflow"},{"location":"getting-started/quick-start/#next-steps","text":"Now that you've created your first workflow, you can: Learn about Core Concepts in depth Explore Event-Driven Architecture Check out more Examples Read about Actions and flow control","title":"Next Steps"},{"location":"getting-started/quick-start/#common-patterns","text":"Here are some common patterns you might want to try: Parallel Processing : Use multiple nodes that process data concurrently Aggregate results in a final node Error Recovery : Implement retry logic Use fallback nodes for error cases State Management : Share state between nodes Persist workflow state Monitoring : Add logging nodes Track workflow metrics For more detailed examples and patterns, check out the Examples section.","title":"Common Patterns"},{"location":"guides/event_driven_architecture/","text":"Event-Driven Workflow Architecture \u00b6 This guide explains the event-driven workflow architecture in the Flow Framework, as implemented in the temperature monitoring example. Overview \u00b6 The event-driven workflow pattern is designed to handle asynchronous, unpredictable events and process them through a directed graph of nodes. Unlike traditional request-response workflows, event-driven workflows continue processing indefinitely until a termination condition is met. Architecture Diagram \u00b6 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 Event Sources \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Event-Driven \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Handler Workflows \u2502 \u2502 (Sensors) \u2502 \u2502 Workflow \u2502 \u2502 (Actions) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u2502 \u2502 Context \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502 Feedback \u2502 \u2502 Management \u2502 \u2502 Loop \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Temperature Monitoring Example \u00b6 In the temperature monitoring example, the architecture is implemented as follows: 1. Event Sources \u00b6 Multiple temperature sensors (simulated) Send events to a channel-based event source Events are asynchronous and unpredictable // Create the event source with a buffer capacity of 100 events let ( source , sender ) = ChannelEventSource :: new ( 100 ); 2. Event-Driven Workflow \u00b6 Receives events from the event source Processes events through a chain of nodes Maintains state between events using context Handles errors and retries gracefully let workflow = EventDrivenWorkflow :: new () . source ( source ) . node ( \"process_temperature\" , process_temperature ) . node ( \"check_threshold\" , check_threshold ) . node ( \"alert\" , alert_handler ); 3. Event Classification \u00b6 Analyzes temperature events Categorizes them based on thresholds Returns appropriate actions // The classifier processes events and returns actions let classifier = TemperatureClassifier :: new ( 30.0 , 10.0 , 40.0 ); 4. Handler Workflows \u00b6 Execute specific actions based on event processing Can spawn additional workflows if needed Report results back to the main workflow // Add nodes for different temperature classifications let normal_handler = NormalTempHandler :: new (); let high_handler = HighTempHandler :: new (); let low_handler = LowTempHandler :: new (); let critical_handler = CriticalTempHandler :: new (); 5. Context Management \u00b6 The context in event-driven workflows needs to handle: Event history and aggregation State persistence between events Configuration and thresholds Error tracking and recovery state // The context maintains state across the workflow struct MonitoringContext { temperature_history : HashMap < String , Vec < f32 >> , alerts : Vec < String > , average_temperatures : HashMap < String , f32 > , } 6. Feedback Loop \u00b6 The feedback loop enables: Dynamic threshold adjustments Learning from historical data Adaptive event processing System health monitoring Flow of Events \u00b6 Event Generation : Sensors generate temperature readings Event Transmission : Readings are sent to the event-driven workflow Event Classification : The classifier node categorizes the temperature Action Routing : Based on the classification, the workflow routes to the appropriate handler Action Execution : The handler performs the necessary actions State Update : The context is updated with new information Continuation/Termination : The workflow continues or terminates based on conditions Implementation Considerations \u00b6 When implementing an event-driven workflow, consider the following: Event Source Design : How events are generated and retrieved Event Classification Logic : Rules for categorizing events Action Types : The set of possible actions that can result from events Routing Logic : How events are routed through the workflow Termination Conditions : When and how the workflow should terminate Timeout Handling : How to handle hanging or slow event sources Context Management : What state needs to be maintained across events Best Practices \u00b6 Error Handling Implement proper error recovery mechanisms Use retries with exponential backoff Log errors for debugging and monitoring State Management Keep state minimal and focused Consider persistence for critical state Use atomic operations when updating state Performance Optimization Buffer events appropriately Use async processing where beneficial Implement backpressure mechanisms Testing Test event sources independently Mock events for workflow testing Verify error handling paths Test state persistence and recovery Monitoring Track event processing latency Monitor queue depths Set up alerting for anomalies Log important state transitions Timeout Handling Set appropriate timeouts for event processing Implement deadletter queues Handle slow event sources gracefully Clean up resources on timeout Context Management Define clear boundaries for context data Implement proper serialization Handle context versioning Clean up stale context data Integration with Standard Workflows \u00b6 Event-driven workflows can be integrated with standard request-response workflows: Use event sources as triggers for standard workflows Convert workflow results into events Implement hybrid patterns for complex use cases Event-driven workflows can be integrated with standard workflows using adapters: // Create a nested event-driven workflow adapter let nested_workflow = NestedEventDrivenWorkflow :: new ( workflow . clone (), TempAction :: Complete , TempAction :: Timeout , ); This allows for combining both synchronous and asynchronous processing patterns. References \u00b6 ADR-0009: Event-Driven Workflow Pattern View the complete example","title":"Event-Driven Architecture"},{"location":"guides/event_driven_architecture/#event-driven-workflow-architecture","text":"This guide explains the event-driven workflow architecture in the Flow Framework, as implemented in the temperature monitoring example.","title":"Event-Driven Workflow Architecture"},{"location":"guides/event_driven_architecture/#overview","text":"The event-driven workflow pattern is designed to handle asynchronous, unpredictable events and process them through a directed graph of nodes. Unlike traditional request-response workflows, event-driven workflows continue processing indefinitely until a termination condition is met.","title":"Overview"},{"location":"guides/event_driven_architecture/#architecture-diagram","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 Event Sources \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Event-Driven \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Handler Workflows \u2502 \u2502 (Sensors) \u2502 \u2502 Workflow \u2502 \u2502 (Actions) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u2502 \u2502 Context \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502 Feedback \u2502 \u2502 Management \u2502 \u2502 Loop \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Architecture Diagram"},{"location":"guides/event_driven_architecture/#temperature-monitoring-example","text":"In the temperature monitoring example, the architecture is implemented as follows:","title":"Temperature Monitoring Example"},{"location":"guides/event_driven_architecture/#1-event-sources","text":"Multiple temperature sensors (simulated) Send events to a channel-based event source Events are asynchronous and unpredictable // Create the event source with a buffer capacity of 100 events let ( source , sender ) = ChannelEventSource :: new ( 100 );","title":"1. Event Sources"},{"location":"guides/event_driven_architecture/#2-event-driven-workflow","text":"Receives events from the event source Processes events through a chain of nodes Maintains state between events using context Handles errors and retries gracefully let workflow = EventDrivenWorkflow :: new () . source ( source ) . node ( \"process_temperature\" , process_temperature ) . node ( \"check_threshold\" , check_threshold ) . node ( \"alert\" , alert_handler );","title":"2. Event-Driven Workflow"},{"location":"guides/event_driven_architecture/#3-event-classification","text":"Analyzes temperature events Categorizes them based on thresholds Returns appropriate actions // The classifier processes events and returns actions let classifier = TemperatureClassifier :: new ( 30.0 , 10.0 , 40.0 );","title":"3. Event Classification"},{"location":"guides/event_driven_architecture/#4-handler-workflows","text":"Execute specific actions based on event processing Can spawn additional workflows if needed Report results back to the main workflow // Add nodes for different temperature classifications let normal_handler = NormalTempHandler :: new (); let high_handler = HighTempHandler :: new (); let low_handler = LowTempHandler :: new (); let critical_handler = CriticalTempHandler :: new ();","title":"4. Handler Workflows"},{"location":"guides/event_driven_architecture/#5-context-management","text":"The context in event-driven workflows needs to handle: Event history and aggregation State persistence between events Configuration and thresholds Error tracking and recovery state // The context maintains state across the workflow struct MonitoringContext { temperature_history : HashMap < String , Vec < f32 >> , alerts : Vec < String > , average_temperatures : HashMap < String , f32 > , }","title":"5. Context Management"},{"location":"guides/event_driven_architecture/#6-feedback-loop","text":"The feedback loop enables: Dynamic threshold adjustments Learning from historical data Adaptive event processing System health monitoring","title":"6. Feedback Loop"},{"location":"guides/event_driven_architecture/#flow-of-events","text":"Event Generation : Sensors generate temperature readings Event Transmission : Readings are sent to the event-driven workflow Event Classification : The classifier node categorizes the temperature Action Routing : Based on the classification, the workflow routes to the appropriate handler Action Execution : The handler performs the necessary actions State Update : The context is updated with new information Continuation/Termination : The workflow continues or terminates based on conditions","title":"Flow of Events"},{"location":"guides/event_driven_architecture/#implementation-considerations","text":"When implementing an event-driven workflow, consider the following: Event Source Design : How events are generated and retrieved Event Classification Logic : Rules for categorizing events Action Types : The set of possible actions that can result from events Routing Logic : How events are routed through the workflow Termination Conditions : When and how the workflow should terminate Timeout Handling : How to handle hanging or slow event sources Context Management : What state needs to be maintained across events","title":"Implementation Considerations"},{"location":"guides/event_driven_architecture/#best-practices","text":"Error Handling Implement proper error recovery mechanisms Use retries with exponential backoff Log errors for debugging and monitoring State Management Keep state minimal and focused Consider persistence for critical state Use atomic operations when updating state Performance Optimization Buffer events appropriately Use async processing where beneficial Implement backpressure mechanisms Testing Test event sources independently Mock events for workflow testing Verify error handling paths Test state persistence and recovery Monitoring Track event processing latency Monitor queue depths Set up alerting for anomalies Log important state transitions Timeout Handling Set appropriate timeouts for event processing Implement deadletter queues Handle slow event sources gracefully Clean up resources on timeout Context Management Define clear boundaries for context data Implement proper serialization Handle context versioning Clean up stale context data","title":"Best Practices"},{"location":"guides/event_driven_architecture/#integration-with-standard-workflows","text":"Event-driven workflows can be integrated with standard request-response workflows: Use event sources as triggers for standard workflows Convert workflow results into events Implement hybrid patterns for complex use cases Event-driven workflows can be integrated with standard workflows using adapters: // Create a nested event-driven workflow adapter let nested_workflow = NestedEventDrivenWorkflow :: new ( workflow . clone (), TempAction :: Complete , TempAction :: Timeout , ); This allows for combining both synchronous and asynchronous processing patterns.","title":"Integration with Standard Workflows"},{"location":"guides/event_driven_architecture/#references","text":"ADR-0009: Event-Driven Workflow Pattern View the complete example","title":"References"},{"location":"guides/using-mermaid-diagrams/","text":"Using Mermaid Diagrams \u00b6 This guide explains how to use Mermaid diagrams in the Floxide documentation. Overview \u00b6 Mermaid is a JavaScript-based diagramming tool that lets you create diagrams using text and code. It's particularly useful for visualizing: Flow charts Sequence diagrams Class diagrams State diagrams Example Node Implementation \u00b6 Here's an example of a transform node implementation in Rust: use async_trait :: async_trait ; use floxide_core :: error :: FloxideError ; #[derive(Debug)] pub struct TransformNode < T > { id : String , transform_fn : Box < dyn Fn ( T ) -> T + Send + Sync > , } #[async_trait] impl < T : Send + Sync + ' static > Node < T , T > for TransformNode < T > { async fn execute ( & self , input : T ) -> Result < T , FloxideError > { // Apply the transformation function let output = ( self . transform_fn )( input ); Ok ( output ) } fn id ( & self ) -> String { self . id . clone () } } Basic Mermaid Usage \u00b6 Here's a simple flow diagram showing the transform node's execution: graph LR Input[Input] --> Transform[Transform Node] Transform --> Output[Output] Node Types \u00b6 Here's a diagram showing different node types in Floxide: graph TD Node[Node] --> Transform[Transform Node] Node --> Timer[Timer Node] Node --> Reactive[Reactive Node] Node --> LongRunning[Long Running Node] Node Lifecycle \u00b6 A state diagram showing node lifecycle: stateDiagram-v2 [*] --> Init Init --> Ready Ready --> Running Running --> Complete Running --> Failed Complete --> [*] Failed --> [*] Best Practices \u00b6 Keep Diagrams Simple Focus on one concept per diagram Use clear labels Maintain consistent styling Common Issues Ensure proper indentation Use correct arrow types (-->, -->, etc.) Test diagrams in the live editor Resources \u00b6 Mermaid Live Editor Mermaid Documentation","title":"Using Mermaid Diagrams"},{"location":"guides/using-mermaid-diagrams/#using-mermaid-diagrams","text":"This guide explains how to use Mermaid diagrams in the Floxide documentation.","title":"Using Mermaid Diagrams"},{"location":"guides/using-mermaid-diagrams/#overview","text":"Mermaid is a JavaScript-based diagramming tool that lets you create diagrams using text and code. It's particularly useful for visualizing: Flow charts Sequence diagrams Class diagrams State diagrams","title":"Overview"},{"location":"guides/using-mermaid-diagrams/#example-node-implementation","text":"Here's an example of a transform node implementation in Rust: use async_trait :: async_trait ; use floxide_core :: error :: FloxideError ; #[derive(Debug)] pub struct TransformNode < T > { id : String , transform_fn : Box < dyn Fn ( T ) -> T + Send + Sync > , } #[async_trait] impl < T : Send + Sync + ' static > Node < T , T > for TransformNode < T > { async fn execute ( & self , input : T ) -> Result < T , FloxideError > { // Apply the transformation function let output = ( self . transform_fn )( input ); Ok ( output ) } fn id ( & self ) -> String { self . id . clone () } }","title":"Example Node Implementation"},{"location":"guides/using-mermaid-diagrams/#basic-mermaid-usage","text":"Here's a simple flow diagram showing the transform node's execution: graph LR Input[Input] --> Transform[Transform Node] Transform --> Output[Output]","title":"Basic Mermaid Usage"},{"location":"guides/using-mermaid-diagrams/#node-types","text":"Here's a diagram showing different node types in Floxide: graph TD Node[Node] --> Transform[Transform Node] Node --> Timer[Timer Node] Node --> Reactive[Reactive Node] Node --> LongRunning[Long Running Node]","title":"Node Types"},{"location":"guides/using-mermaid-diagrams/#node-lifecycle","text":"A state diagram showing node lifecycle: stateDiagram-v2 [*] --> Init Init --> Ready Ready --> Running Running --> Complete Running --> Failed Complete --> [*] Failed --> [*]","title":"Node Lifecycle"},{"location":"guides/using-mermaid-diagrams/#best-practices","text":"Keep Diagrams Simple Focus on one concept per diagram Use clear labels Maintain consistent styling Common Issues Ensure proper indentation Use correct arrow types (-->, -->, etc.) Test diagrams in the live editor","title":"Best Practices"},{"location":"guides/using-mermaid-diagrams/#resources","text":"Mermaid Live Editor Mermaid Documentation","title":"Resources"}]}